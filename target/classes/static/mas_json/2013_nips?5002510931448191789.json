{"title": "The Fast Convergence of Incremental PCA.", "fields": ["convergence", "mathematical optimization", "discrete mathematics", "covariance", "eigenvalues and eigenvectors"], "abstract": "We consider a situation in which we see samples Xn \u2208 \u211dd drawn i.i.d. from some distribution with mean zero and unknown covariance A. We wish to compute the top eigenvector of A in an incremental fashion - with an algorithm that maintains an estimate of the top eigenvector in O(d) space, and incrementally adjusts the estimate with each new data point that arrives. Two classical such schemes are due to Krasulina (1969) and Oja (1983). We give finite-sample convergence rates for both.", "citation": "Citations (94)", "departments": ["University of California, San Diego", "University of California, San Diego", "University of California, San Diego"], "authors": ["Akshay Balsubramani.....http://dblp.org/pers/hd/b/Balsubramani:Akshay", "Sanjoy Dasgupta.....http://dblp.org/pers/hd/d/Dasgupta:Sanjoy", "Yoav Freund.....http://dblp.org/pers/hd/f/Freund:Yoav"], "conf": "nips", "year": "2013", "pages": 9}