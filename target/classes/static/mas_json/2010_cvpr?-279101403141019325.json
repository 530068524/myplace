{"title": "Figure-ground segmentation improves handled object recognition in egocentric video.", "fields": ["optical flow", "3d single object recognition", "figure ground", "scale invariant feature transform", "affine transformation"], "abstract": "Identifying handled objects, i.e. objects being manipulated by a user, is essential for recognizing the person's activities. An egocentric camera as worn on the body enjoys many advantages such as having a natural first-person view and not needing to instrument the environment. It is also a challenging setting, where background clutter is known to be a major source of problems and is difficult to handle with the camera constantly and arbitrarily moving. In this work we develop a bottom-up motion-based approach to robustly segment out foreground objects in egocentric video and show that it greatly improves object recognition accuracy. Our key insight is that egocentric video of object manipulation is a special domain and many domain-specific cues can readily help. We compute dense optical flow and fit it into multiple affine layers. We then use a max-margin classifier to combine motion with empirical knowledge of object location and background movement as well as temporal cues of support region and color appearance. We evaluate our segmentation algorithm on the large Intel Egocentric Object Recognition dataset with 42 objects and 100K frames. We show that, when combined with temporal integration, figure-ground segmentation improves the accuracy of a SIFT-based recognition system from 33% to 60%, and that of a latent-HOG system from 64% to 86%.", "citation": "Citations (145)", "year": "2010", "departments": ["Intel", "University of California, Berkeley"], "conf": "cvpr", "authors": ["Xiaofeng Ren.....http://dblp.org/pers/hd/r/Ren:Xiaofeng", "Chunhui Gu.....http://dblp.org/pers/hd/g/Gu:Chunhui"], "pages": 8}