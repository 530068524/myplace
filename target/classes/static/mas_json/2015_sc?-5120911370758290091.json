{"title": "Scientific benchmarking of parallel computing systems: twelve ways to tell the masses when reporting performance results.", "fields": ["best practice", "parallel processing", "interpretability", "benchmark", "benchmarking"], "abstract": "Measuring and reporting performance of parallel computers constitutes the basis for scientific advancement of high-performance computing (HPC). Most scientific reports show performance improvements of new techniques and are thus obliged to ensure reproducibility or at least interpretability. Our investigation of a stratified sample of 120 papers across three top conferences in the field shows that the state of the practice is lacking. For example, it is often unclear if reported improvements are deterministic or observed by chance. In addition to distilling best practices from existing work, we propose statistically sound analysis and reporting techniques and simple guidelines for experimental design in parallel computing and codify them in a portable benchmarking library. We aim to improve the standards of reporting research results and initiate a discussion in the HPC field. A wide adoption of our minimal set of rules will lead to better interpretability of performance results and improve the scientific culture in HPC.", "citation": "Citations (68)", "year": "2015", "departments": ["ETH Zurich", "ETH Zurich"], "conf": "sc", "authors": ["Torsten Hoefler.....http://dblp.org/pers/hd/h/Hoefler:Torsten", "Roberto Belli.....http://dblp.org/pers/hd/b/Belli:Roberto"], "pages": 12}