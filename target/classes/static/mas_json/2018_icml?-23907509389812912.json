{"title": "Addressing Function Approximation Error in Actor-Critic Methods.", "fields": ["restrict", "function approximation", "pattern recognition", "reinforcement learning", "machine learning"], "abstract": "In value-based reinforcement learning methods such as deep Q-learning, function approximation errors are known to lead to overestimated value estimates and suboptimal policies. We show that this problem persists in an actor-critic setting and propose novel mechanisms to minimize its effects on both the actor and critic. Our algorithm takes the minimum value between a pair of critics to restrict overestimation and delays policy updates to reduce per-update error. We evaluate our method on the suite of OpenAI gym tasks, outperforming the state of the art in every environment tested.", "citation": "Citations (2)", "departments": ["McGill University", "McGill University", "McGill University"], "authors": ["Scott Fujimoto.....http://dblp.org/pers/hd/f/Fujimoto:Scott", "Herke van Hoof.....http://dblp.org/pers/hd/h/Hoof:Herke_van", "David Meger.....http://dblp.org/pers/hd/m/Meger:David"], "conf": "icml", "year": "2018", "pages": 10}