{"title": "A Neural Autoregressive Topic Model.", "fields": ["binary tree", "softmax function", "feature learning", "generative model", "topic model"], "abstract": "We describe a new model for learning meaningful representations of text documents from an unlabeled collection of documents. This model is inspired by the recently proposed Replicated Softmax, an undirected graphical model of word counts that was shown to learn a better generative model and more meaningful document representations. Specifically, we take inspiration from the conditional mean-field recursive equations of the Replicated Softmax in order to define a neural network architecture that estimates the probability of observing a new word in a given document given the previously observed words. This paradigm also allows us to replace the expensive softmax distribution over words with a hierarchical distribution over paths in a binary tree of words. The end result is a model whose training complexity scales logarithmically with the vocabulary size instead of linearly as in the Replicated Softmax. Our experiments show that our model is competitive both as a generative model of documents and as a document representation learning algorithm.", "citation": "Citations (121)", "departments": ["Universit\u00e9 de Sherbrooke", "Universit\u00e9 de Sherbrooke"], "authors": ["Hugo Larochelle.....http://dblp.org/pers/hd/l/Larochelle:Hugo", "Stanislas Lauly.....http://dblp.org/pers/hd/l/Lauly:Stanislas"], "conf": "nips", "year": "2012", "pages": 9}