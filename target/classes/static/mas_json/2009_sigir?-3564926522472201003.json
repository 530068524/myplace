{"title": "Robust sparse rank learning for non-smooth ranking measures.", "fields": ["learning to rank", "pairwise comparison", "ranking svm", "discounted cumulative gain", "regret"], "abstract": "Recently increasing attention has been focused on directly optimizing ranking measures and inducing sparsity in learning models. However, few attempts have been made to relate them together in approaching the problem of learning to rank. In this paper, we consider the sparse algorithms to directly optimize the Normalized Discounted Cumulative Gain (NDCG) which is a widely-used ranking measure. We begin by establishing a reduction framework under which we reduce ranking, as measured by NDCG, to the importance weighted pairwise classification. Furthermore, we provide a sound theoretical guarantee for this reduction, bounding the realized NDCG regret in terms of a properly weighted pairwise classification regret, which implies that good performance can be robustly transferred from pairwise classification to ranking. Based on the converted pairwise loss function, it is conceivable to take into account sparsity in ranking models and to come up with a gradient possessing certain performance guarantee. For the sake of achieving sparsity, a novel algorithm named RSRank has also been devised, which performs L1 regularization using truncated gradient descent. Finally, experimental results on benchmark collection confirm the significant advantage of RSRank in comparison with several baseline methods.", "citation": "Citations (25)", "departments": ["Chinese Academy of Sciences", "Microsoft", "Chinese Academy of Sciences", "Chinese Academy of Sciences"], "authors": ["Zhengya Sun.....http://dblp.org/pers/hd/s/Sun:Zhengya", "Tao Qin.....http://dblp.org/pers/hd/q/Qin:Tao", "Qing Tao.....http://dblp.org/pers/hd/t/Tao:Qing", "Jue Wang.....http://dblp.org/pers/hd/w/Wang_0004:Jue"], "conf": "sigir", "year": "2009", "pages": 8}