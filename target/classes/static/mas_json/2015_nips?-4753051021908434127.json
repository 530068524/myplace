{"title": "Sample Complexity of Episodic Fixed-Horizon Reinforcement Learning.", "fields": ["bounded function", "sample complexity", "interactive learning", "markov decision process", "horizon"], "abstract": "Recently, there has been significant progress in understanding reinforcement learning in discounted infinite-horizon Markov decision processes (MDPs) by deriving tight sample complexity bounds. However, in many real-world applications, an interactive learning agent operates for a fixed or bounded period of time, for example tutoring students for exams or handling customer service requests. Such scenarios can often be better treated as episodic fixed-horizon MDPs, for which only looser bounds on the sample complexity exist. A natural notion of sample complexity in this setting is the number of episodes required to guarantee a certain performance with high probability (PAC guarantee). In this paper, we derive an upper PAC bound O(|S|2|A|H2/\u220a2 ln 1/\u03b4) and a lower PAC bound \u03a9(|S||A|H2/\u220a2 ln 1/\u03b4+c) that match up to log-terms and an additional linear dependency on the number of states |S|. The lower bound is the first of its kind for this setting. Our upper bound leverages Bernstein's inequality to improve on previous bounds for episodic finite-horizon MDPs which have a time-horizon dependency of at least H3.", "citation": "Citations (3)", "year": "2015", "departments": ["Carnegie Mellon University", "Carnegie Mellon University"], "conf": "nips", "authors": ["Christoph Dann.....http://dblp.org/pers/hd/d/Dann:Christoph", "Emma Brunskill.....http://dblp.org/pers/hd/b/Brunskill:Emma"], "pages": 9}