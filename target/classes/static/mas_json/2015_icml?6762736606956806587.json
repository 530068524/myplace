{"title": "\\(\\ell_{1, p}\\)-Norm Regularization: Error Bounds and Convergence Rate Analysis of First-Order Methods.", "fields": ["convergence", "rate of convergence", "regularization", "optimization problem", "norm"], "abstract": "In recent years, the l1,p-regularizer has been widely used to induce structured sparsity in the solutions to various optimization problems. Currently, such l1,p-regularized problems are typically solved by first-order methods. Motivated by the desire to analyze the convergence rates of these methods, we show that for a large class of l1,p-regularized problems, an error bound condition is satisfied when p e 2 [1, 2] or p = \u221e but fails to hold for any p e (2, \u221e). Based on this result, we show that many first-order methods enjoy an asymptotic linear rate of convergence when applied to l1,p-regularized linear or logistic regression with p e [1, 2] or p = \u221e. By contrast, numerical experiments suggest that for the same class of problems with p e (2, \u221e), the aforementioned methods may not converge linearly.", "citation": "Citations (6)", "year": "2015", "departments": ["The Chinese University of Hong Kong", "The Chinese University of Hong Kong", "The Chinese University of Hong Kong"], "conf": "icml", "authors": ["Zirui Zhou.....http://dblp.org/pers/hd/z/Zhou:Zirui", "Qi Zhang.....http://dblp.org/pers/hd/z/Zhang:Qi", "Anthony Man-Cho So.....http://dblp.org/pers/hd/s/So:Anthony_Man=Cho"], "pages": 10}