{"title": "Smoothness, Low Noise and Fast Rates.", "fields": ["rademacher complexity", "mathematical optimization", "convex optimization", "separable space", "smoothness"], "abstract": "We establish an excess risk bound of O(HR2n + \u221aHL*Rn) for ERM with an H-smooth loss function and a hypothesis class with Rademacher complexity Rn, where L* is the best risk achievable by the hypothesis class. For typical hypothesis classes where Rn = \u221aR/n, this translates to a learning rate of O(RH/n) in the separable (L* = 0) case and O(RH/n + \u221aL*RH/n) more generally. We also provide similar guarantees for online and stochastic convex optimization of a smooth non-negative objective.", "citation": "Citations (107)", "departments": ["Toyota Technological Institute at Chicago", "Toyota Technological Institute at Chicago", "University of Texas at Austin"], "authors": ["Nathan Srebro.....http://dblp.org/pers/hd/s/Srebro:Nathan", "Karthik Sridharan.....http://dblp.org/pers/hd/s/Sridharan:Karthik", "Ambuj Tewari.....http://dblp.org/pers/hd/t/Tewari:Ambuj"], "conf": "nips", "year": "2010", "pages": 9}