{"title": "Learning Greedy Policies for the Easy-First Framework.", "fields": ["overfitting", "policy learning", "structured prediction", "dependency grammar", "coreference"], "abstract": "Easy-first, a search-based structured prediction approach, has been applied to many NLP tasks including dependency parsing and coreference resolution. This approach employs a learned greedy policy (action scoring function) to make easy decisions first, which constrains the remaining decisions and makes them easier. We formulate greedy policy learning in the Easy-first approach as a novel non-convex optimization problem and solve it via an efficient Majorization Minimization (MM) algorithm. Results on within-document coreference and cross-document joint entity and event coreference tasks demonstrate that the proposed approach achieves statistically significant performance improvement over existing training regimes for Easy-first and is less susceptible to overfitting.", "citation": "Citations (3)", "departments": ["Oregon State University", "Oregon State University", "Washington State University", "Oregon State University", "Oregon State University"], "authors": ["Jun Xie.....http://dblp.org/pers/hd/x/Xie:Jun", "Chao Ma.....http://dblp.org/pers/hd/m/Ma_0001:Chao", "Janardhan Rao Doppa.....http://dblp.org/pers/hd/d/Doppa:Janardhan_Rao", "Prashanth Mannem.....http://dblp.org/pers/hd/m/Mannem:Prashanth", "Xiaoli Z. Fern.....http://dblp.org/pers/hd/f/Fern:Xiaoli_Z=", "Thomas G. Dietterich.....http://dblp.org/pers/hd/d/Dietterich:Thomas_G=", "Prasad Tadepalli.....http://dblp.org/pers/hd/t/Tadepalli:Prasad"], "conf": "aaai", "year": "2015", "pages": 7}