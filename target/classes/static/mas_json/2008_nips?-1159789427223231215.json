{"title": "Unlabeled data: Now it helps, now it doesn't.", "fields": ["training set", "unsupervised learning", "mathematics", "performance improvement", "semi supervised learning", "supervised learning"], "abstract": "Empirical evidence shows that in favorable situations semi-supervised learning (SSL) algorithms can capitalize on the abundance of unlabeled training data to improve the performance of a learning task, in the sense that fewer labeled training data are needed to achieve a target error bound. However, in other situations unlabeled data do not seem to help. Recent attempts at theoretically characterizing SSL gains only provide a partial and sometimes apparently conflicting explanations of whether, and to what extent, unlabeled data can help. In this paper, we attempt to bridge the gap between the practice and theory of semi-supervised learning. We develop a finite sample analysis that characterizes the value of un-labeled data and quantifies the performance improvement of SSL compared to supervised learning. We show that there are large classes of problems for which SSL can significantly outperform supervised learning, in finite sample regimes and sometimes also in terms of error convergence rates.", "citation": "Not cited", "year": "2008", "departments": ["University of Wisconsin-Madison", "University of Wisconsin-Madison", "University of Wisconsin-Madison"], "conf": "nips", "authors": ["Aarti Singh.....http://dblp.org/pers/hd/s/Singh:Aarti", "Robert D. Nowak.....http://dblp.org/pers/hd/n/Nowak:Robert_D=", "Xiaojin Zhu.....http://dblp.org/pers/hd/z/Zhu_0001:Xiaojin"], "pages": 8}