{"title": "Enhancing Language Models in Statistical Machine Translation with Backward N-grams and Mutual Information Triggers.", "fields": ["transfer based machine translation", "bleu", "rule based machine translation", "phrase", "cache language model"], "abstract": "In this paper, with a belief that a language model that embraces a larger context provides better prediction ability, we present two extensions to standard n-gram language models in statistical machine translation: a backward language model that augments the conventional forward language model, and a mutual information trigger model which captures long-distance dependencies that go beyond the scope of standard n-gram language models. We integrate the two proposed models into phrase-based statistical machine translation and conduct experiments on large-scale training data to investigate their effectiveness. Our experimental results show that both models are able to significantly improve translation quality and collectively achieve up to 1 BLEU point over a competitive baseline.", "citation": "Citations (17)", "year": "2011", "departments": ["Human Language  ... rch, Singapore", "Human Language  ... rch, Singapore", "Human Language  ... rch, Singapore"], "conf": "acl", "authors": ["Deyi Xiong.....http://dblp.org/pers/hd/x/Xiong:Deyi", "Min Zhang.....http://dblp.org/pers/hd/z/Zhang_0005:Min", "Haizhou Li.....http://dblp.org/pers/hd/l/Li_0001:Haizhou"], "pages": 10}