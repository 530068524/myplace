{"title": "Accelerated Mini-Batch Stochastic Dual Coordinate Ascent.", "fields": ["rate of convergence", "gradient descent", "mathematical optimization", "artificial intelligence", "machine learning"], "abstract": "Stochastic dual coordinate ascent (SDCA) is an effective technique for solving regularized loss minimization problems in machine learning. This paper considers an extension of SDCA under the mini-batch setting that is often used in practice. Our main contribution is to introduce an accelerated mini-batch version of SDCA and prove a fast convergence rate for this method. We discuss an implementation of our method over a parallel computing system, and compare the results to both the vanilla stochastic dual coordinate ascent and to the accelerated deterministic gradient descent method of Nesterov [2007].", "citation": "Citations (95)", "departments": ["Hebrew University of Jerusalem", "Rutgers University"], "authors": ["Shai Shalev-Shwartz.....http://dblp.org/pers/hd/s/Shalev=Shwartz:Shai", "Tong Zhang.....http://dblp.org/pers/hd/z/Zhang_0001:Tong"], "conf": "nips", "year": "2013", "pages": 8}