{"title": "Non-convex Finite-Sum Optimization Via SCSG Methods.", "fields": ["variance reduction", "stationary point", "stochastic gradient descent", "smoothness", "nabla symbol"], "abstract": "We develop a class of algorithms, as variants of the stochastically controlled stochastic gradient (SCSG) methods , for the smooth nonconvex finite-sum optimization problem. Only assuming the smoothness of each component, the complexity of SCSG to reach a stationary point with $E \\|\\nabla f(x)\\|^{2}\\le \\epsilon$ is $O(\\min\\{\\epsilon^{-5/3}, \\epsilon^{-1}n^{2/3}\\})$, which strictly outperforms the stochastic gradient descent. Moreover, SCSG is never worse than the state-of-the-art methods based on variance reduction and it significantly outperforms them when the target accuracy is low. A similar acceleration is also achieved when the functions satisfy the Polyak-Lojasiewicz condition. Empirical experiments demonstrate that SCSG outperforms stochastic gradient methods on training multi-layers neural networks in terms of both training and validation loss.", "citation": "Citations (6)", "year": "2017", "departments": ["University of California, Berkeley", "University of California, Berkeley", "University of California, Berkeley", "University of California, Berkeley"], "conf": "nips", "authors": ["Lihua Lei.....http://dblp.org/pers/hd/l/Lei:Lihua", "Cheng Ju.....http://dblp.org/pers/hd/j/Ju:Cheng", "Jianbo Chen.....http://dblp.org/pers/hd/c/Chen:Jianbo", "Michael I. Jordan.....http://dblp.org/pers/hd/j/Jordan:Michael_I="], "pages": 11}