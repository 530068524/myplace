{"title": "Structured Training for Neural Network Transition-Based Parsing.", "fields": ["treebank", "perceptron", "decoding methods", "dependency grammar", "parsing"], "abstract": "We present structured perceptron training for neural network transition-based dependency parsing. We learn the neural network representation using a gold corpus augmented by a large number of automatically parsed sentences. Given this fixed network representation, we learn a final layer using the structured perceptron with beam-search decoding. On the Penn Treebank, our parser reaches 94.26% unlabeled and 92.41% labeled attachment accuracy, which to our knowledge is the best accuracy on Stanford Dependencies to date. We also provide in-depth ablative analysis to determine which aspects of our model provide the largest gains in accuracy.", "citation": "Citations (135)", "year": "2015", "departments": ["Google", "Google", "Google", "Google"], "conf": "acl", "authors": ["David Weiss.....http://dblp.org/pers/hd/w/Weiss:David", "Chris Alberti.....http://dblp.org/pers/hd/a/Alberti:Chris", "Michael Collins.....http://dblp.org/pers/hd/c/Collins_0001:Michael", "Slav Petrov.....http://dblp.org/pers/hd/p/Petrov:Slav"], "pages": 11}