{"title": "Globally Optimal Training of Generalized Polynomial Neural Networks with Nonlinear Spectral Methods.", "fields": ["feedforward neural network", "stochastic gradient descent", "spectral method", "optimization problem", "rate of convergence"], "abstract": "The optimization problem behind neural networks is highly non-convex. Training with stochastic gradient descent and variants requires careful parameter tuning and provides no guarantee to achieve the global optimum. In contrast we show under quite weak assumptions on the data that a particular class of feedforward neural networks can be trained globally optimal with a linear convergence rate. Up to our knowledge this is the first practically feasible method which achieves such a guarantee. While the method can in principle be applied to deep networks, we restrict ourselves for simplicity in this paper to one- and two hidden layer networks. Our experiments confirms that these models are already rich enough to achieve good performance on a series of real-world datasets.", "citation": "Citations (7)", "departments": ["Saarland University", "Saarland University", "Saarland University"], "authors": ["Antoine Gautier.....http://dblp.org/pers/hd/g/Gautier:Antoine", "Quynh N. Nguyen.....http://dblp.org/pers/hd/n/Nguyen:Quynh_N=", "Matthias Hein.....http://dblp.org/pers/hd/h/Hein_0001:Matthias"], "conf": "nips", "year": "2016", "pages": 9}