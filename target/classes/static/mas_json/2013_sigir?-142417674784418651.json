{"title": "Deciding on an adjustment for multiplicity in IR experiments.", "fields": ["statistical inference", "multiplicity", "false positive paradox", "resampling", "permutation"], "abstract": "We evaluate statistical inference procedures for small-scale IR experiments that involve multiple comparisons against the baseline. These procedures adjust for multiple comparisons by ensuring that the probability of observing at least one false positive in the experiment is below a given threshold. We use only publicly available test collections and make our software available for download. In particular, we employ the TREC runs and runs constructed from the Microsoft learning-to-rank (MSLR) data set. Our focus is on non-parametric statistical procedures that include the Holm-Bonferroni adjustment of the permutation test p-values, the MaxT permutation test, and the permutation-based closed testing. In TREC-based simulations, these procedures retain from 66% to 92% of individually significant results (i.e., those obtained without taking other comparisons into account). Similar retention rates are observed in the MSLR simulations. For the largest evaluated query set size (i.e., 6400), procedures that adjust for multiplicity find at most 5% fewer true differences compared to unadjusted tests. At the same time, unadjusted tests produce many more false positives.", "citation": "Citations (9)", "departments": ["Carnegie Mellon University", "Texas Tech University", "Abt Associates  ... thesda, MD, USA"], "authors": ["Leonid Boytsov.....http://dblp.org/pers/hd/b/Boytsov:Leonid", "Anna Belova.....http://dblp.org/pers/hd/b/Belova:Anna", "Peter Westfall.....http://dblp.org/pers/hd/w/Westfall:Peter"], "conf": "sigir", "year": "2013", "pages": 10}