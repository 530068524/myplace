{"title": "Path-SGD: Path-Normalized Optimization in Deep Neural Networks.", "fields": ["invariant", "regularization", "method of steepest descent", "normalization", "artificial neural network"], "abstract": "We revisit the choice of SGD for training deep neural networks by reconsidering the appropriate geometry in which to optimize the weights. We argue for a geometry invariant to rescaling of weights that does not affect the output of the network, and suggest Path-SGD, which is an approximate steepest descent method with respect to a path-wise regularizer related to max-norm regularization. Path-SGD is easy and efficient to implement and leads to empirical gains over SGD and Ada-Grad.", "citation": "Citations (73)", "year": "2015", "departments": ["Toyota Technological Institute at Chicago", "University of Toronto", "Toyota Technological Institute at Chicago"], "conf": "nips", "authors": ["Behnam Neyshabur.....http://dblp.org/pers/hd/n/Neyshabur:Behnam", "Ruslan Salakhutdinov.....http://dblp.org/pers/hd/s/Salakhutdinov:Ruslan", "Nathan Srebro.....http://dblp.org/pers/hd/s/Srebro:Nathan"], "pages": 9}