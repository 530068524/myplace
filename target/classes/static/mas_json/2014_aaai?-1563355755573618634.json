{"title": "Preference Elicitation and Interview Minimization in Stable Matchings.", "fields": ["minification", "heuristic", "preference elicitation", "artificial intelligence", "machine learning"], "abstract": "While stable matching problems are widely studied, little work has investigated schemes for effectively eliciting agent preferences using either preference (e.g., comparison) queries or interviews (to form such comparisons); and no work has addressed how to combine both. We develop a new model for representing and assessing agent preferences that accommodates both forms of information and (heuristically) minimizes the number of queries and interviews required to determine a stable matching. Our Refine-then-Interview (RtI) scheme uses coarse preference queries to refine knowledge of agent preferences and relies on interviews only to assess comparisons of relatively \"close\" options. Empirical results show that RtI compares favorably to a recent pure interview minimization algorithm, and that the number of interviews it requires is generally independent of the size of the market.", "citation": "Citations (8)", "departments": ["University of Toronto", "University of Toronto"], "authors": ["Joanna Drummond.....http://dblp.org/pers/hd/d/Drummond:Joanna", "Craig Boutilier.....http://dblp.org/pers/hd/b/Boutilier:Craig"], "conf": "aaai", "year": "2014", "pages": 9}