{"title": "A reinterpretation of the policy oscillation phenomenon in approximate policy iteration.", "fields": ["bellman equation", "special case", "reinforcement learning", "reinterpretation", "phenomenon"], "abstract": "A majority of approximate dynamic programming approaches to the reinforcement learning problem can be categorized into greedy value function methods and value-based policy gradient methods. The former approach, although fast, is well known to be susceptible to the policy oscillation phenomenon. We take a fresh view to this phenomenon by casting a considerable subset of the former approach as a limiting special case of the latter. We explain the phenomenon in terms of this view and illustrate the underlying mechanism with artificial examples. We also use it to derive the constrained natural actor-critic algorithm that can interpolate between the aforementioned approaches. In addition, it has been suggested in the literature that the oscillation phenomenon might be subtly connected to the grossly suboptimal performance in the Tetris benchmark problem of all attempted approximate dynamic programming methods. We report empirical evidence against such a connection and in favor of an alternative explanation. Finally, we report scores in the Tetris problem that improve on existing dynamic programming based results.", "citation": "Citations (8)", "departments": ["Aalto University"], "authors": ["Paul Wagner.....http://dblp.org/pers/hd/w/Wagner:Paul"], "conf": "nips", "year": "2011", "pages": 9}