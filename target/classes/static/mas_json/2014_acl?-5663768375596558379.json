{"title": "Learning Grounded Meaning Representations with Autoencoders.", "fields": ["machine learning", "natural language processing", "categorization", "lexical definition", "modalities"], "abstract": "In this paper we address the problem of grounding distributional representations of lexical meaning. We introduce a new model which uses stacked autoencoders to learn higher-level embeddings from textual and visual input. The two modalities are encoded as vectors of attributes and are obtained automatically from text and images, respectively. We evaluate our model on its ability to simulate similarity judgments and concept categorization. On both tasks, our approach outperforms baselines and related models.", "citation": "Citations (85)", "year": "2014", "departments": ["University of Edinburgh", "School of Informatics"], "conf": "acl", "authors": ["Carina Silberer.....http://dblp.org/pers/hd/s/Silberer:Carina", "Mirella Lapata.....http://dblp.org/pers/hd/l/Lapata:Mirella"], "pages": 12}