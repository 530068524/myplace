{"title": "Stochastic convex optimization with bandit feedback.", "fields": ["regular polygon", "convex analysis", "regret", "frank wolfe algorithm", "lipschitz continuity", "derivative free optimization", "subderivative", "ellipsoid method", "proper convex function"], "abstract": "This paper addresses the problem of minimizing a convex, Lipschitz function $f$ over a convex, compact set $\\mathcal{X}$ under a stochastic bandit (i.e., noisy zeroth-order) feedback model. In this model, the algorithm is allowed to observe noisy realizations of the function value $f(x)$ at any query point $x \\in \\mathcal{X}$. The quantity of interest is the regret of the algorithm, which is the sum of the function values at algorithm's query points minus the optimal function value. We demonstrate a generalization of the ellipsoid algorithm that incurs $\\widetilde{\\mathcal{O}}({\\rm poly}(d)\\sqrt{T})$ regret. Since any algorithm has regret at least $\\Omega(\\sqrt{T})$ on this problem, our algorithm is optimal in terms of the scaling with $T$.", "citation": "Citations (74)", "departments": ["University of California, Berkeley", "Microsoft", "Microsoft", "Department of S ...  Pennysylvania", "Department of S ...  Pennysylvania"], "authors": ["Alekh Agarwal.....http://dblp.org/pers/hd/a/Agarwal:Alekh", "Dean P. Foster.....http://dblp.org/pers/hd/f/Foster:Dean_P=", "Daniel J. Hsu.....http://dblp.org/pers/hd/h/Hsu:Daniel_J=", "Sham M. Kakade.....http://dblp.org/pers/hd/k/Kakade:Sham_M=", "Alexander Rakhlin.....http://dblp.org/pers/hd/r/Rakhlin:Alexander"], "conf": "nips", "year": "2011", "pages": 9}