{"title": "Learning to Generalize: Meta-Learning for Domain Generalization.", "fields": ["contextual image classification", "domain", "reinforcement learning", "artificial intelligence", "machine learning"], "abstract": "Domain shift refers to the well known problem that a model trained in one source domain performs poorly when applied to a target domain with different statistics. {Domain Generalization} (DG) techniques attempt to alleviate this issue by producing models which by design generalize well to novel testing domains. We propose a novel {meta-learning} method for domain generalization. Rather than designing a specific model that is robust to domain shift as in most previous DG work, we propose a model agnostic training procedure for DG. Our algorithm simulates train/test domain shift during training by synthesizing virtual testing domains within each mini-batch. The meta-optimization objective requires that steps to improve training domain performance should also improve testing domain performance. This meta-learning procedure trains models with good generalization ability to novel domains. We evaluate our method and achieve state of the art results on a recent cross-domain image classification benchmark, as well demonstrating its potential on two classic reinforcement learning tasks.", "citation": "Citations (7)", "departments": ["Queen Mary University of London", "University of Edinburgh", "Queen Mary University of London"], "authors": ["Da Li.....http://dblp.org/pers/hd/l/Li:Da", "Yongxin Yang.....http://dblp.org/pers/hd/y/Yang:Yongxin", "Yi-Zhe Song.....http://dblp.org/pers/hd/s/Song:Yi=Zhe", "Timothy M. Hospedales.....http://dblp.org/pers/hd/h/Hospedales:Timothy_M="], "conf": "aaai", "year": "2018", "pages": 8}