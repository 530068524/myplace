{"title": "Learning Multimodal Attention LSTM Networks for Video Captioning.", "fields": ["deep learning", "exploit", "closed captioning", "sequence learning", "modalities"], "abstract": "Automatic generation of video caption is a challenging task as video is an information-intensive media with complex variations. Most existing methods, either based on language templates or sequence learning, have treated video as a flat data sequence while ignoring intrinsic multimodality nature. Observing that different modalities (e.g., frame, motion, and audio streams), as well as the elements within each modality, contribute differently to the sentence generation, we present a novel deep framework to boost video captioning by learning Multimodal Attention Long-Short Term Memory networks (MA-LSTM). Our proposed MA-LSTM fully exploits both multimodal streams and temporal attention to selectively focus on specific elements during the sentence generation. Moreover, we design a novel child-sum fusion unit in the MA-LSTM to effectively combine different encoded modalities to the initial decoding states. Different from existing approaches that employ the same LSTM structure for different modalities, we train modality-specific LSTM to capture the intrinsic representations of individual modalities. The experiments on two benchmark datasets (MSVD and MSR-VTT) show that our MA-LSTM significantly outperforms the state-of-the-art methods with  52.3  BLEU@4 and  70.4  CIDER-D metrics on MSVD dataset, respectively.", "citation": "Citations (2)", "departments": ["University of Science and Technology of China", "Microsoft", "University of Science and Technology of China", "Mircrosoft Rese ...  Beijing, China"], "authors": ["Jun Xu.....http://dblp.org/pers/hd/x/Xu:Jun", "Ting Yao.....http://dblp.org/pers/hd/y/Yao:Ting", "Yongdong Zhang.....http://dblp.org/pers/hd/z/Zhang:Yongdong", "Tao Mei.....http://dblp.org/pers/hd/m/Mei:Tao"], "conf": "mm", "year": "2017", "pages": 9}