{"title": "Complexity theoretic limitations on learning halfspaces.", "fields": ["combinatorics", "mathematics", "cube", "discrete mathematics"], "abstract": "We study the problem of agnostically learning halfspaces which is defined by a fixed but unknown distribution D on Q^n X {-1,1}. We define Err_H(D) as the least error of a halfspace classifier for D. A learner who can access D has to return a hypothesis whose error is small compared to Err_H(D).     Using the recently developed method of Daniely, Linial and Shalev-Shwartz we prove hardness of learning results assuming that random K-XOR formulas are hard to (strongly) refute. We show that no efficient learning algorithm has non-trivial worst-case performance even under the guarantees that Err_H(D)  0, and that D is supported in the Boolean cube. Namely, even under these favorable conditions, and for every c>0, it is hard to return a hypothesis with error  0. These results substantially improve on previously known results, that only show hardness of exact learning.", "citation": "Citations (21)", "year": "2016", "departments": ["Google"], "conf": "stoc", "authors": ["Amit Daniely.....http://dblp.org/pers/hd/d/Daniely:Amit"], "pages": 13}