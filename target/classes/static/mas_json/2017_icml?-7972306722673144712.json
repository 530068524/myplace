{"title": "Efficient Distributed Learning with Sparsity.", "fields": ["logarithm", "machine learning", "distributed learning", "mathematical optimization", "theoretical computer science"], "abstract": "We propose a novel, efficient approach for distributed sparse learning in high-dimensions, where observations are randomly partitioned across machines. Computationally, at each round our method only requires the master machine to solve a shifted ell_1 regularized M-estimation problem, and other workers to compute the gradient. In respect of communication, the proposed approach provably matches the estimation error bound of centralized methods within constant rounds of communications (ignoring logarithmic factors). We conduct extensive experiments on both simulated and real world datasets, and demonstrate encouraging performances on high-dimensional regression and classification tasks.", "citation": "Citations (2)", "year": "2017", "departments": ["Toyota Technological Institute at Chicago", "University of Chicago", "University of Chicago", "Tencent"], "conf": "icml", "authors": ["Jialei Wang.....http://dblp.org/pers/hd/w/Wang:Jialei", "Mladen Kolar.....http://dblp.org/pers/hd/k/Kolar:Mladen", "Nathan Srebro.....http://dblp.org/pers/hd/s/Srebro:Nathan", "Tong Zhang.....http://dblp.org/pers/hd/z/Zhang_0001:Tong"], "pages": 10}