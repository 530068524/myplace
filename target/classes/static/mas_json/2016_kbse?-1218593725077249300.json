{"title": "Evaluating the evaluations of code recommender systems: a reality check.", "fields": ["recommender system", "empirical research", "ground truth", "benchmark", "software"], "abstract": "While researchers develop many new exciting code recommender systems, such as method-call completion, code-snippet completion, or code search, an accurate evaluation of such systems is always a challenge. We analyzed the current literature and found that most of the current evaluations rely on artificial queries extracted from released code, which begs the question: Do such evaluations reflect real-life usages? To answer this question, we capture 6,189 fine-grained development histories from real IDE interactions. We use them as a ground truth and extract 7,157 real queries for a specific method-call recommender system. We compare the results of such real queries with different artificial evaluation strategies and check several assumptions that are repeatedly used in research, but never empirically evaluated. We find that an evolving context that is often observed in practice has a major effect on the prediction quality of recommender systems, but is not commonly reflected in artificial evaluations.", "citation": "Citations (6)", "departments": ["Technische Universit\u00e4t Darmstadt", "Technische Universit\u00e4t Darmstadt", "Technische Universit\u00e4t Darmstadt", "Technische Universit\u00e4t Darmstadt"], "authors": ["Sebastian Proksch.....http://dblp.org/pers/hd/p/Proksch:Sebastian", "Sven Amann.....http://dblp.org/pers/hd/a/Amann:Sven", "Sarah Nadi.....http://dblp.org/pers/hd/n/Nadi:Sarah", "Mira Mezini.....http://dblp.org/pers/hd/m/Mezini:Mira"], "conf": "kbse", "year": "2016", "pages": 11}