{"title": "Deep Exploration via Bootstrapped DQN.", "fields": ["learning environment", "reinforcement learning", "parameterized complexity", "dither", "bootstrapping"], "abstract": "Efficient exploration remains a major challenge for reinforcement learning (RL). Common dithering strategies for exploration, such as epsilon-greedy, do not carry out temporally-extended (or deep) exploration; this can lead to exponentially larger data requirements. However, most algorithms for statistically efficient RL are not computationally tractable in complex environments. Randomized value functions offer a promising approach to efficient exploration with generalization, but existing algorithms are not compatible with nonlinearly parameterized value functions. As a first step towards addressing such contexts we develop bootstrapped DQN. We demonstrate that bootstrapped DQN can combine deep exploration with deep neural networks for exponentially faster learning than any dithering strategy. In the Arcade Learning Environment bootstrapped DQN substantially improves learning speed and cumulative performance across most games.", "citation": "Citations (160)", "departments": ["Stanford University", "Google", "Google", "Stanford University"], "authors": ["Ian Osband.....http://dblp.org/pers/hd/o/Osband:Ian", "Charles Blundell.....http://dblp.org/pers/hd/b/Blundell:Charles", "Alexander Pritzel.....http://dblp.org/pers/hd/p/Pritzel:Alexander", "Benjamin Van Roy.....http://dblp.org/pers/hd/r/Roy:Benjamin_Van"], "conf": "nips", "year": "2016", "pages": 9}