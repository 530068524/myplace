{"title": "Ranking annotators for crowdsourced labeling tasks.", "fields": ["crowdsourcing", "ranking", "data mining", "spamming", "machine learning"], "abstract": "With the advent of crowdsourcing services it has become quite cheap and reasonably effective to get a dataset labeled by multiple annotators in a short amount of time. Various methods have been proposed to estimate the consensus labels by correcting for the bias of annotators with different kinds of expertise. Often we have low quality annotators or spammers\u2013annotators who assign labels randomly (e.g., without actually looking at the instance). Spammers can make the cost of acquiring labels very expensive and can potentially degrade the quality of the consensus labels. In this paper we formalize the notion of a spammer and define a score which can be used to rank the annotators\u2014with the spammers having a score close to zero and the good annotators having a high score close to one.", "citation": "Citations (90)", "departments": ["Siemens", "Siemens"], "authors": ["Vikas C. Raykar.....http://dblp.org/pers/hd/r/Raykar:Vikas_C=", "Shipeng Yu.....http://dblp.org/pers/hd/y/Yu:Shipeng"], "conf": "nips", "year": "2011", "pages": 9}