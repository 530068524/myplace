{"title": "Layrub: layer-centric GPU memory reuse and data migration in extreme-scale deep learning systems.", "fields": ["deep learning", "data migration", "robustness", "reuse", "high memory"], "abstract": "Growing accuracy and robustness of  Deep Neural Networks  (DNN) models are accompanied by growing model capacity (going deeper or wider). However, high memory requirements of those models make it difficult to execute the training process in one GPU. To address it, we first identify the memory usage characteristics for deep and wide convolutional networks, and demonstrate the opportunities of memory reuse on both intra-layer and inter-layer levels. We then present Layrub, a runtime data placement strategy that orchestrates the execution of training process. It achieves layer-centric reuse to reduce memory consumption for extreme-scale deep learning that cannot be run on one single GPU.", "citation": "Not cited", "year": "2018", "departments": ["Huazhong University of Science and Technology", "Huazhong University of Science and Technology", "Huazhong University of Science and Technology", "Huazhong University of Science and Technology", "Huazhong University of Science and Technology"], "conf": "ppopp", "authors": ["Bo Liu.....http://dblp.org/pers/hd/l/Liu:Bo", "Wenbin Jiang.....http://dblp.org/pers/hd/j/Jiang:Wenbin", "Hai Jin.....http://dblp.org/pers/hd/j/Jin_0001:Hai", "Xuanhua Shi.....http://dblp.org/pers/hd/s/Shi:Xuanhua", "Yang Ma.....http://dblp.org/pers/hd/m/Ma:Yang"], "pages": 2}