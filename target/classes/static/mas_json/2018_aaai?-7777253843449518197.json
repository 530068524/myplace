{"title": "Using k-Way Co-Occurrences for Learning Word Embeddings.", "fields": ["word embedding", "machine learning", "joint probability distribution", "semantics", "natural language"], "abstract": "Co-occurrences between two words provide useful insights into the semantics of those words. Consequently, numerous prior work on word embedding learning have used co-occurrences between two words as the training signal for learning word embeddings. However, in natural language texts it is common for multiple words to be related and co-occurring in the same context. We extend the notion of co-occurrences to cover $k(\\geq\\!\\!2)$-way co-occurrences among a set of $k$-words. Specifically, we prove a theoretical relationship between the joint probability of $k(\\geq\\!\\!2)$ words, and the sum of $\\ell_2$ norms of their embeddings. Next, we propose a learning objective motivated by our theoretical result that utilises $k$-way co-occurrences for learning word embeddings. Our experimental results show that the derived theoretical relationship does indeed hold empirically, and despite data sparsity, for some smaller $k$ values, $k$-way embeddings perform comparably or better than $2$-way embeddings in a range of tasks.", "citation": "Not cited", "departments": ["University of Liverpool", "National Institute of Informatics", "National Institute of Informatics"], "authors": ["Danushka Bollegala.....http://dblp.org/pers/hd/b/Bollegala:Danushka", "Yuichi Yoshida.....http://dblp.org/pers/hd/y/Yoshida:Yuichi", "Ken-ichi Kawarabayashi.....http://dblp.org/pers/hd/k/Kawarabayashi:Ken=ichi"], "conf": "aaai", "year": "2018", "pages": 8}