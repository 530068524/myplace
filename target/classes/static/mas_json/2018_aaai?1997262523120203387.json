{"title": "Syntax-Directed Attention for Neural Machine Translation.", "fields": ["machine translation", "natural language processing", "left and right", "architecture", "syntax"], "abstract": "Attention mechanism, including global attention and local attention, plays a key role in neural machine translation (NMT). Global attention attends to all source words for word prediction. In comparison, local attention selectively looks at fixed-window source words. However, alignment weights for the current target word often decrease to the left and right by linear distance centering on the aligned source position and neglect syntax-directed distance constraints. In this paper, we extend local attention with syntax-distance constraint, to focus on syntactically related source words with the predicted target word, thus learning a more effective context vector for word prediction. Moreover, we further propose a double context NMT architecture, which consists of a global context vector and a syntax-directed context vector over the global attention, to provide more translation performance for NMT from source representation. The experiments on the large-scale Chinese-to-English and English-to-Germen translation tasks show that the proposed approach achieves a substantial and significant improvement over the baseline system.", "citation": "Citations (2)", "departments": ["Harbin Institute of Technology", "Shanghai Jiao Tong University", "National Institute of Information and Communications Technology", "National Institute of Information and Communications Technology", "Harbin Institute of Technology"], "authors": ["Kehai Chen.....http://dblp.org/pers/hd/c/Chen:Kehai", "Rui Wang.....http://dblp.org/pers/hd/w/Wang_0015:Rui", "Masao Utiyama.....http://dblp.org/pers/hd/u/Utiyama:Masao", "Eiichiro Sumita.....http://dblp.org/pers/hd/s/Sumita:Eiichiro", "Tiejun Zhao.....http://dblp.org/pers/hd/z/Zhao:Tiejun"], "conf": "aaai", "year": "2018", "pages": 8}