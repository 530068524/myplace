{"title": "Towards automating disambiguation of regulations: using the wisdom of crowds.", "fields": ["crowdsourcing", "ground truth", "crowds", "comprehension", "scalability"], "abstract": "Compliant software is a critical need of all modern businesses. Disambiguating regulations to derive requirements is therefore an important software engineering activity. Regulations however are ridden with ambiguities that make their comprehension a challenge, seemingly surmountable only by legal experts. Since legal experts' involvement in every project is expensive, approaches to automate the disambiguation need to be explored. These approaches however require a large amount of annotated data. Collecting data exclusively from experts is not a scalable and affordable solution. In this paper, we present the results of a crowd sourcing experiment to collect annotations on ambiguities in regulations from professional software engineers. We discuss an approach to automate the arduous and critical step of identifying ground truth labels by employing crowd consensus using Expectation Maximization (EM). We demonstrate that the annotations reaching a consensus match those of experts with an accuracy of 87%.", "citation": "Not cited", "departments": ["TCS Research, India", "TCS Research, India", "TCS Research, India", "TCS Research, India", "TCS Research, India"], "authors": ["Manasi Patwardhan.....http://dblp.org/pers/hd/p/Patwardhan:Manasi", "Abhishek Sainani.....http://dblp.org/pers/hd/s/Sainani:Abhishek", "Richa Sharma.....http://dblp.org/pers/hd/s/Sharma:Richa", "Shirish Karande.....http://dblp.org/pers/hd/k/Karande:Shirish", "Smita Ghaisas.....http://dblp.org/pers/hd/g/Ghaisas:Smita"], "conf": "kbse", "year": "2018", "pages": 6}