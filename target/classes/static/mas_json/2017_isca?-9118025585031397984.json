{"title": "ScaleDeep: A Scalable Compute Architecture for Learning and Evaluating Deep Networks.", "fields": ["pipeline", "systems architecture", "memory hierarchy", "speedup", "granularity"], "abstract": "Deep Neural Networks (DNNs) have demonstrated state-of-the-art performance on a broad range of tasks involving natural language, speech, image, and video processing, and are deployed in many real world applications. However, DNNs impose significant computational challenges owing to the complexity of the networks and the amount of data they process, both of which are projected to grow in the future. To improve the efficiency of DNNs, we propose ScaleDeep, a dense, scalable server architecture, whose processing, memory and interconnect subsystems are specialized to leverage the compute and communication characteristics of DNNs. While several DNN accelerator designs have been proposed in recent years, the key difference is that ScaleDeep primarily targets DNN training, as opposed to only inference or evaluation. The key architectural features from which ScaleDeep derives its efficiency are: (i) heterogeneous processing tiles and chips to match the wide diversity in computational characteristics (FLOPs and Bytes/FLOP ratio) that manifest at different levels of granularity in DNNs, (ii) a memory hierarchy and 3-tiered interconnect topology that is suited to the memory access and communication patterns in DNNs, (iii) a low-overhead synchronization mechanism based on hardware data-flow trackers, and (iv) methods to map DNNs to the proposed architecture that minimize data movement and improve core utilization through nested pipelining. We have developed a compiler to allow any DNN topology to be programmed onto ScaleDeep, and a detailed architectural simulator to estimate performance and energy. The simulator incorporates timing and power models of ScaleDeep's components based on synthesis to Intel's 14nm technology. We evaluate an embodiment of ScaleDeep with 7032 processing tiles that operates at 600 MHz and has a peak performance of 680 TFLOPs (single precision) and 1.35 PFLOPs (half-precision) at 1.4KW. Across 11 state-of-the-art DNNs containing 0.65M-14.9M neurons and 6.8M-145.9M weights, including winners from 5 years of the ImageNet competition, ScaleDeep demonstrates 6x-28x speedup at iso-power over the state-of-the-art performance on GPUs.", "citation": "Citations (7)", "departments": ["IBM", "Purdue University", "Intel", "Intel", "Intel"], "authors": ["Swagath Venkataramani.....http://dblp.org/pers/hd/v/Venkataramani:Swagath", "Ashish Ranjan.....http://dblp.org/pers/hd/r/Ranjan:Ashish", "Subarno Banerjee.....http://dblp.org/pers/hd/b/Banerjee:Subarno", "Dipankar Das.....http://dblp.org/pers/hd/d/Das_0002:Dipankar", "Sasikanth Avancha.....http://dblp.org/pers/hd/a/Avancha:Sasikanth", "Ashok Jagannathan.....http://dblp.org/pers/hd/j/Jagannathan:Ashok", "Ajaya Durg.....http://dblp.org/pers/hd/d/Durg:Ajaya", "Dheemanth Nagaraj.....http://dblp.org/pers/hd/n/Nagaraj:Dheemanth", "Bharat Kaul.....http://dblp.org/pers/hd/k/Kaul:Bharat", "Pradeep Dubey.....http://dblp.org/pers/hd/d/Dubey:Pradeep", "Anand Raghunathan.....http://dblp.org/pers/hd/r/Raghunathan:Anand"], "conf": "isca", "year": "2017", "pages": 14}