{"title": "A unified memory network architecture for in-memory computing in commodity servers.", "fields": ["conventional memory", "uniform memory access", "registered memory", "computing with memory", "interleaved memory"], "abstract": "In-memory computing is emerging as a promising paradigm in commodity servers to accelerate data-intensive processing by striving to keep the entire dataset in DRAM. To address the tremendous pressure on the main memory system, discrete memory modules can be networked together to form a memory pool, enabled by recent trends towards richer memory interfaces (e.g. Hybrid Memory Cubes, or HMCs). Such an  inter-memory network  provides a scalable fabric to expand memory capacity, but still suffers from long multi-hop latency, limited bandwidth, and high power consumption---problems that will continue to exacerbate as the gap between interconnect and transistor performance grows. Moreover, inside each memory module, an  intra-memory network  (NoC) is typically employed to connect different memory partitions. Without careful design, the back-pressure inside the memory modules can further propagate to the inter-memory network to cause a performance bottleneck.   To address these problems, we propose co-optimization of intra- and inter-memory network. First, we re-organize the intra-memory network structure, and provide a smart I/O interface to reuse the intra-memory NoC as the network switches for inter-memory communication, thus forming a unified memory network. Based on this architecture, we further optimize the inter-memory network for both high performance and lower energy, including a distance-aware  selective  compression scheme to drastically reduce communication burden, and a light-weight power-gating algorithm to turn off under-utilized links while guaranteeing a connected graph and deadlock-free routing. We develop an event-driven simulator to model our proposed architectures. Experiment results based on both synthetic traffic and real big-data workloads show that our unified memory network architecture can achieve 75.1% average memory access latency reduction and 22.1% total memory energy saving.", "citation": "Citations (3)", "departments": ["University of California, Santa Barbara", "University of California, Santa Barbara", "University of California, Santa Cruz", "Hewlett-Packard", "Hewlett-Packard"], "authors": ["Jia Zhan.....http://dblp.org/pers/hd/z/Zhan:Jia", "Itir Akgun.....http://dblp.org/pers/hd/a/Akgun:Itir", "Jishen Zhao.....http://dblp.org/pers/hd/z/Zhao:Jishen", "Al Davis.....http://dblp.org/pers/hd/d/Davis:Al", "Paolo Faraboschi.....http://dblp.org/pers/hd/f/Faraboschi:Paolo", "Yuangang Wang.....http://dblp.org/pers/hd/w/Wang:Yuangang", "Yuan Xie.....http://dblp.org/pers/hd/x/Xie_0001:Yuan"], "conf": "micro", "year": "2016", "pages": 14}