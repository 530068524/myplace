{"title": "Hierarchical Neural Story Generation.", "fields": ["fluent", "machine learning", "artificial intelligence", "premise", "natural language processing"], "abstract": "We explore story generation: creative systems that can build coherent and fluent passages of text about a topic. We collect a large dataset of 300K human-written stories paired with writing prompts from an online forum. Our dataset enables hierarchical story generation, where the model first generates a premise, and then transforms it into a passage of text. We gain further improvements with a novel form of model fusion that improves the relevance of the story to the prompt, and adding a new gated multi-scale self-attention mechanism to model long-range context. Experiments show large improvements over strong baselines on both automated and human evaluations. Human judges prefer stories generated by our approach to those from a strong non-hierarchical model by a factor of two to one.", "citation": "Citations (2)", "year": "2018", "departments": ["University of Washington", "Universit\u00e9 de Montr\u00e9al"], "conf": "acl", "authors": ["Mike Lewis.....http://dblp.org/pers/hd/l/Lewis:Mike", "Yann Dauphin.....http://dblp.org/pers/hd/d/Dauphin:Yann", "Angela Fan.....http://dblp.org/pers/hd/f/Fan:Angela"], "pages": 10}