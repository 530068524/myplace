{"title": "Deep Visual Analogy-Making.", "fields": ["embedding", "machine learning", "subtraction", "analogy", "language model"], "abstract": "In addition to identifying the content within a single image, relating images and generating related images are critical tasks for image understanding. Recently, deep convolutional networks have yielded breakthroughs in predicting image labels, annotations and captions, but have only just begun to be used for generating high-quality images. In this paper we develop a novel deep network trained end-to-end to perform visual analogy making, which is the task of transforming a query image according to an example pair of related images. Solving this problem requires both accurately recognizing a visual relationship and generating a transformed query image accordingly. Inspired by recent advances in language modeling, we propose to solve visual analogies by learning to map images to a neural embedding in which analogical reasoning is simple, such as by vector subtraction and addition. In experiments, our model effectively models visual analogies on several datasets: 2D shapes, animated video game sprites, and 3D car models.", "citation": "Citations (95)", "year": "2015", "departments": ["University of Michigan", "University of Michigan", "University of Michigan", "University of Michigan"], "conf": "nips", "authors": ["Scott E. Reed.....http://dblp.org/pers/hd/r/Reed:Scott_E=", "Yi Zhang.....http://dblp.org/pers/hd/z/Zhang:Yi", "Yuting Zhang.....http://dblp.org/pers/hd/z/Zhang:Yuting", "Honglak Lee.....http://dblp.org/pers/hd/l/Lee:Honglak"], "pages": 9}