{"title": "Deep learning with COTS HPC systems.", "fields": ["deep learning", "supercomputer", "multi core processor", "cuda", "infiniband"], "abstract": "Scaling up deep learning algorithms has been shown to lead to increased performance in benchmark tasks and to enable discovery of complex high-level features. Recent efforts to train extremely large networks (with over 1 billion parameters) have relied on cloudlike computing infrastructure and thousands of CPU cores. In this paper, we present technical details and results from our own system based on Commodity Off-The-Shelf High Performance Computing (COTS HPC) technology: a cluster of GPU servers with Infiniband interconnects and MPI. Our system is able to train 1 billion parameter networks on just 3 machines in a couple of days, and we show that it can scale to networks with over 11 billion parameters using just 16 machines. As this infrastructure is much more easily marshaled by others, the approach enables much wider-spread research with extremely large neural networks.", "citation": "Citations (447)", "departments": ["Stanford University", "Stanford University", "Stanford University", "Stanford University", "Nvidia"], "authors": ["Adam Coates.....http://dblp.org/pers/hd/c/Coates:Adam", "Brody Huval.....http://dblp.org/pers/hd/h/Huval:Brody", "Tao Wang.....http://dblp.org/pers/hd/w/Wang:Tao", "David J. Wu.....http://dblp.org/pers/hd/w/Wu:David_J=", "Bryan Catanzaro.....http://dblp.org/pers/hd/c/Catanzaro:Bryan", "Andrew Y. Ng.....http://dblp.org/pers/hd/n/Ng:Andrew_Y="], "conf": "icml", "year": "2013", "pages": 9}