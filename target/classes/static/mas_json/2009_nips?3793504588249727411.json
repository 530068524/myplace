{"title": "Efficient Large-Scale Distributed Training of Conditional Maximum Entropy Models.", "fields": ["data set", "principle of maximum entropy", "time complexity", "central processing unit", "computation"], "abstract": "Training conditional maximum entropy models on massive data sets requires significant computational resources. We examine three common distributed training methods for conditional maxent: a distributed gradient computation method, a majority vote method, and a mixture weight method. We analyze and compare the CPU and network time complexity of each of these methods and present a theoretical analysis of conditional maxent models, including a study of the convergence of the mixture weight method, the most resource-efficient technique. We also report the results of large-scale experiments comparing these three methods which demonstrate the benefits of the mixture weight method: this method consumes less resources, while achieving a performance comparable to that of standard approaches.", "citation": "Citations (209)", "year": "2009", "departments": ["Google", "Courant Institute of Mathematical Sciences", "Google", "Brigham Young University", "Google"], "conf": "nips", "authors": ["Gideon Mann.....http://dblp.org/pers/hd/m/Mann:Gideon", "Ryan T. McDonald.....http://dblp.org/pers/hd/m/McDonald:Ryan_T=", "Mehryar Mohri.....http://dblp.org/pers/hd/m/Mohri:Mehryar", "Nathan Silberman.....http://dblp.org/pers/hd/s/Silberman:Nathan", "Dan Walker.....http://dblp.org/pers/hd/w/Walker:Dan"], "pages": 9}