{"title": "On Tensor Train Rank Minimization : Statistical Efficiency and Scalable Algorithm.", "fields": ["minification", "efficiency", "statistical theory", "tensor", "scalability"], "abstract": "Tensor train (TT) decomposition provides a space-efficient representation for higher-order tensors. Despite its advantage, we face two crucial limitations when we apply the TT decomposition to machine learning problems: the lack of statistical theory and of scalable algorithms. In this paper, we address the limitations. First, we introduce a convex relaxation of the TT decomposition problem and derive its error bound for the tensor completion task. Next, we develop a randomized optimization method, in which the time complexity is as efficient as the space complexity is. In experiments, we numerically confirm the derived bounds and empirically demonstrate the performance of our method with a real higher-order tensor.", "citation": "Not cited", "year": "2017", "departments": ["National Institute of Advanced Industrial Science and Technology", "Institute of St ... cal Mathematics", "RIKEN AIP"], "conf": "nips", "authors": ["Masaaki Imaizumi.....http://dblp.org/pers/hd/i/Imaizumi:Masaaki", "Takanori Maehara.....http://dblp.org/pers/hd/m/Maehara:Takanori", "Kohei Hayashi.....http://dblp.org/pers/hd/h/Hayashi:Kohei"], "pages": 10}