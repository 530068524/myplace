{"title": "Adaptive Submodular Maximization in Bandit Setting.", "fields": ["uncertainty principle", "preference elicitation", "maximization", "regret", "submodular set function"], "abstract": "Maximization of submodular functions has wide applications in machine learning and artificial intelligence. Adaptive submodular maximization has been traditionally studied under the assumption that the model of the world, the expected gain of choosing an item given previously selected items and their states, is known. In this paper, we study the setting where the expected gain is initially unknown, and it is learned by interacting repeatedly with the optimized function. We propose an efficient algorithm for solving our problem and prove that its expected cumulative regret increases logarithmically with time. Our regret bound captures the inherent property of submodular maximization, earlier mistakes are more costly than later ones. We refer to our approach as Optimistic Adaptive Submodular Maximization (OASM) because it trades off exploration and exploitation based on the optimism in the face of uncertainty principle. We evaluate our method on a preference elicitation problem and show that non-trivial K-step policies can be learned from just a few hundred interactions with the problem.", "citation": "Citations (19)", "departments": ["French Institute for Research in Computer Science and Automation", "Technicolor", "Stanford University", "Technicolor", "Rutgers University"], "authors": ["Victor Gabillon.....http://dblp.org/pers/hd/g/Gabillon:Victor", "Branislav Kveton.....http://dblp.org/pers/hd/k/Kveton:Branislav", "Zheng Wen.....http://dblp.org/pers/hd/w/Wen:Zheng", "Brian Eriksson.....http://dblp.org/pers/hd/e/Eriksson:Brian", "S. Muthukrishnan.....http://dblp.org/pers/hd/m/Muthukrishnan:S="], "conf": "nips", "year": "2013", "pages": 9}