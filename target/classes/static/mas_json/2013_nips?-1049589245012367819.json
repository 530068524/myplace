{"title": "Linear Convergence with Condition Number Independent Access of Full Gradients.", "fields": ["convex function", "rate of convergence", "condition number", "gradient descent", "optimization problem"], "abstract": "For smooth and strongly convex optimizations, the optimal iteration complexity of the gradient-based algorithm is O(\u221ak log 1/e), where k is the condition number. In the case that the optimization problem is ill-conditioned, we need to evaluate a large number of full gradients, which could be computationally expensive. In this paper, we propose to remove the dependence on the condition number by allowing the algorithm to access stochastic gradients of the objective function. To this end, we present a novel algorithm named Epoch Mixed Gradient Descent (EMGD) that is able to utilize two kinds of gradients. A distinctive step in EMGD is the mixed gradient descent, where we use a combination of the full and stochastic gradients to update the intermediate solution. Theoretical analysis shows that EMGD is able to find an e-optimal solution by computing O(log 1/e) full gradients and O(k2 log 1/e) stochastic gradients.", "citation": "Citations (75)", "departments": ["Michigan State University", "Michigan State University", "Michigan State University"], "authors": ["Lijun Zhang.....http://dblp.org/pers/hd/z/Zhang_0005:Lijun", "Mehrdad Mahdavi.....http://dblp.org/pers/hd/m/Mahdavi:Mehrdad", "Rong Jin.....http://dblp.org/pers/hd/j/Jin:Rong"], "conf": "nips", "year": "2013", "pages": 9}