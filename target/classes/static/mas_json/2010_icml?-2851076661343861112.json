{"title": "Model-based reinforcement learning with nearly tight exploration complexity bounds.", "fields": ["logarithm", "reinforcement learning", "machine learning", "upper and lower bounds", "binary logarithm", "time complexity", "markov decision process"], "abstract": "One might believe that model-based algorithms of reinforcement learning can propagate the obtained experience more quickly, and are able to direct exploration better. As a consequence, fewer exploratory actions should be enough to learn a good policy. Strangely enough, current theoretical results for model-based algorithms do not support this claim: In a finite Markov decision process with N states, the best bounds on the number of exploratory steps necessary are of order O(N2 log N), in contrast to the O(N log N) bound available for the model-free, delayed Q-learning algorithm. In this paper we show that Mormax, a modified version of the Rmax algorithm needs to make at most O(N log N) exploratory steps. This matches the lower bound up to logarithmic factors, as well as the upper bound of the state-of-the-art model-free algorithm, while our new bound improves the dependence on other problem parameters.", "citation": "Not cited", "departments": ["University of Alberta", "University of Alberta"], "authors": ["Istvan Szita.....http://dblp.org/pers/hd/s/Szita:Istvan", "Csaba Szepesv\u00e1ri.....http://dblp.org/pers/hd/s/Szepesv=aacute=ri:Csaba"], "conf": "icml", "year": "2010", "pages": 8}