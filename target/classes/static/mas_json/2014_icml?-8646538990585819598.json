{"title": "Sample Efficient Reinforcement Learning with Gaussian Processes.", "fields": ["global positioning system", "sample complexity", "reinforcement learning", "gaussian process", "exponential function"], "abstract": "This paper derives sample complexity results for using Gaussian Processes (GPs) in both modelbased and model-free reinforcement learning (RL). We show that GPs are KWIK learnable, proving for the first time that a model-based RL approach using GPs, GP-Rmax, is sample efficient (PAC-MDP). However, we then show that previous approaches to model-free RL using GPs take an exponential number of steps to find an optimal policy, and are therefore not sample efficient. The third and main contribution is the introduction of a model-free RL algorithm using GPs, DGPQ, which is sample efficient and, in contrast to model-based algorithms, capable of acting in real time, as demonstrated on a fivedimensional aircraft simulator.", "citation": "Citations (10)", "year": "2014", "departments": ["Massachusetts Institute of Technology", "Massachusetts Institute of Technology", "Massachusetts Institute of Technology"], "conf": "icml", "authors": ["Robert C. Grande.....http://dblp.org/pers/hd/g/Grande:Robert_C=", "Thomas J. Walsh.....http://dblp.org/pers/hd/w/Walsh:Thomas_J=", "Jonathan P. How.....http://dblp.org/pers/hd/h/How:Jonathan_P="], "pages": 9}