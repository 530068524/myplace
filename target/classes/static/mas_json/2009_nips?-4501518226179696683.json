{"title": "Kernels and learning curves for Gaussian process regression on random graphs.", "fields": ["variable kernel density estimation", "kernel embedding of distributions", "vertex", "kriging", "crossover"], "abstract": "We investigate how well Gaussian process regression can learn functions defined on graphs, using large regular random graphs as a paradigmatic example. Random-walk based kernels are shown to have some non-trivial properties: within the standard approximation of a locally tree-like graph structure, the kernel does not become constant, i.e. neighbouring function values do not become fully correlated, when the lengthscale \u03c3 of the kernel is made large. Instead the kernel attains a non-trivial limiting form, which we calculate. The fully correlated limit is reached only once loops become relevant, and we estimate where the crossover to this regime occurs. Our main subject are learning curves of Bayes error versus training set size. We show that these are qualitatively well predicted by a simple approximation using only the spectrum of a large tree as input, and generically scale with n/V, the number of training examples per vertex. We also explore how this behaviour changes for kernel lengthscales that are large enough for loops to become important.", "citation": "Citations (3)", "year": "2009", "departments": ["King's College London", "King's College London", "French Institute for Research in Computer Science and Automation"], "conf": "nips", "authors": ["Peter Sollich.....http://dblp.org/pers/hd/s/Sollich:Peter", "Matthew Urry.....http://dblp.org/pers/hd/u/Urry:Matthew", "Camille Coti.....http://dblp.org/pers/hd/c/Coti:Camille"], "pages": 9}