{"title": "Reversible Architectures for Arbitrarily Deep Residual Neural Networks.", "fields": ["training set", "machine learning", "residual", "artificial neural network", "ordinary differential equation"], "abstract": "Recently, deep residual networks have been successfully applied in many computer vision and natural language processing tasks, pushing the state-of-the-art performance with deeper and wider architectures. In this work, we interpret deep residual networks as ordinary differential equations (ODEs), which have long been studied in mathematics and physics with rich theoretical and empirical success. From this interpretation, we develop a theoretical framework on stability and reversibility of deep neural networks, and derive three reversible neural network architectures that can go arbitrarily deep in theory. The reversibility property allows a memory-efficient implementation, which does not need to store the activations for most hidden layers. Together with the stability of our architectures, this enables training deeper networks using only modest computational resources. We provide both theoretical analyses and empirical results. Experimental results demonstrate the efficacy of our architectures against several strong baselines on CIFAR-10, CIFAR-100 and STL-10 with superior or on-par state-of-the-art performance. Furthermore, we show our architectures yield superior results when trained using fewer training data.", "citation": "Citations (11)", "departments": ["University of British Columbia", "University of British Columbia", "University of British Columbia", "Emory University"], "authors": ["Bo Chang.....http://dblp.org/pers/hd/c/Chang:Bo", "Lili Meng.....http://dblp.org/pers/hd/m/Meng:Lili", "Eldad Haber.....http://dblp.org/pers/hd/h/Haber:Eldad", "Lars Ruthotto.....http://dblp.org/pers/hd/r/Ruthotto:Lars", "David Begert.....http://dblp.org/pers/hd/b/Begert:David", "Elliot Holtham.....http://dblp.org/pers/hd/h/Holtham:Elliot"], "conf": "aaai", "year": "2018", "pages": 8}