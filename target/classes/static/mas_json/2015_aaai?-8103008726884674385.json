{"title": "High-Confidence Off-Policy Evaluation.", "fields": ["concentration inequality", "management science", "data mining", "expected return", "reinforcement learning"], "abstract": "Many reinforcement learning algorithms use trajectories collected from the execution of one or more policies to propose a new policy. Because execution of a bad policy can be costly or dangerous, techniques for evaluating the performance of the new policy without requiring its execution have been of recent interest in industry. Such off-policy evaluation methods, which estimate the performance of a policy using trajectories collected from the execution of other policies, heretofore have not provided confidences regarding the accuracy of their estimates. In this paper we propose an off-policy method for computing a lower confidence bound on the expected return of a policy.", "citation": "Citations (58)", "departments": ["University of Massachusetts Amherst", "Adobe Systems", "Adobe Systems"], "authors": ["Philip S. Thomas.....http://dblp.org/pers/hd/t/Thomas:Philip_S=", "Georgios Theocharous.....http://dblp.org/pers/hd/t/Theocharous:Georgios", "Mohammad Ghavamzadeh.....http://dblp.org/pers/hd/g/Ghavamzadeh:Mohammad"], "conf": "aaai", "year": "2015", "pages": 7}