{"title": "Non-strongly-convex smooth stochastic approximation with convergence rate O(1/n).", "fields": ["convex function", "quadratic equation", "stochastic gradient descent", "stochastic approximation", "convexity"], "abstract": "We consider the stochastic approximation problem where a convex function has to be minimized, given only the knowledge of unbiased estimates of its gradients at certain points, a framework which includes machine learning methods based on the minimization of the empirical risk. We focus on problems without strong convexity, for which all previously known algorithms achieve a convergence rate for function values of O(1/\u221an) after n iterations. We consider and analyze two algorithms that achieve a rate of O(1/n) for classical supervised learning problems. For least-squares regression, we show that averaged stochastic gradient descent with constant step-size achieves the desired rate. For logistic regression, this is achieved by a simple novel stochastic gradient algorithm that (a) constructs successive local quadratic approximations of the loss functions, while (b) preserving the same running-time complexity as stochastic gradient descent. For these algorithms, we provide a non-asymptotic analysis of the generalization error (in expectation, and also in high probability for least-squares), and run extensive experiments showing that they often outperform existing approaches.", "citation": "Citations (121)", "departments": ["\u00c9cole Normale Sup\u00e9rieure", "T\u00e9l\u00e9com ParisTech"], "authors": ["Francis R. Bach.....http://dblp.org/pers/hd/b/Bach:Francis_R=", "Eric Moulines.....http://dblp.org/pers/hd/m/Moulines:Eric"], "conf": "nips", "year": "2013", "pages": 9}