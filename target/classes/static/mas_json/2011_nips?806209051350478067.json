{"title": "Scalable Training of Mixture Models via Coresets.", "fields": ["coreset", "data point", "density estimation", "data set", "generalization"], "abstract": "How can we train a statistical mixture model on a massive data set? In this paper, we show how to construct coresets for mixtures of Gaussians and natural generalizations. A coreset is a weighted subset of the data, which guarantees that models fitting the coreset will also provide a good fit for the original data set. We show that, perhaps surprisingly, Gaussian mixtures admit coresets of size independent of the size of the data set. More precisely, we prove that a weighted set of O(dk3/e2) data points suffices for computing a (1 + e)-approximation for the optimal model on the original n data points. Moreover, such coresets can be efficiently constructed in a map-reduce style computation, as well as in a streaming setting. Our results rely on a novel reduction of statistical estimation to problems in computational geometry, as well as new complexity results about mixtures of Gaussians. We empirically evaluate our algorithms on several real data sets, including a density estimation problem in the context of earthquake detection using accelerometers in mobile phones.", "citation": "Citations (86)", "departments": ["Massachusetts Institute of Technology", "California Institute of Technology", "ETH Zurich"], "authors": ["Dan Feldman.....http://dblp.org/pers/hd/f/Feldman:Dan", "Matthew Faulkner.....http://dblp.org/pers/hd/f/Faulkner:Matthew", "Andreas Krause.....http://dblp.org/pers/hd/k/Krause_0001:Andreas"], "conf": "nips", "year": "2011", "pages": 9}