{"title": "The Generalized Reparameterization Gradient.", "fields": ["score", "probabilistic logic", "gaussian", "invertible matrix", "latent variable"], "abstract": "The reparameterization gradient has become a widely used method to obtain Monte Carlo gradients to optimize the variational objective. However, this technique does not easily apply to commonly used distributions such as beta or gamma without further approximations, and most practical applications of the reparameterization gradient fit Gaussian distributions. In this paper, we introduce the generalized reparameterization gradient, a method that extends the reparameterization gradient to a wider class of variational distributions. Generalized reparameterizations use invertible transformations of the latent variables which lead to transformed distributions that weakly depend on the variational parameters. This results in new Monte Carlo gradients that combine reparameterization gradients and score function gradients. We demonstrate our approach on variational inference for two complex probabilistic models. The generalized reparameterization is effective: even a single sample from the variational distribution is enough to obtain a low-variance gradient.", "citation": "Citations (21)", "departments": ["Columbia University", "Athens University of Economics and Business", "Columbia University"], "authors": ["Francisco J. R. Ruiz.....http://dblp.org/pers/hd/r/Ruiz:Francisco_J=_R=", "Michalis K. Titsias.....http://dblp.org/pers/hd/t/Titsias:Michalis_K=", "David M. Blei.....http://dblp.org/pers/hd/b/Blei:David_M="], "conf": "nips", "year": "2016", "pages": 9}