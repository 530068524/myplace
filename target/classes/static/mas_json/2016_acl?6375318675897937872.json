{"title": "N-gram language models for massively parallel devices.", "fields": ["memory footprint", "massively parallel", "bottleneck", "central processing unit", "data parallelism"], "abstract": "For many applications, the query speed of N -gram language models is a computational bottleneck. Although massively parallel hardware like GPUs offer a potential solution to this bottleneck, exploiting this hardware requires a careful rethinking of basic algorithms and data structures. We present the first language model designed for such hardware, using B-trees to maximize data parallelism and minimize memory footprint and latency. Compared with a single-threaded instance of KenLM (Heafield, 2011), a highly optimized CPUbased language model, our GPU implementation produces identical results with a smaller memory footprint and a sixfold increase in throughput on a batch query task. When we saturate both devices, the GPU delivers nearly twice the throughput per hardware dollar even when the CPU implementation uses faster data structures. Our implementation is freely available at", "citation": "Citations (1)", "year": "2016", "departments": ["School of Informatics"], "conf": "acl", "authors": ["Nikolay Bogoychev.....http://dblp.org/pers/hd/b/Bogoychev:Nikolay", "Adam Lopez.....http://dblp.org/pers/hd/l/Lopez:Adam"], "pages": -1}