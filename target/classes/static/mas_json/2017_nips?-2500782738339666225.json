{"title": "Stochastic Mirror Descent in Variationally Coherent Optimization Problems.", "fields": ["quasiconvex function", "special case", "stochastic gradient descent", "solution set", "subclass"], "abstract": "In this paper, we examine a class of non-convex stochastic optimization problems which we call variationally coherent, and which properly includes pseudo-/quasiconvex and star-convex optimization problems. To solve such problems, we focus on the widely used stochastic mirror descent (SMD) family of algorithms (which contains stochastic gradient descent as a special case), and we show that the last iterate of SMD converges to the problem\u2019s solution set with probability 1. This result contributes to the landscape of non-convex stochastic optimization by clarifying that neither pseudo-/quasi-convexity nor star-convexity is essential for (almost sure) global convergence; rather, variational coherence, a much weaker requirement, suffices. Characterization of convergence rates for the subclass of strongly variationally coherent optimization problems as well as simulation results are also presented.", "citation": "Citations (2)", "year": "2017", "departments": ["Stanford University", "Centre national de la recherche scientifique", "Stanford University", "Stanford University", "Stanford University"], "conf": "nips", "authors": ["Zhengyuan Zhou.....http://dblp.org/pers/hd/z/Zhou:Zhengyuan", "Panayotis Mertikopoulos.....http://dblp.org/pers/hd/m/Mertikopoulos:Panayotis", "Nicholas Bambos.....http://dblp.org/pers/hd/b/Bambos:Nicholas", "Stephen P. Boyd.....http://dblp.org/pers/hd/b/Boyd:Stephen_P=", "Peter W. Glynn.....http://dblp.org/pers/hd/g/Glynn:Peter_W="], "pages": 10}