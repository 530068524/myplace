{"title": "Round compression for parallel matching algorithms.", "fields": ["spark", "graph partition", "dense graph", "binary logarithm", "matching"], "abstract": "For over a decade now we have been witnessing the success of  massive parallel computation  (MPC) frameworks, such as MapReduce, Hadoop, Dryad, or Spark. One of the reasons for their success is the fact that these frameworks are able to accurately capture the nature of large-scale computation. In particular, compared to the classic distributed algorithms or PRAM models, these frameworks allow for much more local computation. The fundamental question that arises in this context is though: can we leverage this additional power to obtain even faster parallel algorithms?    A prominent example here is the  maximum matching  problem\u2014one of the most classic graph problems. It is well known that in the PRAM model one can compute a 2-approximate maximum matching in  O (log n ) rounds. However, the exact complexity of this problem in the MPC framework is still far from understood. Lattanzi et al.\u00a0(SPAA 2011) showed that if each machine has  n  1+\u03a9(1)  memory, this problem can also be solved 2-approximately in a constant number of rounds. These techniques, as well as the approaches developed in the follow up work, seem though to get stuck in a fundamental way at roughly  O (log n ) rounds once we enter the (at most) near-linear memory regime. It is thus entirely possible that in this regime, which captures in particular the case of sparse graph computations, the best MPC round complexity matches what one can already get in the PRAM model, without the need to take advantage of the extra local computation power.    In this paper, we finally refute that possibility. That is, we break the above  O (log n ) round complexity bound even in the case of  slightly sublinear  memory per machine. In fact, our improvement here is  almost exponential : we are able to deliver a (2+\u0454)-approximate maximum matching, for any fixed constant \u0454>0, in  O ((loglog n ) 2 ) rounds.    To establish our result we need to deviate from the previous work in two important ways that are crucial for exploiting the power of the MPC model, as compared to the PRAM model. Firstly, we use  vertex\u2013based  graph partitioning, instead of the edge\u2013based approaches that were utilized so far. Secondly, we develop a technique of  round compression . This technique enables one to take a (distributed) algorithm that computes an  O (1)-approximation of maximum matching in  O (log n )  independent  PRAM phases and implement a super-constant number of these phases in only a constant number of MPC rounds.", "citation": "Citations (4)", "year": "2018", "departments": ["University of Warwick", "Google", "Massachusetts Institute of Technology", "\u00c9cole Polytechnique F\u00e9d\u00e9rale de Lausanne", "IBM"], "conf": "stoc", "authors": ["Artur Czumaj.....http://dblp.org/pers/hd/c/Czumaj:Artur", "Jakub Lacki.....http://dblp.org/pers/hd/l/Lacki:Jakub", "Aleksander Madry.....http://dblp.org/pers/hd/m/Madry:Aleksander", "Slobodan Mitrovic.....http://dblp.org/pers/hd/m/Mitrovic:Slobodan", "Krzysztof Onak.....http://dblp.org/pers/hd/o/Onak:Krzysztof", "Piotr Sankowski.....http://dblp.org/pers/hd/s/Sankowski:Piotr"], "pages": 14}