{"title": "Knowing where and when to look in a time-critical multimodal dual task.", "fields": ["eye tracking", "peripheral", "radar display", "auditory display", "human multitasking"], "abstract": "Human-computer systems intended for time-critical multitasking need to be designed with an understanding of how humans can coordinate and interleave perceptual, memory, and motor processes. This paper presents human performance data for a highly-practiced time-critical dual task. In the first of the two interleaved tasks, participants tracked a target with a joystick. In the second, participants keyed-in responses to objects moving across a radar display. Task manipulations include the peripheral visibility of the secondary display (visible or not) and\u00a0the presence or absence of auditory cues to assist with the radar task. Eye movement analyses reveal extensive coordination and overlapping of human information processes and the extent to which task manipulations helped or hindered dual task performance. For example, auditory cues helped only a little when the secondary display was peripherally visible, but they helped a lot when it was not peripherally visible.", "citation": "Citations (16)", "departments": ["University of Oregon", "University of Oregon", "University of Oregon"], "authors": ["Anthony J. Hornof.....http://dblp.org/pers/hd/h/Hornof:Anthony_J=", "Yunfeng Zhang.....http://dblp.org/pers/hd/z/Zhang:Yunfeng", "Tim Halverson.....http://dblp.org/pers/hd/h/Halverson:Tim"], "conf": "chi", "year": "2010", "pages": 10}