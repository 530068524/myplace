{"title": "Approximate Dynamic Programming for Two-Player Zero-Sum Markov Games.", "fields": ["operator", "bellman equation", "horizon", "markov decision process", "propagation of uncertainty"], "abstract": "This paper provides an analysis of error propagation in Approximate Dynamic Programming applied to zero-sum two-player Stochastic Games. We provide a novel and unified error propagation analysis in Lp-norm of three well-known algorithms adapted to Stochastic Games (namely Approximate Value Iteration, Approximate Policy Iteration and Approximate Generalized Policy Iteration). We show that we can achieve a stationary policy which is 2\u03b3e+e\u2032/(1-\u03b3)2 -optimal, where e is the value function approximation error and e\u2032 is the approximate greedy operator error. In addition, we provide a practical algorithm (AGPI-Q) to solve infinite horizon \u03b3-discounted two-player zero-sum Stochastic Games in a batch setting. It is an extension of the Fitted-Q algorithm (which solves Markov Decisions Processes from data) and can be non-parametric. Finally, we demonstrate experimentally the performance of AGPI-Q on a simultaneous two-player game, namely Alesia.", "citation": "Citations (10)", "year": "2015", "departments": ["university of lille", "French Institute for Research in Computer Science and Automation", "university of lille", "Institut Universitaire de France", "university of lille"], "conf": "icml", "authors": ["Julien P\u00e9rolat.....http://dblp.org/pers/hd/p/P=eacute=rolat:Julien", "Bruno Scherrer.....http://dblp.org/pers/hd/s/Scherrer:Bruno", "Bilal Piot.....http://dblp.org/pers/hd/p/Piot:Bilal", "Olivier Pietquin.....http://dblp.org/pers/hd/p/Pietquin:Olivier"], "pages": 9}