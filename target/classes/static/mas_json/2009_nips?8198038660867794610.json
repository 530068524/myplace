{"title": "Learning to Explore and Exploit in POMDPs.", "fields": ["oracle", "observable", "exploit", "error driven learning", "active learning"], "abstract": "A fundamental objective in reinforcement learning is the maintenance of a proper balance between exploration and exploitation. This problem becomes more challenging when the agent can only partially observe the states of its environment. In this paper we propose a dual-policy method for jointly learning the agent behavior and the balance between exploration exploitation, in partially observable environments. The method subsumes traditional exploration, in which the agent takes actions to gather information about the environment, and active learning, in which the agent queries an oracle for optimal actions (with an associated cost for employing the oracle). The form of the employed exploration is dictated by the specific problem. Theoretical guarantees are provided concerning the optimality of the balancing of exploration and exploitation. The effectiveness of the method is demonstrated by experimental results on benchmark problems.", "citation": "Citations (13)", "year": "2009", "departments": ["Duke University", "Duke University", "Xuejun Liao"], "conf": "nips", "authors": ["Chenghui Cai.....http://dblp.org/pers/hd/c/Cai:Chenghui", "Xuejun Liao.....http://dblp.org/pers/hd/l/Liao:Xuejun", "Lawrence Carin.....http://dblp.org/pers/hd/c/Carin:Lawrence"], "pages": 9}