{"title": "Tree quantization for large-scale similarity search and classification.", "fields": ["lossy compression", "vector quantization", "scale invariant feature transform", "learning vector quantization", "nearest neighbor search"], "abstract": "We propose a new vector encoding scheme (tree quantization) that obtains lossy compact codes for high-dimensional vectors via tree-based dynamic programming. Similarly to several previous schemes such as product quantization, these codes correspond to codeword numbers within multiple codebooks. We propose an integer programming-based optimization that jointly recovers the coding tree structure and the codebooks by minimizing the compression error on a training dataset. In the experiments with diverse visual descriptors (SIFT, neural codes, Fisher vectors), tree quantization is shown to combine fast encoding and state-of-the-art accuracy in terms of the compression error, the retrieval performance, and the image classification error.", "citation": "Citations (54)", "year": "2015", "departments": ["National Research University \u2013 Higher School of Economics", "Skolkovo Institute of Science and Technology"], "conf": "cvpr", "authors": ["Artem Babenko.....http://dblp.org/pers/hd/b/Babenko:Artem", "Victor S. Lempitsky.....http://dblp.org/pers/hd/l/Lempitsky:Victor_S="], "pages": 9}