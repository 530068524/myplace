{"title": "Learning Transferrable Representations for Unsupervised Domain Adaptation.", "fields": ["overfitting", "deep learning", "supervised learning", "test data", "hyperparameter"], "abstract": "Supervised learning with large scale labelled datasets and deep layered models has caused a paradigm shift in diverse areas in learning and recognition. However, this approach still suffers from generalization issues under the presence of a domain shift between the training and the test data distribution. Since unsupervised domain adaptation algorithms directly address this domain shift problem between a labelled source dataset and an unlabelled target dataset, recent papers have shown promising results by fine-tuning the networks with domain adaptation loss functions which try to align the mismatch between the training and testing data distributions. Nevertheless, these recent deep learning based domain adaptation approaches still suffer from issues such as high sensitivity to the gradient reversal hyperparameters and overfitting during the fine-tuning stage. In this paper, we propose a unified deep learning framework where the representation, cross domain transformation, and target label inference are all jointly optimized in an end-to-end fashion for unsupervised domain adaptation. Our experiments show that the proposed method significantly outperforms state-of-the-art algorithms in both object recognition and digit classification experiments by a large margin. We will make our learned models as well as the source code available immediately upon acceptance.", "citation": "Citations (57)", "departments": ["Stanford University", "University of California, Berkeley", "Cornell University", "Stanford University"], "authors": ["Ozan Sener.....http://dblp.org/pers/hd/s/Sener:Ozan", "Hyun Oh Song.....http://dblp.org/pers/hd/s/Song:Hyun_Oh", "Ashutosh Saxena.....http://dblp.org/pers/hd/s/Saxena:Ashutosh", "Silvio Savarese.....http://dblp.org/pers/hd/s/Savarese:Silvio"], "conf": "nips", "year": "2016", "pages": 9}