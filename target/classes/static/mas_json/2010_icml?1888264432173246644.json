{"title": "Internal Rewards Mitigate Agent Boundedness.", "fields": ["bounded function", "machine learning", "intelligent agent", "artificial intelligence", "reinforcement learning"], "abstract": "Reinforcement learning (RL) research typically develops algorithms for helping an RL agent best achieve its goals\u2014however they came to be defined\u2014while ignoring the relationship of those goals to the goals of the agent designer. We extend agent design to include the meta-optimization problem of selecting internal agent goals (rewards) which optimize the designer's goals. Our claim is that well-designed internal rewards can help improve the performance of RL agents which are computationally bounded in some way (as practical agents are). We present a formal framework for understanding both bounded agents and the meta-optimization problem, and we empirically demonstrate several instances of common agent bounds being mitigated by general internal reward functions.", "citation": "Citations (28)", "departments": ["University of Michigan", "University of Michigan", "University of Michigan"], "authors": ["Jonathan Sorg.....http://dblp.org/pers/hd/s/Sorg:Jonathan", "Satinder P. Singh.....http://dblp.org/pers/hd/s/Singh:Satinder_P=", "Richard L. Lewis.....http://dblp.org/pers/hd/l/Lewis:Richard_L="], "conf": "icml", "year": "2010", "pages": 8}