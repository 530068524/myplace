{"title": "A fast and efficient algorithm for low-rank approximation of a matrix.", "fields": ["matrix norm", "low rank approximation", "orthogonal matrix", "block matrix", "square root of a 2 by 2 matrix"], "abstract": "The low-rank matrix approximation problem involves finding of a rank k version of a m x n matrix A, labeled A k , such that A k  is as \"close\" as possible to the best SVD approximation version of A at the same rank level. Previous approaches approximate matrix A by non-uniformly adaptive sampling some columns (or rows) of A, hoping that this subset of columns contain enough information about A. The sub-matrix is then used for the approximation process. However, these approaches are often computationally intensive due to the complexity in the adaptive sampling. In this paper, we propose a fast and efficient algorithm which at first pre-processes matrix A in order to spread out information (energy) of every columns (or rows) of A, then randomly selects some of its columns (or rows). Finally, a rank-k approximation is generated from the row space of these selected sets. The preprocessing step is performed by uniformly randomizing signs of entries of A and transforming all columns of A by an orthonormal matrix F with existing fast implementation (e.g. Hadamard, FFT, DCT...). Our main contribution is summarized as follows. 1) We show that by uniformly selecting at random d rows of the preprocessed matrix with d = ( 1/\u03b7 k max {log k, log 1/\u03b2} ), we guarantee the relative Frobenius norm error approximation: (1 + \u03b7) norm{A - A k } F  with probability at least 1 - 5\u03b2. 2) With d above, we establish a spectral norm error approximation: (2 + \u221a2m/d) norm{A - A k } 2  with probability at least 1 - 2\u03b2. 3) The algorithm requires 2 passes over the data and runs in time (mn log d + (m+n) d 2 ) which, as far as the best of our knowledge, is the fastest algorithm when the matrix A is dense. 4) As a bonus, applying this framework to the well-known least square approximation problem min norm{A x - b} where A \u2208 R m x r , we show that by randomly choosing d = (1/\u03b7 \u03b3 r log m), the approximation solution is proportional to the optimal one with a factor of \u03b7 and with extremely high probability, (1 - 6 m -\u03b3 ), say.", "citation": "Citations (64)", "departments": ["Johns Hopkins University", "Johns Hopkins University", "Johns Hopkins University"], "authors": ["Nam H. Nguyen.....http://dblp.org/pers/hd/n/Nguyen:Nam_H=", "Thong T. Do.....http://dblp.org/pers/hd/d/Do:Thong_T=", "Trac D. Tran.....http://dblp.org/pers/hd/t/Tran:Trac_D="], "conf": "stoc", "year": "2009", "pages": 10}