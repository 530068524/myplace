{"title": "Classes of Multiagent Q-learning Dynamics with epsilon-greedy Exploration.", "fields": ["convergence", "dynamical systems theory", "gradient descent", "idealization", "q learning"], "abstract": "Q-learning in single-agent environments is known to converge in the limit given sufficient exploration. The same algorithm has been applied, with some success, in multi-agent environments, where traditional analysis techniques break down. Using established dynamical systems methods, we derive and study an idealization of Q-learning in 2-player 2-action repeated general-sum games. In particular, we address the discontinuous case of e-greedy exploration and use it as a proxy for value-based algorithms to highlight a contrast with existing results in policy search. Analogously to previous results for gradient ascent algorithms, we provide a complete catalog of the convergence behavior of the e-greedy Q-learning algorithm by introducing new subclasses of these games. We identify two subclasses of Prisoner's Dilemma-like games where the application of Q-learning with e-greedy exploration results in higher-than-Nash average payoffs for some initial conditions.", "citation": "Citations (25)", "departments": ["Rutgers University", "Rutgers University", "Rutgers University"], "authors": ["Michael Wunder.....http://dblp.org/pers/hd/w/Wunder:Michael", "Michael L. Littman.....http://dblp.org/pers/hd/l/Littman:Michael_L=", "Monica Babes.....http://dblp.org/pers/hd/b/Babes:Monica"], "conf": "icml", "year": "2010", "pages": 8}