{"title": "Learning Stochastic Feedforward Neural Networks.", "fields": ["feedforward neural network", "boltzmann machine", "importance sampling", "perceptron", "multimodal distribution"], "abstract": "Multilayer perceptrons (MLPs) or neural networks are popular models used for nonlinear regression and classification tasks. As regressors, MLPs model the conditional distribution of the predictor variables Y given the input variables X. However, this predictive distribution is assumed to be unimodal (e.g. Gaussian). For tasks involving structured prediction, the conditional distribution should be multi-modal, resulting in one-to-many mappings. By using stochastic hidden variables rather than deterministic ones, Sigmoid Belief Nets (SBNs) can induce a rich multimodal distribution in the output space. However, previously proposed learning algorithms for SBNs are not efficient and unsuitable for modeling real-valued data. In this paper, we propose a stochastic feedforward network with hidden layers composed of both deterministic and stochastic variables. A new Generalized EM training procedure using importance sampling allows us to efficiently learn complicated conditional distributions. Our model achieves superior performance on synthetic and facial expressions datasets compared to conditional Restricted Boltzmann Machines and Mixture Density Networks. In addition, the latent features of our model improves classification and can learn to generate colorful textures of objects.", "citation": "Citations (76)", "departments": ["University of Toronto", "University of Toronto"], "authors": ["Yichuan Tang.....http://dblp.org/pers/hd/t/Tang:Yichuan", "Ruslan Salakhutdinov.....http://dblp.org/pers/hd/s/Salakhutdinov:Ruslan"], "conf": "nips", "year": "2013", "pages": 9}