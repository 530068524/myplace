{"title": "Near-Optimal Smoothing of Structured Conditional Probability Matrices.", "fields": ["additive smoothing", "matrix decomposition", "bigram", "conditional probability", "smoothing"], "abstract": "Utilizing the structure of a probabilistic model can significantly increase its learning speed. Motivated by several recent applications, in particular bigram models in language processing, we consider learning low-rank conditional probability matrices under expected KL-risk. This choice makes smoothing, that is the careful handling of low-probability elements, paramount. We derive an iterative algorithm that extends classical non-negative matrix factorization to naturally incorporate additive smoothing and prove that it converges to the stationary points of a penalized empirical risk. We then derive sample-complexity bounds for the global minimizer of the penalized risk and show that it is within a small factor of the optimal sample complexity. This framework generalizes to more sophisticated smoothing techniques, including absolute-discounting.", "citation": "Citations (1)", "departments": ["University of California, San Diego", "Massachusetts Institute of Technology", "University of California, San Diego"], "authors": ["Moein Falahatgar.....http://dblp.org/pers/hd/f/Falahatgar:Moein", "Mesrob I. Ohannessian.....http://dblp.org/pers/hd/o/Ohannessian:Mesrob_I=", "Alon Orlitsky.....http://dblp.org/pers/hd/o/Orlitsky:Alon"], "conf": "nips", "year": "2016", "pages": 9}