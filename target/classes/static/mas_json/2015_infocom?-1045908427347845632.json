{"title": "Around the web in six weeks: Documenting a large-scale crawl.", "fields": ["web crawler", "search engine", "focused crawler", "scalability", "data structure"], "abstract": "Exponential growth of the web continues to present challenges to the design and scalability of web crawlers. Our previous work on a high-performance platform called IRLbot [28] led to the development of new algorithms for realtime URL manipulation, domain ranking, and budgeting, which were tested in a 6.3B-page crawl. Since very little is known about the crawl itself, our goal in this paper is to undertake an extensive measurement study of the collected dataset and document its crawl dynamics. We also propose a framework for modeling the scaling rate of various data structures as crawl size goes to infinity and offer a methodology for comparing crawl coverage to that of commercial search engines.", "citation": "Citations (2)", "departments": ["Texas A&M University", "Texas A&M University", "Texas A&M University", "Texas A&M University"], "authors": ["Sarker Tanzir Ahmed.....http://dblp.org/pers/hd/a/Ahmed:Sarker_Tanzir", "Clint Sparkman.....http://dblp.org/pers/hd/s/Sparkman:Clint", "Hsin-Tsang Lee.....http://dblp.org/pers/hd/l/Lee:Hsin=Tsang", "Dmitri Loguinov.....http://dblp.org/pers/hd/l/Loguinov:Dmitri"], "conf": "infocom", "year": "2015", "pages": 9}