{"title": "Bounding Performance Loss in Approximate MDP Homomorphisms.", "fields": ["heuristics", "homomorphism", "markov decision process", "bisimulation", "bellman equation"], "abstract": "We define a metric for measuring behavior similarity between states in a Markov decision process (MDP), which takes action similarity into account. We show that the kernel of our metric corresponds exactly to the classes of states defined by MDP homomorphisms (Ravindran & Barto, 2003). We prove that the difference in the optimal value function of different states can be upper-bounded by the value of this metric, and that the bound is tighter than previous bounds provided by bisimulation metrics (Ferns et al. 2004, 2005). Our results hold both for discrete and for continuous actions. We provide an algorithm for constructing approximate homomorphisms, by using this metric to identify states that can be grouped together, as well as actions that can be matched. Previous research on this topic is based mainly on heuristics.", "citation": "Citations (19)", "year": "2008", "departments": ["McGill University", "University of Toronto", "McGill University", "McGill University"], "conf": "nips", "authors": ["Jonathan Taylor.....http://dblp.org/pers/hd/t/Taylor:Jonathan", "Doina Precup.....http://dblp.org/pers/hd/p/Precup:Doina", "Prakash Panangaden.....http://dblp.org/pers/hd/p/Panangaden:Prakash"], "pages": 8}