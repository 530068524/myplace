{"title": "Nonparametric Return Distribution Approximation for Reinforcement Learning.", "fields": ["recursion", "bellman equation", "estimator", "expected return", "expected shortfall"], "abstract": "Standard Reinforcement Learning (RL) aims to optimize decision-making rules in terms of the expected return. However, especially for risk-management purposes, other criteria such as the expected shortfall are sometimes preferred. Here, we describe a method of approximating the distribution of returns, which allows us to derive various kinds of information about the returns. We first show that the Bellman equation, which is a recursive formula for the expected return, can be extended to the cumulative return distribution. Then we derive a nonparametric return distribution estimator with particle smoothing based on this extended Bellman equation. A key aspect of the proposed algorithm is to represent the recursion relation in the extended Bellman equation by a simple replacement procedure of particles associated with a state by using those of the successor state. We show that our algorithm leads to a risk-sensitive RL paradigm. The usefulness of the proposed approach is demonstrated through numerical experiments.", "citation": "Citations (13)", "departments": ["IBM", "Tokyo Institute of Technology", "University of Tokyo", "Tokyo Institute of Technology", "Kyoto University"], "authors": ["Tetsuro Morimura.....http://dblp.org/pers/hd/m/Morimura:Tetsuro", "Masashi Sugiyama.....http://dblp.org/pers/hd/s/Sugiyama:Masashi", "Hisashi Kashima.....http://dblp.org/pers/hd/k/Kashima:Hisashi", "Hirotaka Hachiya.....http://dblp.org/pers/hd/h/Hachiya:Hirotaka", "Toshiyuki Tanaka.....http://dblp.org/pers/hd/t/Tanaka:Toshiyuki"], "conf": "icml", "year": "2010", "pages": 8}