{"title": "Large-Scale Syntactic Language Modeling with Treelets.", "fields": ["perplexity", "discriminative model", "grammaticality", "fluency", "machine translation"], "abstract": "We propose a simple generative, syntactic language model that conditions on overlapping windows of tree context (or treelets) in the same way that n-gram language models condition on overlapping windows of linear context. We estimate the parameters of our model by collecting counts from automatically parsed text using standard n-gram language model estimation techniques, allowing us to train a model on over one billion tokens of data using a single machine in a matter of hours. We evaluate on perplexity and a range of grammaticality tasks, and find that we perform as well or better than n-gram models and other generative baselines. Our model even competes with state-of-the-art discriminative models hand-designed for the grammaticality tasks, despite training on positive data alone. We also show fluency improvements in a preliminary machine translation experiment.", "citation": "Citations (17)", "year": "2012", "departments": ["University of California, Berkeley", "University of California, Berkeley"], "conf": "acl", "authors": ["Adam Pauls.....http://dblp.org/pers/hd/p/Pauls:Adam", "Dan Klein.....http://dblp.org/pers/hd/k/Klein:Dan"], "pages": 10}