{"title": "Large scale manifold transduction.", "fields": ["transduction", "manifold", "network architecture", "stochastic gradient descent", "word error rate"], "abstract": "We show how the regularizer of Transductive Support Vector Machines (TSVM) can be trained by stochastic gradient descent for linear models and multi-layer architectures. The resulting methods can be trained  online , have vastly superior training and testing speed to existing TSVM algorithms, can encode prior knowledge in the network architecture, and obtain competitive error rates. We then go on to propose a natural generalization of the TSVM loss function that takes into account neighborhood and manifold information directly, unifying the two-stage Low Density Separation method into a single criterion, and leading to state-of-the-art results.", "citation": "Citations (37)", "year": "2008", "departments": ["\u00c9cole Polytechnique F\u00e9d\u00e9rale de Lausanne", "Princeton University", "New York University", "Princeton University"], "conf": "icml", "authors": ["Michael Karlen.....http://dblp.org/pers/hd/k/Karlen:Michael", "Jason Weston.....http://dblp.org/pers/hd/w/Weston:Jason", "Ayse Erkan.....http://dblp.org/pers/hd/e/Erkan:Ayse", "Ronan Collobert.....http://dblp.org/pers/hd/c/Collobert:Ronan"], "pages": 8}