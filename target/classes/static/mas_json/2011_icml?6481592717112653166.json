{"title": "Large-Scale Learning of Embeddings with Reconstruction Sampling.", "fields": ["vocabulary", "language model", "bias of an estimator", "sparse matrix", "speedup"], "abstract": "In this paper, we present a novel method to speed up the learning of embeddings for large-scale learning tasks involving very sparse data, as is typically the case for Natural Language Processing tasks. Our speed-up method has been developed in the context of Denoising Auto-encoders, which are trained in a purely unsupervised way to capture the input distribution, and learn embeddings for words and text similar to earlier neural language models. The main contribution is a new method to approximate reconstruction error by a sampling procedure. We show how this approximation can be made to obtain an unbiased estimator of the training criterion, and we show how it can be leveraged to make learning much more computationally efficient. We demonstrate the effectiveness of this method on the Amazon and RCV1 NLP datasets. Instead of reducing vocabulary size to make learning practical, our method allows us to train using very large vocabularies. In particular, reconstruction sampling requires 22x less training time on the full Amazon dataset.", "citation": "Citations (27)", "year": "2011", "departments": ["Universit\u00e9 de Montr\u00e9al", "Universit\u00e9 de Montr\u00e9al", "Universit\u00e9 de Montr\u00e9al"], "conf": "icml", "authors": ["Yann Dauphin.....http://dblp.org/pers/hd/d/Dauphin:Yann", "Xavier Glorot.....http://dblp.org/pers/hd/g/Glorot:Xavier", "Yoshua Bengio.....http://dblp.org/pers/hd/b/Bengio:Yoshua"], "pages": 8}