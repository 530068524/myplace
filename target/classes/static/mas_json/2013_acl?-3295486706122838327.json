{"title": "Smoothed marginal distribution constraints for language modeling.", "fields": ["perplexity", "marginal distribution", "smoothing", "word error rate", "language model"], "abstract": "We present an algorithm for re-estimating parameters of backoff n-gram language models so as to preserve given marginal distributions, along the lines of wellknown Kneser-Ney (1995) smoothing. Unlike Kneser-Ney, our approach is designed to be applied to any given smoothed backoff model, including models that have already been heavily pruned. As a result, the algorithm avoids issues observed when pruning Kneser-Ney models (Siivola et al., 2007; Chelba et al., 2010), while retaining the benefits of such marginal distribution constraints. We present experimental results for heavily pruned backoff ngram models, and demonstrate perplexity and word error rate reductions when used with various baseline smoothing methods. An open-source version of the algorithm has been released as part of the OpenGrm ngram library. 1", "citation": "Citations (8)", "year": "2013", "departments": ["Oregon Health & Science University", "Google", "Google"], "conf": "acl", "authors": ["Brian Roark.....http://dblp.org/pers/hd/r/Roark:Brian", "Cyril Allauzen.....http://dblp.org/pers/hd/a/Allauzen:Cyril", "Michael Riley.....http://dblp.org/pers/hd/r/Riley:Michael"], "pages": 10}