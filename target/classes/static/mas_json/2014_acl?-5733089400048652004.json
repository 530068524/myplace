{"title": "Semantic Parsing via Paraphrasing.", "fields": ["utterance", "top down parsing", "paraphrase", "bottom up parsing", "s attributed grammar"], "abstract": "A central challenge in semantic parsing is handling the myriad ways in which knowledge base predicates can be expressed. Traditionally, semantic parsers are trained primarily from text paired with knowledge base information. Our goal is to exploit the much larger amounts of raw text not tied to any knowledge base. In this paper, we turn semantic parsing on its head. Given an input utterance, we first use a simple method to deterministically generate a set of candidate logical forms with a canonical realization in natural language for each. Then, we use a paraphrase model to choose the realization that best paraphrases the input, and output the corresponding logical form. We present two simple paraphrase models, an association model and a vector space model, and train them jointly from question-answer pairs. Our system PARASEMPRE improves stateof-the-art accuracies on two recently released question-answering datasets.", "citation": "Citations (266)", "year": "2014", "departments": ["Stanford University", "Stanford University"], "conf": "acl", "authors": ["Jonathan Berant.....http://dblp.org/pers/hd/b/Berant:Jonathan", "Percy Liang.....http://dblp.org/pers/hd/l/Liang:Percy"], "pages": 11}