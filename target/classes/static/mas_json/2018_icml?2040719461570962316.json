{"title": "Analyzing the Robustness of Nearest Neighbors to Adversarial Examples.", "fields": ["robustness", "long line", "k nearest neighbors algorithm", "adversary", "adversarial system"], "abstract": "Motivated by applications such as autonomous vehicles, test-time attacks via adversarial examples have received a great deal of recent attention. In this setting, an adversary is capable of making queries to a classifier, and perturbs a test example by a small amount in order to force the classifier to report an incorrect label. While a long line of work has explored a number of attacks, not many reliable defenses are known, and there is an overall lack of general understanding about the foundations of designing machine learning algorithms robust to adversarial examples. \nIn this paper, we take a step towards addressing this challenging question by introducing a new theoretical framework, analogous to bias-variance theory, which we can use to tease out the causes of vulnerability. We apply our framework to a simple classification algorithm: nearest neighbors, and analyze its robustness to adversarial examples. Motivated by our analysis, we propose a modified version of the nearest neighbor algorithm, and demonstrate both theoretically and empirically that it has superior robustness to standard nearest neighbors.", "citation": "Citations (8)", "departments": ["University of California, San Diego", "University of Wisconsin-Madison", "University of California, San Diego"], "authors": ["Yizhen Wang.....http://dblp.org/pers/hd/w/Wang:Yizhen", "Somesh Jha.....http://dblp.org/pers/hd/j/Jha:Somesh", "Kamalika Chaudhuri.....http://dblp.org/pers/hd/c/Chaudhuri:Kamalika"], "conf": "icml", "year": "2018", "pages": 10}