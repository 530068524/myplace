{"title": "Near-Optimal Bounds for Cross-Validation via Loss Stability.", "fields": ["variance reduction", "cross validation", "mathematical optimization", "statistics", "word error rate"], "abstract": "Multi-fold cross-validation is an established practice to estimate the error rate of a learning algorithm. Quantifying the variance reduction gains due to cross-validation has been challenging due to the inherent correlations introduced by the folds. In this work we introduce a new and weak measure called loss stability and relate the cross-validation performance to this measure; we also establish that this relationship is near-optimal. Our work thus quantitatively improves the current best bounds on cross-validation.", "citation": "Citations (2)", "departments": ["Google", "University of California, San Diego", "Google", "University of California, San Diego"], "authors": ["Ravi Kumar.....http://dblp.org/pers/hd/k/Kumar_0001:Ravi", "Daniel Lokshtanov.....http://dblp.org/pers/hd/l/Lokshtanov:Daniel", "Sergei Vassilvitskii.....http://dblp.org/pers/hd/v/Vassilvitskii:Sergei", "Andrea Vattani.....http://dblp.org/pers/hd/v/Vattani:Andrea"], "conf": "icml", "year": "2013", "pages": 9}