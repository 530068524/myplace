{"title": "Online Gradient Boosting.", "fields": ["online machine learning", "population based incremental learning", "linear span", "gradient boosting", "brownboost"], "abstract": "We extend the theory of boosting for regression problems to the online learning setting. Generalizing from the batch setting for boosting, the notion of a weak learning algorithm is modeled as an online learning algorithm with linear loss functions that competes with a base class of regression functions, while a strong learning algorithm is an online learning algorithm with smooth convex loss functions that competes with a larger class of regression functions. Our main result is an online gradient boosting algorithm that converts a weak online learning algorithm into a strong one where the larger class of functions is the linear span of the base class. We also give a simpler boosting algorithm that converts a weak online learning algorithm into a strong one where the larger class of functions is the convex hull of the base class, and prove its optimality.", "citation": "Citations (13)", "year": "2015", "departments": ["Yahoo!", "Princeton University", "Yahoo!", "Princeton University"], "conf": "nips", "authors": ["Alina Beygelzimer.....http://dblp.org/pers/hd/b/Beygelzimer:Alina", "Elad Hazan.....http://dblp.org/pers/hd/h/Hazan:Elad", "Satyen Kale.....http://dblp.org/pers/hd/k/Kale:Satyen", "Haipeng Luo.....http://dblp.org/pers/hd/l/Luo:Haipeng"], "pages": 9}