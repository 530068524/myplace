{"title": "Online Reinforcement Learning in Stochastic Games.", "fields": ["reinforcement learning", "sample complexity", "adversary", "sublinear function", "regret"], "abstract": "We study online reinforcement learning in average-reward stochastic games (SGs). An SG models a two-player zero-sum game in a Markov environment, where state transitions and one-step payoffs are determined simultaneously by a learner and an adversary. We propose the \\textsc{UCSG} algorithm that achieves a sublinear regret compared to the game value when competing with an arbitrary opponent. This result improves previous ones under the same setting. The regret bound has a dependency on the \\textit{diameter}, which is an intrinsic value related to the mixing property of SGs. Slightly extended, \\textsc{UCSG} finds an $\\varepsilon$-maximin stationary policy with a sample complexity of $\\tilde{\\mathcal{O}}\\left(\\text{poly}(1/\\varepsilon)\\right)$, where $\\varepsilon$ is the error parameter. To the best of our knowledge, this extended result is the first in the average-reward setting. In the analysis, we develop Markov chain's perturbation bounds for mean first passage times and techniques to deal with non-stationary opponents, which may be of interest in their own right.", "citation": "Citations (1)", "year": "2017", "departments": ["Academia Sinica", "National Taiwan University", "Academia Sinica"], "conf": "nips", "authors": ["Chen-Yu Wei.....http://dblp.org/pers/hd/w/Wei:Chen=Yu", "Yi-Te Hong.....http://dblp.org/pers/hd/h/Hong:Yi=Te", "Chi-Jen Lu.....http://dblp.org/pers/hd/l/Lu:Chi=Jen"], "pages": 11}