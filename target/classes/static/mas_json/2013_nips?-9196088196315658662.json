{"title": "Scoring Workers in Crowdsourcing: How Many Control Questions are Enough?", "fields": ["crowdsourcing", "rule of thumb", "data mining", "versa", "machine learning"], "abstract": "We study the problem of estimating continuous quantities, such as prices, probabilities, and point spreads, using a crowdsourcing approach. A challenging aspect of combining the crowd's answers is that workers' reliabilities and biases are usually unknown and highly diverse. Control items with known answers can be used to evaluate workers' performance, and hence improve the combined results on the target items with unknown answers. This raises the problem of how many control items to use when the total number of items each workers can answer is limited: more control items evaluates the workers better, but leaves fewer resources for the target items that are of direct interest, and vice versa. We give theoretical results for this problem under different scenarios, and provide a simple rule of thumb for crowdsourcing practitioners. As a byproduct, we also provide theoretical analysis of the accuracy of different consensus methods.", "citation": "Citations (28)", "departments": ["University of California, Irvine", "University of California, Irvine", "University of California, Irvine"], "authors": ["Qiang Liu.....http://dblp.org/pers/hd/l/Liu_0001:Qiang", "Alexander T. Ihler.....http://dblp.org/pers/hd/i/Ihler:Alexander_T=", "Mark Steyvers.....http://dblp.org/pers/hd/s/Steyvers:Mark"], "conf": "nips", "year": "2013", "pages": 9}