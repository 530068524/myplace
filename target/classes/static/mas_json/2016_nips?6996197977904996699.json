{"title": "Global Optimality of Local Search for Low Rank Matrix Recovery.", "fields": ["maxima and minima", "low rank approximation", "stochastic gradient descent", "local search", "saddle point"], "abstract": "We show that there are no spurious local minima in the non-convex factorized parametrization of low-rank matrix recovery from incoherent linear measurements. With noisy measurements we show all local minima are very close to a global optimum. Together with a curvature bound at saddle points, this yields a polynomial time global convergence guarantee for stochastic gradient descent {\\em from random initialization}.", "citation": "Citations (119)", "departments": ["University of Texas at Austin", "Toyota Technological Institute at Chicago", "Toyota Technological Institute at Chicago"], "authors": ["Srinadh Bhojanapalli.....http://dblp.org/pers/hd/b/Bhojanapalli:Srinadh", "Behnam Neyshabur.....http://dblp.org/pers/hd/n/Neyshabur:Behnam", "Nati Srebro.....http://dblp.org/pers/hd/s/Srebro:Nati"], "conf": "nips", "year": "2016", "pages": 9}