{"title": "Learning Multiple Models via Regularized Weighting.", "fields": ["correlation clustering", "cure data clustering algorithm", "canopy clustering algorithm", "constrained clustering", "fuzzy clustering"], "abstract": "We consider the general problem of Multiple Model Learning (MML) from data, from the statistical and algorithmic perspectives; this problem includes clustering, multiple regression and subspace clustering as special cases. A common approach to solving new MML problems is to generalize Lloyd's algorithm for clustering (or Expectation-Maximization for soft clustering). However this approach is unfortunately sensitive to outliers and large noise: a single exceptional point may take over one of the models.\n\nWe propose a different general formulation that seeks for each model a distribution over data points; the weights are regularized to be sufficiently spread out. This enhances robustness by making assumptions on class balance. We further provide generalization bounds and explain how the new iterations may be computed efficiently. We demonstrate the robustness benefits of our approach with some experimental results and prove for the important case of clustering that our approach has a non-trivial breakdown point, i.e., is guaranteed to be robust to a fixed percentage of adversarial unbounded outliers.", "citation": "Citations (1)", "departments": ["Technion \u2013 Israel Institute of Technology", "Technion \u2013 Israel Institute of Technology", "National University of Singapore"], "authors": ["Daniel Vainsencher.....http://dblp.org/pers/hd/v/Vainsencher:Daniel", "Shie Mannor.....http://dblp.org/pers/hd/m/Mannor:Shie", "Huan Xu.....http://dblp.org/pers/hd/x/Xu:Huan"], "conf": "nips", "year": "2013", "pages": 9}