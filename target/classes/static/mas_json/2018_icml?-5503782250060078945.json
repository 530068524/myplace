{"title": "Autoregressive Quantile Networks for Generative Modeling.", "fields": ["quantile", "generative grammar", "inpainting", "quantile regression", "autoregressive model"], "abstract": "We introduce autoregressive implicit quantile networks (AIQN), a fundamentally different approach to generative modeling than those commonly used, that implicitly captures the distribution using quantile regression. AIQN is able to achieve superior perceptual quality and improvements in evaluation metrics, without incurring a loss of sample diversity. The method can be applied to many existing models and architectures. In this work we extend the PixelCNN model with AIQN and demonstrate results on CIFAR-10 and ImageNet using Inception score, FID, non-cherry-picked samples, and inpainting results. We consistently observe that AIQN yields a highly stable algorithm that improves perceptual quality while maintaining a highly diverse distribution.", "citation": "Not cited", "departments": ["Google", "DeepMind", "DeepMind"], "authors": ["Georg Ostrovski.....http://dblp.org/pers/hd/o/Ostrovski:Georg", "Will Dabney.....http://dblp.org/pers/hd/d/Dabney:Will", "R\u00e9mi Munos.....http://dblp.org/pers/hd/m/Munos:R=eacute=mi"], "conf": "icml", "year": "2018", "pages": 10}