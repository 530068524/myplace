{"title": "Worst-Case Linear Discriminant Analysis.", "fields": ["training set", "special case", "linear discriminant analysis", "curse of dimensionality", "dimensionality reduction"], "abstract": "Dimensionality reduction is often needed in many applications due to the high dimensionality of the data involved. In this paper, we first analyze the scatter measures used in the conventional linear discriminant analysis (LDA) model and note that the formulation is based on the average-case view. Based on this analysis, we then propose a new dimensionality reduction method called worst-case linear discriminant analysis (WLDA) by defining new between-class and within-class scatter measures. This new model adopts the worst-case view which arguably is more suitable for applications such as classification. When the number of training data points or the number of features is not very large, we relax the optimization problem involved and formulate it as a metric learning problem. Otherwise, we take a greedy approach by finding one direction of the transformation at a time. Moreover, we also analyze a special case of WLDA to show its relationship with conventional LDA. Experiments conducted on several benchmark datasets demonstrate the effectiveness of WLDA when compared with some related dimensionality reduction methods.", "citation": "Citations (19)", "departments": ["Hong Kong University of Science and Technology", "Hong Kong University of Science and Technology"], "authors": ["Yu Zhang.....http://dblp.org/pers/hd/z/Zhang_0006:Yu", "Dit-Yan Yeung.....http://dblp.org/pers/hd/y/Yeung:Dit=Yan"], "conf": "nips", "year": "2010", "pages": 9}