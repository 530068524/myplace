{"title": "Lower bounds on the robustness to adversarial perturbations.", "fields": ["data set", "convolutional neural network", "perturbation", "mnist database", "adversarial system"], "abstract": "The input-output mappings learned by state-of-the-art neural networks are significantly discontinuous. It is possible to cause a neural network used for image recognition to misclassify its input by applying very specific, hardly perceptible perturbations to the input, called adversarial perturbations. Many hypotheses have been proposed to explain the existence of these peculiar samples as well as several methods to mitigate them. A proven explanation remains elusive, however. In this work, we take steps towards a formal characterization of adversarial perturbations by deriving lower bounds on the magnitudes of perturbations necessary to change the classification of neural networks. The bounds are experimentally verified on the MNIST and CIFAR-10 data sets.", "citation": "Citations (2)", "year": "2017", "departments": ["Ghent University", "Ghent University", "Ghent University", "Ghent University"], "conf": "nips", "authors": ["Jonathan Peck.....http://dblp.org/pers/hd/p/Peck:Jonathan", "Joris Roels.....http://dblp.org/pers/hd/r/Roels:Joris", "Bart Goossens.....http://dblp.org/pers/hd/g/Goossens:Bart", "Yvan Saeys.....http://dblp.org/pers/hd/s/Saeys:Yvan"], "pages": 10}