{"title": "Spectral Representations for Convolutional Neural Networks.", "fields": ["deep learning", "curse of dimensionality", "convolutional neural network", "speedup", "pooling"], "abstract": "Discrete Fourier transforms provide a significant speedup in the computation of convolutions in deep learning. In this work, we demonstrate that, beyond its advantages for efficient computation, the spectral domain also provides a powerful representation in which to model and train convolutional neural networks (CNNs). We employ spectral representations to introduce a number of innovations to CNN design. First, we propose spectral pooling, which performs dimensionality reduction by truncating the representation in the frequency domain. This approach preserves considerably more information per parameter than other pooling strategies and enables flexibility in the choice of pooling output dimensionality. This representation also enables a new form of stochastic regularization by randomized modification of resolution. We show that these methods achieve competitive results on classification and approximation tasks, without using any dropout or max-pooling.\n\nFinally, we demonstrate the effectiveness of complex-coefficient spectral parameterization of convolutional filters. While this leaves the underlying model unchanged, it results in a representation that greatly facilitates optimization. We observe on a variety of popular CNN configurations that this leads to significantly faster convergence during training.", "citation": "Citations (73)", "year": "2015", "departments": ["Massachusetts Institute of Technology", "Harvard University", "Harvard University"], "conf": "nips", "authors": ["Oren Rippel.....http://dblp.org/pers/hd/r/Rippel:Oren", "Jasper Snoek.....http://dblp.org/pers/hd/s/Snoek:Jasper", "Ryan P. Adams.....http://dblp.org/pers/hd/a/Adams:Ryan_P="], "pages": 9}