{"title": "Learning to Learn without Gradient Descent by Gradient Descent.", "fields": ["bayesian optimization", "gradient descent", "recurrent neural network", "gaussian process", "global optimization"], "abstract": "We learn recurrent neural network optimizers trained on simple synthetic functions by gradient descent. We show that these learned optimizers exhibit a remarkable degree of transfer in that they can be used to efficiently optimize a broad range of derivative-free black-box functions, including Gaussian process bandits, simple control objectives, global optimization benchmarks and hyper-parameter tuning tasks. Up to the training horizon, the learned optimizers learn to trade-off exploration and exploitation, and compare favourably with heavily engineered Bayesian optimization packages for hyper-parameter tuning.", "citation": "Citations (15)", "year": "2017", "departments": ["University of Cambridge", "Google", "Dalhousie University", "Google"], "conf": "icml", "authors": ["Yutian Chen.....http://dblp.org/pers/hd/c/Chen:Yutian", "Matthew W. Hoffman.....http://dblp.org/pers/hd/h/Hoffman:Matthew_W=", "Sergio Gomez Colmenarejo.....http://dblp.org/pers/hd/c/Colmenarejo:Sergio_Gomez", "Misha Denil.....http://dblp.org/pers/hd/d/Denil:Misha", "Timothy P. Lillicrap.....http://dblp.org/pers/hd/l/Lillicrap:Timothy_P=", "Matthew Botvinick.....http://dblp.org/pers/hd/b/Botvinick:Matthew", "Nando de Freitas.....http://dblp.org/pers/hd/f/Freitas:Nando_de"], "pages": 9}