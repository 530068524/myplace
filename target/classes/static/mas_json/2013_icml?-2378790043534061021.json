{"title": "Gated Autoencoders with Tied Input Weights.", "fields": ["deep learning", "pose", "semantic interpretation", "linear subspace", "encoder"], "abstract": "The semantic interpretation of images is one of the core applications of deep learning. Several techniques have been recently proposed to model the relation between two images, with application to pose estimation, action recognition or invariant object recognition. Among these techniques, higher-order Boltzmann machines or relational autoencoders consider projections of the images on different subspaces and intermediate layers act as transformation specific detectors. In this work, we extend the mathematical study of (Memisevic, 2012b) to show that it is possible to use a unique projection for both images in a way that turns intermediate layers as spectrum encoders of transformations. We show that this results in networks that are easier to tune and have greater generalization capabilities.", "citation": "Citations (18)", "departments": ["Pierre-and-Marie-Curie University", "Pierre-and-Marie-Curie University"], "authors": ["Alain Droniou.....http://dblp.org/pers/hd/d/Droniou:Alain", "Olivier Sigaud.....http://dblp.org/pers/hd/s/Sigaud:Olivier"], "conf": "icml", "year": "2013", "pages": 9}