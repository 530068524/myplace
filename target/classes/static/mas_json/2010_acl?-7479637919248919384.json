{"title": "PCFGs, Topic Models, Adaptor Grammars and Learning Topical Collocations and the Structure of Proper Names.", "fields": ["proper noun", "rule based machine translation", "probabilistic logic", "topic model", "latent dirichlet allocation"], "abstract": "This paper establishes a connection between two apparently very different kinds of probabilistic models. Latent Dirichlet Allocation (LDA) models are used as \"topic models\" to produce a low-dimensional representation of documents, while Probabilistic Context-Free Grammars (PCFGs) define distributions over trees. The paper begins by showing that LDA topic models can be viewed as a special kind of PCFG, so Bayesian inference for PCFGs can be used to infer Topic Models as well. Adaptor Grammars (AGs) are a hierarchical, non-parameteric Bayesian extension of PCFGs. Exploiting the close relationship between LDA and PCFGs just described, we propose two novel probabilistic models that combine insights from LDA and AG models. The first replaces the unigram component of LDA topic models with multi-word sequences or collocations generated by an AG. The second extension builds on the first one to learn aspects of the internal structure of proper names.", "citation": "Citations (72)", "year": "2010", "departments": ["Macquarie University"], "conf": "acl", "authors": ["Mark Johnson.....http://dblp.org/pers/hd/j/Johnson_0001:Mark"], "pages": 10}