{"title": "A Bayesian Framework for Online Classifier Ensemble.", "fields": ["expected loss", "stationary process", "stochastic gradient descent", "likelihood function", "boosting"], "abstract": "We propose a Bayesian framework for recursively estimating the classifier weights in online learning of a classifier ensemble. In contrast with past methods, such as stochastic gradient descent or online boosting, our framework estimates the weights in terms of evolving posterior distributions. For a specified class of loss functions, we show that it is possible to formulate a suitably defined likelihood function and hence use the posterior distribution as an approximation to the global empirical loss minimizer. If the stream of training data is sampled from a stationary process, we can also show that our framework admits a superior rate of convergence to the expected loss minimizer than is possible with standard stochastic gradient descent. In experiments with real-world datasets, our formulation often performs better than online boosting algorithms.", "citation": "Citations (4)", "year": "2014", "departments": ["Boston University", "Boston University", "Boston University"], "conf": "icml", "authors": ["Qinxun Bai.....http://dblp.org/pers/hd/b/Bai:Qinxun", "Henry Lam.....http://dblp.org/pers/hd/l/Lam:Henry", "Stan Sclaroff.....http://dblp.org/pers/hd/s/Sclaroff:Stan"], "pages": 9}