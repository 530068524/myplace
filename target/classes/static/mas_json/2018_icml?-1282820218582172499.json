{"title": "Escaping Saddles with Stochastic Gradients.", "fields": ["stationary point", "saddle point", "magnitude", "curse of dimensionality", "isotropy"], "abstract": "We analyze the variance of stochastic gradients along negative curvature directions in certain non-convex machine learning models and show that stochastic gradients exhibit a strong component along these directions. Furthermore, we show that - contrary to the case of isotropic noise - this variance is proportional to the magnitude of the corresponding eigenvalues and not decreasing in the dimensionality. Based upon this observation we propose a new assumption under which we show that the injection of explicit, isotropic noise usually applied to make gradient descent escape saddle points can successfully be replaced by a simple SGD step. Additionally - and under the same condition - we derive the first convergence rate for plain SGD to a second-order stationary point in a number of iterations that is independent of the problem dimension.", "citation": "Not cited", "departments": ["ETH Zurich", "ETH Zurich", "ETH Zurich", "ETH Zurich"], "authors": ["Hadi Daneshmand.....http://dblp.org/pers/hd/d/Daneshmand:Hadi", "Jonas Moritz Kohler.....http://dblp.org/pers/hd/k/Kohler:Jonas_Moritz", "Aur\u00e9lien Lucchi.....http://dblp.org/pers/hd/l/Lucchi:Aur=eacute=lien", "Thomas Hofmann.....http://dblp.org/pers/hd/h/Hofmann:Thomas"], "conf": "icml", "year": "2018", "pages": 10}