{"title": "Natural Neural Networks.", "fields": ["gradient descent", "supervised learning", "computation", "speedup", "scalability"], "abstract": "We introduce Natural Neural Networks, a novel family of algorithms that speed up convergence by adapting their internal representation during training to improve conditioning of the Fisher matrix. In particular, we show a specific example that employs a simple and efficient reparametrization of the neural network weights by implicitly whitening the representation obtained at each layer, while preserving the feed-forward computation of the network. Such networks can be trained efficiently via the proposed Projected Natural Gradient Descent algorithm (PRONG), which amortizes the cost of these reparametrizations over many parameter updates and is closely related to the Mirror Descent online learning algorithm. We highlight the benefits of our method on both unsupervised and supervised learning tasks, and showcase its scalability by training on the large-scale ImageNet Challenge dataset.", "citation": "Citations (74)", "year": "2015", "departments": ["Google", "Google", "Google", "Google"], "conf": "nips", "authors": ["Guillaume Desjardins.....http://dblp.org/pers/hd/d/Desjardins:Guillaume", "Karen Simonyan.....http://dblp.org/pers/hd/s/Simonyan:Karen", "Razvan Pascanu.....http://dblp.org/pers/hd/p/Pascanu:Razvan", "Koray Kavukcuoglu.....http://dblp.org/pers/hd/k/Kavukcuoglu:Koray"], "pages": 9}