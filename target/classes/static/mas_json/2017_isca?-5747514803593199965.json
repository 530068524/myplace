{"title": "Understanding and Optimizing Asynchronous Low-Precision Stochastic Gradient Descent.", "fields": ["conceptualization", "multi core processor", "cache", "stochastic gradient descent", "central processing unit"], "abstract": "Stochastic gradient descent (SGD) is one of the most popular numerical algorithms used in machine learning and other domains. Since this is likely to continue for the foreseeable future, it is important to study techniques that can make it run fast on parallel hardware. In this paper, we provide the first analysis of a technique called Buck-wild! that uses both asynchronous execution and low-precision computation. We introduce the DMGC model, the first conceptualization of the parameter space that exists when implementing low-precision SGD, and show that it provides a way to both classify these algorithms and model their performance. We leverage this insight to propose and analyze techniques to improve the speed of low-precision SGD. First, we propose software optimizations that can increase throughput on existing CPUs by up to 11X. Second, we propose architectural changes, including a new cache technique we call an obstinate cache, that increase throughput beyond the limits of current-generation hardware. We also implement and analyze low-precision SGD on the FPGA, which is a promising alternative to the CPU for future SGD systems.", "citation": "Citations (10)", "departments": ["Stanford University", "Stanford University", "Stanford University", "Stanford University"], "authors": ["Christopher De Sa.....http://dblp.org/pers/hd/s/Sa:Christopher_De", "Matthew Feldman.....http://dblp.org/pers/hd/f/Feldman:Matthew", "Christopher R\u00e9.....http://dblp.org/pers/hd/r/R=eacute=:Christopher", "Kunle Olukotun.....http://dblp.org/pers/hd/o/Olukotun:Kunle"], "conf": "isca", "year": "2017", "pages": 14}