{"title": "The Power of Interpolation: Understanding the Effectiveness of SGD in Modern Over-parametrized Learning.", "fields": ["variance reduction", "rate of convergence", "gradient descent", "stochastic gradient descent", "parametrization"], "abstract": "Stochastic Gradient Descent (SGD) with small mini-batch is a key component in modern large-scale learning. However, its efficiency has not been easy to analyze as most theoretical results require adaptive rates and show convergence rates far slower than that for gradient descent, making computational comparisons difficult. \nIn this paper we aim to clarify the issue of fast SGD convergence. The key observation is that most modern architectures are over-parametrized and are trained to interpolate the data by driving the empirical loss close to zero. While it is still unclear why these interpolated solutions perform well on test data, we show that these regimes allow for very fast convergence of SGD, comparable in the number of iterations to full gradient descent. \nSpecifically, consider the setting with a quadratic objective function, or a general objective function in the proximity of a minimum, where the quadratic term is dominant. First, we obtain the optimal convergence rate for any mini-batch SGD and derive the optimal step size as a function of the batch size $m$. Second, we show: (1) $m=1$ is optimal in terms of number of computations required to achieve any desired accuracy. (2) There is a critical mini-batch size $m^*$ such that: (2a) SGD iteration with batch size $m\\leq m^*$ is nearly equivalent to $m$ iterations of batch size $1$. (2b) SGD iteration with mini-batch $m> m^*$ is nearly equivalent to a full gradient descent iteration. \nThe critical mini-batch size can be viewed as the limit for effective mini-batch parallelization. It is also nearly independent of the data size, implying $O(n)$ acceleration over GD per unit of computation. These theoretical analyses are verified by experimental results. \nFinally, we show how the interpolation perspective and our results fit in the deep neural networks and discuss connections to adaptive rates for SGD and variance reduction.", "citation": "Citations (2)", "departments": ["Ohio State University", "Ohio State University", "Ohio State University"], "authors": ["Siyuan Ma.....http://dblp.org/pers/hd/m/Ma:Siyuan", "Raef Bassily.....http://dblp.org/pers/hd/b/Bassily:Raef", "Mikhail Belkin.....http://dblp.org/pers/hd/b/Belkin:Mikhail"], "conf": "icml", "year": "2018", "pages": 10}