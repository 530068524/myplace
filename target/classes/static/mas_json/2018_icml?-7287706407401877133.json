{"title": "Gradient-Based Meta-Learning with Learned Layerwise Metric and Subspace.", "fields": ["subspace topology", "gradient descent", "pattern recognition", "metric", "machine learning"], "abstract": "Gradient-based meta-learning has been shown to be expressive enough to approximate any learning algorithm. While previous such methods have been successful in meta-learning tasks, they resort to simple gradient descent during meta-testing. Our primary contribution is the {\\em MT-net}, which enables the meta-learner to learn on each layer's activation space a subspace that the task-specific learner performs gradient descent on. Additionally, a task-specific learner of an {\\em MT-net} performs gradient descent with respect to a meta-learned distance metric, which warps the activation space to be more sensitive to task identity. We demonstrate that the dimension of this learned subspace reflects the complexity of the task-specific learner's adaptation task, and also that our model is less sensitive to the choice of initial learning rates than previous gradient-based meta-learning methods. Our method achieves state-of-the-art or comparable performance on few-shot classification and regression tasks.", "citation": "Not cited", "departments": ["Pohang University of Science and Technology", "Pohang Universi ... and Techonology"], "authors": ["Yoonho Lee.....http://dblp.org/pers/hd/l/Lee:Yoonho", "Seungjin Choi.....http://dblp.org/pers/hd/c/Choi:Seungjin"], "conf": "icml", "year": "2018", "pages": 10}