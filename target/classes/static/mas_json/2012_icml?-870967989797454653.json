{"title": "A convex relaxation for weakly supervised classifiers.", "fields": ["cluster analysis", "regular polygon", "linear classifier", "maxima and minima", "machine learning"], "abstract": "This paper introduces a general multi-class approach to weakly supervised classification. Inferring the labels and learning the parameters of the model is usually done jointly through a block-coordinate descent algorithm such as expectation-maximization (EM), which may lead to local minima. To avoid this problem, we propose a cost function based on a convex relaxation of the soft-max loss. We then propose an algorithm specifically designed to efficiently solve the corresponding semidefinite program (SDP). Empirically, our method compares favorably to standard ones on different datasets for multiple instance learning and semi-supervised learning, as well as on clustering tasks.", "citation": "Citations (15)", "departments": ["\u00c9cole Normale Sup\u00e9rieure", "\u00c9cole Normale Sup\u00e9rieure"], "authors": ["Armand Joulin.....http://dblp.org/pers/hd/j/Joulin:Armand", "Francis R. Bach.....http://dblp.org/pers/hd/b/Bach:Francis_R="], "conf": "icml", "year": "2012", "pages": -1}