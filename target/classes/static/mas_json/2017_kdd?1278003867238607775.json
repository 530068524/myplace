{"title": "Large-scale Collaborative Ranking in Near-Linear Time.", "fields": ["ranking", "pairwise comparison", "ranking svm", "learning to rank"], "abstract": "In this paper, we consider the Collaborative Ranking (CR) problem for recommendation systems. Given a set of pairwise preferences between items for each user, collaborative ranking can be used to rank un-rated items for each user, and this ranking can be naturally used for recommendation. It is observed that collaborative ranking algorithms usually achieve better performance since they directly minimize the ranking loss; however, they are rarely used in practice due to the poor scalability. All the existing CR algorithms have time complexity at least  O (|\u03a9| r ) per iteration, where  r  is the target rank and |\u03a9| is number of pairs which grows quadratically with number of ratings per user. For example, the Netflix data contains totally 20 billion rating pairs, and at this scale all the current algorithms have to work with significant subsampling, resulting in poor prediction on testing data.   In this paper, we propose a new collaborative ranking algorithm called Primal-CR that reduces the time complexity to  O (|\u03a9|+ d  1  | d  2   r ), where  d  1  is number of users and | d  2  is the averaged number of items rated by a user. Note that  d  1  |d 2  is strictly smaller and often much smaller than |\u03a9|.   Furthermore, by exploiting the fact that most data is in the form of numerical ratings instead of pairwise comparisons, we propose Primal-CR++ with  O ( d  1 | d  2  ( r + log | d  2 )) time complexity. Both algorithms have better theoretical time complexity than existing approaches and also outperform existing approaches in terms of NDCG and pairwise error on real data sets. To the best of our knowledge, this is the first collaborative ranking algorithm capable of working on the full Netflix dataset using all the 20 billion rating pairs, and this leads to a model with much better recommendation compared with previous models trained on subsamples. Finally, compared with classical matrix factorization algorithm which also requires  O ( d  1  d  2  r ) time, our algorithm has almost the same efficiency while making much better recommendations since we consider the ranking loss.", "citation": "Not cited", "departments": ["University of California, Davis", "University of California, Davis", "University of California, Davis"], "authors": ["Liwei Wu.....http://dblp.org/pers/hd/w/Wu:Liwei", "Cho-Jui Hsieh.....http://dblp.org/pers/hd/h/Hsieh:Cho=Jui", "James Sharpnack.....http://dblp.org/pers/hd/s/Sharpnack:James"], "conf": "kdd", "year": "2017", "pages": 10}