{"title": "Modeling Deep Temporal Dependencies with Recurrent \"Grammar Cells\".", "fields": ["grammar", "machine learning", "autoencoder", "recurrent neural network", "syntax"], "abstract": "We propose modeling time series by representing the transformations that take a frame at time t to a frame at time t+1. To this end we show how a bi-linear model of transformations, such as a gated autoencoder, can be turned into a recurrent network, by training it to predict future frames from the current one and the inferred transformation using backprop-through-time. We also show how stacking multiple layers of gating units in a recurrent pyramid makes it possible to represent the \"syntax\" of complicated time series, and that it can outperform standard recurrent neural networks in terms of prediction accuracy on a variety of tasks.", "citation": "Citations (67)", "year": "2014", "departments": ["Goethe University Frankfurt", "Universit\u00e9 de Montr\u00e9al", "Goethe University Frankfurt"], "conf": "nips", "authors": ["Vincent Michalski.....http://dblp.org/pers/hd/m/Michalski:Vincent", "Roland Memisevic.....http://dblp.org/pers/hd/m/Memisevic:Roland", "Kishore Reddy Konda.....http://dblp.org/pers/hd/k/Konda:Kishore_Reddy"], "pages": 9}