{"title": "Potential-based Shaping in Model-based Reinforcement Learning.", "fields": ["computer science", "error driven learning", "reinforcement learning", "artificial intelligence", "machine learning"], "abstract": "Potential-based shaping was designed as a way of introducing background knowledge into model-free reinforcement-learning algorithms. By identifying states that are likely to have high value, this approach can decrease experience complexity--the number of trials needed to find near-optimal behavior. An orthogonal way of decreasing experience complexity is to use a model-based learning approach, building and exploiting an explicit transition model. In this paper, we show how potential-based shaping can be redefined to work in the model-based setting to produce an algorithm that shares the benefits of both ideas.", "citation": "Citations (69)", "departments": ["Rutgers University", "Rutgers University", "Rutgers University"], "authors": ["John Asmuth.....http://dblp.org/pers/hd/a/Asmuth:John", "Michael L. Littman.....http://dblp.org/pers/hd/l/Littman:Michael_L=", "Robert Zinkov.....http://dblp.org/pers/hd/z/Zinkov:Robert"], "conf": "aaai", "year": "2008", "pages": 6}