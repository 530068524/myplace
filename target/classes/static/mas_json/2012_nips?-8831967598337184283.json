{"title": "Finite Sample Convergence Rates of Zero-Order Stochastic Optimization Methods.", "fields": ["stochastic optimization", "minimax", "convergence tests", "rate of convergence", "compact convergence"], "abstract": "We consider derivative-free algorithms for stochastic optimization problems that use only noisy function values rather than gradients, analyzing their finite-sample convergence rates. We show that if pairs of function values are available, algorithms that use gradient estimates based on random perturbations suffer a factor of at most \u221ad in convergence rate over traditional stochastic gradient methods, where d is the problem dimension. We complement our algorithmic development with information-theoretic lower bounds on the minimax convergence rate of such problems, which show that our bounds are sharp with respect to all problem-dependent quantities: they cannot be improved by more than constant factors.", "citation": "Citations (6)", "departments": ["University of California, Berkeley", "University of California, Berkeley", "University of California, Berkeley", "University of California, Berkeley"], "authors": ["John C. Duchi.....http://dblp.org/pers/hd/d/Duchi:John_C=", "Michael I. Jordan.....http://dblp.org/pers/hd/j/Jordan:Michael_I=", "Martin J. Wainwright.....http://dblp.org/pers/hd/w/Wainwright:Martin_J=", "Andre Wibisono.....http://dblp.org/pers/hd/w/Wibisono:Andre"], "conf": "nips", "year": "2012", "pages": 9}