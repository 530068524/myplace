{"title": "Two-Target Algorithms for Infinite-Armed Bandits with Bernoulli Rewards.", "fields": ["exploit", "machine learning", "bernoulli s principle", "time horizon", "regret"], "abstract": "We consider an infinite-armed bandit problem with Bernoulli rewards. The mean rewards are independent, uniformly distributed over [0,1]. Rewards 0 and 1 are referred to as a success and a failure, respectively. We propose a novel algorithm where the decision to exploit any arm is based on two successive targets, namely, the total number of successes until the first failure and until the first m failures, respectively, where m is a fixed parameter. This two-target algorithm achieves a long-term average regret in \u221a2n for a large parameter m and a known time horizon n. This regret is optimal and strictly less than the regret achieved by the best known algorithms, which is in 2\u221an. The results are extended to any mean-reward distribution whose support contains 1 and to unknown time horizons. Numerical experiments show the performance of the algorithm for finite time horizons.", "citation": "Citations (3)", "departments": ["T\u00e9l\u00e9com ParisTech", "Royal Institute of Technology"], "authors": ["Thomas Bonald.....http://dblp.org/pers/hd/b/Bonald:Thomas", "Alexandre Prouti\u00e8re.....http://dblp.org/pers/hd/p/Prouti=egrave=re:Alexandre"], "conf": "nips", "year": "2013", "pages": 9}