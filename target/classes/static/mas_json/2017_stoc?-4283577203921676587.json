{"title": "Katyusha: the first direct acceleration of stochastic gradient methods.", "fields": ["acceleration", "pattern recognition", "stochastic gradient descent", "gradient descent", "machine learning", "mathematics", "backpropagation", "gradient method", "computer science", "discrete mathematics", "computational physics", "stochastic optimization", "artificial intelligence"], "abstract": "Nesterov\u00e2  s momentum trick is famously known for accelerating gradient descent, and has been proven useful in building fast iterative algorithms. However, in the stochastic setting, counterexamples exist and prevent Nesterov\u00e2  s momentum from providing similar acceleration, even if the underlying problem is convex.   We introduce  Katyusha , a direct, primal-only stochastic gradient method to fix this issue. It has a provably accelerated convergence rate in convex (off-line) stochastic optimization. The main ingredient is  Katyusha momentum , a novel \u00e2  negative momentum\u00e2   on top of Nesterov\u00e2  s momentum that can be incorporated into a variance-reduction based algorithm and speed it up. Since variance reduction has been successfully applied to a growing list of practical problems, our paper suggests that in each of such cases, one could potentially give Katyusha a hug.", "citation": "Not cited", "year": "2017", "departments": ["Princeton University"], "conf": "stoc", "authors": ["Zeyuan Allen Zhu.....http://dblp.org/pers/hd/z/Zhu:Zeyuan_Allen"], "pages": 6}