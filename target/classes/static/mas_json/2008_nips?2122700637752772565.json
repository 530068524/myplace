{"title": "Near-optimal Regret Bounds for Reinforcement Learning.", "fields": ["logarithm", "sample complexity", "regret", "reinforcement learning", "q learning", "upper and lower bounds", "markov decision process"], "abstract": "For undiscounted reinforcement learning in Markov decision processes (MDPs) we consider the total regret of a learning algorithm with respect to an optimal policy. In order to describe the transition structure of an MDP we propose a new parameter: An MDP has diameter D if for any pair of states s,s' there is a policy which moves from s to s' in at most D steps (on average). We present a reinforcement learning algorithm with total regret O(DS\u221aAT) after T steps for any unknown MDP with S states, A actions per state, and diameter D. A corresponding lower bound of \u03a9(\u221aDSAT) on the total regret of any learning algorithm is given as well.\n\nThese results are complemented by a sample complexity bound on the number of suboptimal steps taken by our algorithm. This bound can be used to achieve a (gap-dependent) regret bound that is logarithmic in T.\n\nFinally, we also consider a setting where the MDP is allowed to change a fixed number of l times. We present a modification of our algorithm that is able to deal with this setting and show a regret bound of O(l1/3T2/3DS\u221aA).", "citation": "Citations (339)", "year": "2008", "departments": ["University of Leoben", "University of Leoben", "University of Leoben"], "conf": "nips", "authors": ["Peter Auer.....http://dblp.org/pers/hd/a/Auer:Peter", "Thomas Jaksch.....http://dblp.org/pers/hd/j/Jaksch:Thomas", "Ronald Ortner.....http://dblp.org/pers/hd/o/Ortner:Ronald"], "pages": 8}