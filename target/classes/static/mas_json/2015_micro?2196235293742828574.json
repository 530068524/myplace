{"title": "Neuromorphic accelerators: a comparison between neuroscience and machine-learning approaches.", "fields": ["implementation", "handwriting recognition", "neuromorphic engineering", "ranging", "artificial neural network"], "abstract": "A vast array of devices, ranging from industrial robots to self-driven cars or smartphones, require increasingly sophisticated processing of real-world input data (image, voice, radio, ...). Interestingly, hardware neural network accelerators are emerging again as attractive candidate architectures for such tasks. The neural network algorithms considered come from two, largely separate, domains: machine-learning and neuroscience. These neural networks have very different characteristics, so it is unclear which approach should be favored for hardware implementation. Yet, few studies compare them from a hardware perspective. We implement both types of networks down to the layout, and we compare the relative merit of each approach in terms of energy, speed, area cost, accuracy and functionality.   Within the limit of our study (current SNN and machine-learning NN algorithms, current best effort at hardware implementation efforts, and workloads used in this study), our analysis helps dispel the notion that hardware neural network accelerators inspired from neuroscience, such as SNN+STDP, are currently a competitive alternative to hardware neural networks accelerators inspired from machine-learning, such as MLP+BP: not only in terms of accuracy, but also in terms of hardware cost for realistic implementations, which is less expected. However, we also outline that SNN+STDP carry potential for reduced hardware cost compared to machine-learning networks at very large scales, if accuracy issues can be controlled (or for applications where they are less important). We also identify the key sources of inaccuracy of SNN+STDP which are less related to the loss of information due to spike coding than to the nature of the STDP learning algorithm. Finally, we outline that for the category of applications which require permanent online learning and moderate accuracy, SNN+STDP hardware accelerators could be a very cost-efficient solution.", "citation": "Citations (16)", "departments": ["Chinese Academy of Sciences", "Intel", "Chinese Academy of Sciences", "Inner Mongolia University", "Chinese Academy of Sciences"], "authors": ["Zidong Du.....http://dblp.org/pers/hd/d/Du:Zidong", "Daniel D. Ben-Dayan Rubin.....http://dblp.org/pers/hd/r/Rubin:Daniel_D=_Ben=Dayan", "Yunji Chen.....http://dblp.org/pers/hd/c/Chen:Yunji", "Liqiang He.....http://dblp.org/pers/hd/h/He:Liqiang", "Tianshi Chen.....http://dblp.org/pers/hd/c/Chen:Tianshi", "Lei Zhang.....http://dblp.org/pers/hd/z/Zhang:Lei", "Chengyong Wu.....http://dblp.org/pers/hd/w/Wu:Chengyong", "Olivier Temam.....http://dblp.org/pers/hd/t/Temam:Olivier"], "conf": "micro", "year": "2015", "pages": 14}