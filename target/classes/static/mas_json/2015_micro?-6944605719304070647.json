{"title": "Filtered runahead execution with a runahead buffer.", "fields": ["memory level parallelism", "runahead", "cache", "instruction window", "out of order execution"], "abstract": "Runahead execution dynamically expands the instruction window of an out of order processor to generate memory level parallelism (MLP) while the core would otherwise be stalled. Unfortunately, runahead has the disadvantage of requiring the front-end to remain active to supply instructions. We propose a new structure (the Runahead Buffer) for supplying these instructions. We note that cache misses are often caused by repetitive, short dependence chains. We store these dependence chains in the runahead buffer. During runahead, the runahead buffer is used to supply instructions. This generates 2\u00d7 more MLP than traditional runahead on average because the core can run further ahead. It also saves energy since the front-end can be clock-gated, reducing dynamic energy consumption. Over a no-prefetching/prefetching baseline, the result is a performance benefit of 17.2%/7.8% and an energy reduction of 6.7%/4.5% respectively. Traditional runahead with additional energy optimizations results in a performance benefit of 12.1%/5.9% but an energy increase of 9.5%/5.4%. Finally, we propose a hybrid policy that switches between the runahead buffer and traditional runahead, maximizing performance.", "citation": "Citations (2)", "departments": ["University of Texas at Austin", "University of Texas at Austin"], "authors": ["Milad Hashemi.....http://dblp.org/pers/hd/h/Hashemi:Milad", "Yale N. Patt.....http://dblp.org/pers/hd/p/Patt:Yale_N="], "conf": "micro", "year": "2015", "pages": 12}