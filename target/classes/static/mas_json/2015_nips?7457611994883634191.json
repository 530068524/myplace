{"title": "Asynchronous Parallel Stochastic Gradient for Nonconvex Optimization.", "fields": ["deep learning", "convex optimization", "speedup", "asynchronous communication", "bounded function"], "abstract": "Asynchronous parallel implementations of stochastic gradient (SG) have been broadly used in solving deep neural network and received many successes in practice recently. However, existing theories cannot explain their convergence and speedup properties, mainly due to the nonconvexity of most deep learning formulations and the asynchronous parallel mechanism. To fill the gaps in theory and provide theoretical supports, this paper studies two asynchronous parallel implementations of SG: one is over a computer network and the other is on a shared memory system. We establish an ergodic convergence rate O(1/ \u221aK) for both algorithms and prove that the linear speedup is achievable if the number of workers is bounded by \u221aK (K is the total number of iterations). Our results generalize and improve existing analysis for convex minimization.", "citation": "Citations (133)", "year": "2015", "departments": ["University of Rochester", "University of Rochester", "University of Rochester", "University of Rochester"], "conf": "nips", "authors": ["Xiangru Lian.....http://dblp.org/pers/hd/l/Lian:Xiangru", "Yijun Huang.....http://dblp.org/pers/hd/h/Huang:Yijun", "Yuncheng Li.....http://dblp.org/pers/hd/l/Li:Yuncheng", "Ji Liu.....http://dblp.org/pers/hd/l/Liu_0002:Ji"], "pages": 9}