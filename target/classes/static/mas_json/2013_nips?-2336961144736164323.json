{"title": "Training and Analysing Deep Recurrent Neural Networks.", "fields": ["hierarchy", "machine learning", "stochastic gradient descent", "recurrent neural network", "architecture"], "abstract": "Time series often have a temporal hierarchy, with information that is spread out over multiple time scales. Common recurrent neural networks, however, do not explicitly accommodate such a hierarchy, and most research on them has been focusing on training algorithms rather than on their basic architecture. In this pa- per we study the effect of a hierarchy of recurrent neural networks on processing time series. Here, each layer is a recurrent network which receives the hidden state of the previous layer as input. This architecture allows us to perform hi- erarchical processing on difficult temporal tasks, and more naturally capture the structure of time series. We show that they reach state-of-the-art performance for recurrent networks in character-level language modelling when trained with sim- ple stochastic gradient descent. We also offer an analysis of the different emergent time scales.", "citation": "Citations (196)", "departments": ["Ghent University", "Ghent University"], "authors": ["Michiel Hermans.....http://dblp.org/pers/hd/h/Hermans:Michiel", "Benjamin Schrauwen.....http://dblp.org/pers/hd/s/Schrauwen:Benjamin"], "conf": "nips", "year": "2013", "pages": 9}