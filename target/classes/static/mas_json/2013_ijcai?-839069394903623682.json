{"title": "Online Expectation Maximization for Reinforcement Learning in POMDPs.", "fields": ["sufficient statistic", "expectation maximization algorithm", "partially observable markov decision process", "online algorithm", "reinforcement learning"], "abstract": "We present online nested expectation maximization for model-free reinforcement learning in a POMDP. The algorithm evaluates the policy only in the current learning episode, discarding the episode after the evaluation and memorizing the sufficient statistic, from which the policy is computed in closed-form. As a result, the online algorithm has a time complexity O(n) and a memory complexity O(1), compared to O(n2) and O(n) for the corresponding batch-mode algorithm, where n is the number of learning episodes. The online algorithm, which has a provable convergence, is demonstrated on five benchmark POMDP problems.", "citation": "Citations (3)", "year": "2013", "departments": ["Duke University", "Duke University", "Duke University"], "conf": "ijcai", "authors": ["Miao Liu.....http://dblp.org/pers/hd/l/Liu:Miao", "Xuejun Liao.....http://dblp.org/pers/hd/l/Liao:Xuejun", "Lawrence Carin.....http://dblp.org/pers/hd/c/Carin:Lawrence"], "pages": 7}