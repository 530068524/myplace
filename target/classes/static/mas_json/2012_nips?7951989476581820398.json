{"title": "Proximal Newton-type methods for convex optimization.", "fields": ["subderivative", "proximal gradient methods", "convex analysis", "proper convex function", "proximal gradient methods for learning"], "abstract": "We seek to solve convex optimization problems in composite form: minimizex\u2208\u211dn f (x) := g(x) + h(x), where g is convex and continuously differentiable and h : \u211dn \u2192 \u211d is a convex but not necessarily differentiable function whose proximal mapping can be evaluated efficiently. We derive a generalization of Newton-type methods to handle such convex but nonsmooth objective functions. We prove such methods are globally convergent and achieve superlinear rates of convergence in the vicinity of an optimal solution. We also demonstrate the performance of these methods using problems of relevance in machine learning and statistics.", "citation": "Citations (80)", "departments": ["Stanford University", "Stanford University", "Stanford University"], "authors": ["Jason D. Lee.....http://dblp.org/pers/hd/l/Lee:Jason_D=", "Yuekai Sun.....http://dblp.org/pers/hd/s/Sun:Yuekai", "Michael A. Saunders.....http://dblp.org/pers/hd/s/Saunders:Michael_A="], "conf": "nips", "year": "2012", "pages": 9}