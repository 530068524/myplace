{"title": "Optimal reverse prediction: a unified perspective on supervised, unsupervised and semi-supervised learning.", "fields": ["stability", "semi supervised learning", "k means clustering", "competitive learning", "supervised learning"], "abstract": "Training principles for unsupervised learning are often derived from motivations that appear to be independent of supervised learning. In this paper we present a simple unification of several supervised and unsupervised training principles through the concept of  optimal reverse prediction : predict the inputs from the target labels, optimizing both over model parameters and any missing labels. In particular, we show how supervised least squares, principal components analysis, k-means clustering and normalized graph-cut can all be expressed as instances of the same training principle. Natural forms of semi-supervised regression and classification are then automatically derived, yielding semi-supervised learning algorithms for regression and classification that, surprisingly, are novel and refine the state of the art. These algorithms can all be combined with standard regularizers and made non-linear via kernels.", "citation": "Citations (14)", "departments": ["University of Alberta", "University of Alberta", "University of Alberta"], "authors": ["Linli Xu.....http://dblp.org/pers/hd/x/Xu:Linli", "Martha White.....http://dblp.org/pers/hd/w/White:Martha", "Dale Schuurmans.....http://dblp.org/pers/hd/s/Schuurmans:Dale"], "conf": "icml", "year": "2009", "pages": 8}