{"title": "Risk-Aversion in Multi-armed Bandits.", "fields": ["risk aversion", "mathematical optimization", "dilemma", "regret", "management science"], "abstract": "Stochastic multi-armed bandits solve the Exploration-Exploitation dilemma and ultimately maximize the expected reward. Nonetheless, in many practical problems, maximizing the expected reward is not the most desirable objective. In this paper, we introduce a novel setting based on the principle of risk-aversion where the objective is to compete against the arm with the best risk-return trade-off. This setting proves to be more difficult than the standard multi-arm bandit setting due in part to an exploration risk which introduces a regret associated to the variability of an algorithm. Using variance as a measure of risk, we define two algorithms, investigate their theoretical guarantees, and report preliminary empirical results.", "citation": "Citations (15)", "departments": ["French Institute for Research in Computer Science and Automation", "French Institute for Research in Computer Science and Automation", "French Institute for Research in Computer Science and Automation"], "authors": ["Amir Sani.....http://dblp.org/pers/hd/s/Sani:Amir", "Alessandro Lazaric.....http://dblp.org/pers/hd/l/Lazaric:Alessandro", "R\u00e9mi Munos.....http://dblp.org/pers/hd/m/Munos:R=eacute=mi"], "conf": "nips", "year": "2012", "pages": 9}