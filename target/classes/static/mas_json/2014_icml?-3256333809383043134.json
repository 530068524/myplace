{"title": "(Near) Dimension Independent Risk Bounds for Differentially Private Learning.", "fields": ["bounded function", "empirical risk minimization", "absolute risk reduction", "regret", "differentiable function"], "abstract": "In this paper, we study the problem of differentially private risk minimization where the goal is to provide differentially private algorithms that have small excess risk. In particular we address the following open problem: Is it possible to design computationally efficient differentially private risk minimizers with excess risk bounds that do not explicitly depend on dimensionality (p) and do not require structural assumptions like restricted strong convexity?\n\nIn this paper, we answer the question in the affirmative for a variant of the well-known output and objective perturbation algorithms (Chaudhuri et al., 2011). In particular, we show that under certain assumptions, variants of both output and objective perturbation algorithms have no explicit dependence on p; the excess risk depends only on the L2-norm of the true risk minimizer and that of training points.\n\nNext, we present a novel privacy preserving algorithm for risk minimization over simplex in the generalized linear model, where the loss function is a doubly differentiable convex function. Assuming that the training points have bounded L\u221e-norm, our algorithm provides risk bound that has only logarithmic dependence on p. We also apply our technique to the online learning setting and obtain a regret bound with similar logarithmic dependence on p. In contrast, the existing differentially private online learning methods incur O(\u221ap) dependence.", "citation": "Citations (23)", "year": "2014", "departments": ["Microsoft", "Stanford University"], "conf": "icml", "authors": ["Prateek Jain.....http://dblp.org/pers/hd/j/Jain_0002:Prateek", "Abhradeep Guha Thakurta.....http://dblp.org/pers/hd/t/Thakurta:Abhradeep_Guha"], "pages": 9}