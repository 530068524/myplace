{"title": "Latent Dependency Forest Models.", "fields": ["random variable", "probabilistic logic", "dependency grammar", "probabilistic relevance model", "latent variable"], "abstract": "Probabilistic modeling is one of the foundations of modern machine learning and artificial intelligence. In this paper, we propose a novel type of probabilistic models named latent dependency forest models (LDFMs). A LDFM models the dependencies between random variables with a forest structure that can change dynamically based on the variable values. It is therefore capable of modeling context-specific independence. We parameterize a LDFM using a first-order non-projective dependency grammar. Learning LDFMs from data can be formulated purely as a parameter learning problem, and hence the difficult problem of model structure learning is circumvented. Our experimental results show that LDFMs are competitive with existing probabilistic models.", "citation": "Citations (1)", "departments": ["Chinese Academy of Sciences", "ShanghaiTech University"], "authors": ["Shanbo Chu.....http://dblp.org/pers/hd/c/Chu:Shanbo", "Yong Jiang.....http://dblp.org/pers/hd/j/Jiang:Yong", "Kewei Tu.....http://dblp.org/pers/hd/t/Tu:Kewei"], "conf": "aaai", "year": "2017", "pages": 7}