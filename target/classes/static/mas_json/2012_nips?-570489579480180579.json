{"title": "Cost-Sensitive Exploration in Bayesian Reinforcement Learning.", "fields": ["bayesian probability", "machine learning", "reinforcement learning", "markov decision process", "total cost"], "abstract": "In this paper, we consider Bayesian reinforcement learning (BRL) where actions incur costs in addition to rewards, and thus exploration has to be constrained in terms of the expected total cost while learning to maximize the expected long-term total reward. In order to formalize cost-sensitive exploration, we use the constrained Markov decision process (CMDP) as the model of the environment, in which we can naturally encode exploration requirements using the cost function. We extend BEETLE, a model-based BRL method, for learning in the environment with cost constraints. We demonstrate the cost-sensitive exploration behaviour in a number of simulated problems.", "citation": "Citations (3)", "departments": ["University of Cambridge", "KAIST", "University of Waterloo"], "authors": ["Dongho Kim.....http://dblp.org/pers/hd/k/Kim:Dongho", "Kee-Eung Kim.....http://dblp.org/pers/hd/k/Kim:Kee=Eung", "Pascal Poupart.....http://dblp.org/pers/hd/p/Poupart:Pascal"], "conf": "nips", "year": "2012", "pages": 9}