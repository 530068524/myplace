{"title": "Variance-Reduced and Projection-Free Stochastic Optimization.", "fields": ["variance reduction", "lipschitz continuity", "multiclass classification", "stochastic gradient descent", "stochastic optimization"], "abstract": "The Frank-Wolfe optimization algorithm has recently regained popularity for machine learning applications due to its projection-free property and its ability to handle structured constraints. However, in the stochastic learning setting, it is still relatively understudied compared to the gradient descent counterpart. In this work, leveraging a recent variance reduction technique, we propose two stochastic Frank-Wolfe variants which substantially improve previous results in terms of the number of stochastic gradient evaluations needed to achieve 1 - e accuracy. For example, we improve from O(1/e) to O(ln 1/e) if the objective function is smooth and strongly convex, and from O(1/e2) to O(1/e1.5) if the objective function is smooth and Lipschitz. The theoretical improvement is also observed in experiments on real-world datasets for a multiclass classification application.", "citation": "Citations (20)", "year": "2016", "departments": ["Princeton University", "Princeton University"], "conf": "icml", "authors": ["Elad Hazan.....http://dblp.org/pers/hd/h/Hazan:Elad", "Haipeng Luo.....http://dblp.org/pers/hd/l/Luo:Haipeng"], "pages": 9}