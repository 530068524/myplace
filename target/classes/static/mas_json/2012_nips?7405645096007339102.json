{"title": "Mirror Descent Meets Fixed Share (and feels no regret).", "fields": ["logarithm", "mathematics", "mathematical optimization", "regret", "generalization"], "abstract": "Mirror descent with an entropic regularizer is known to achieve shifting regret bounds that are logarithmic in the dimension. This is done using either a carefully designed projection or by a weight sharing technique. Via a novel unified analysis, we show that these two approaches deliver essentially equivalent bounds on a notion of regret generalizing shifting, adaptive, discounted, and other related regrets. Our analysis also captures and extends the generalized weight sharing technique of Bousquet and Warmuth, and can be refined in several ways, including improvements for small losses and adaptive tuning of parameters.", "citation": "Citations (16)", "departments": ["University of Milan", "\u00c9cole Normale Sup\u00e9rieure", "Pompeu Fabra University", "\u00c9cole Normale Sup\u00e9rieure"], "authors": ["Nicol\u00f2 Cesa-Bianchi.....http://dblp.org/pers/hd/c/Cesa=Bianchi:Nicol=ograve=", "Pierre Gaillard.....http://dblp.org/pers/hd/g/Gaillard:Pierre", "G\u00e1bor Lugosi.....http://dblp.org/pers/hd/l/Lugosi:G=aacute=bor", "Gilles Stoltz.....http://dblp.org/pers/hd/s/Stoltz:Gilles"], "conf": "nips", "year": "2012", "pages": 9}