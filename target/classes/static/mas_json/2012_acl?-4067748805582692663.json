{"title": "Big Data versus the Crowd: Looking for Relationships in All the Right Places.", "fields": ["training set", "precision and recall", "labeled data", "f1 score", "big data"], "abstract": "Classically, training relation extractors relies on high-quality, manually annotated training data, which can be expensive to obtain. To mitigate this cost, NLU researchers have considered two newly available sources of less expensive (but potentially lower quality) labeled data from distant supervision and crowd sourcing. There is, however, no study comparing the relative impact of these two sources on the precision and recall of post-learning answers. To fill this gap, we empirically study how state-of-the-art techniques are affected by scaling these two sources. We use corpus sizes of up to 100 million documents and tens of thousands of crowd-source labeled examples. Our experiments show that increasing the corpus size for distant supervision has a statistically significant, positive impact on quality (F1 score). In contrast, human feedback has a positive and statistically significant, but lower, impact on precision and recall.", "citation": "Citations (27)", "year": "2012", "departments": ["University of Wisconsin-Madison", "University of Wisconsin-Madison", "University of Wisconsin-Madison", "University of Wisconsin-Madison"], "conf": "acl", "authors": ["Ce Zhang.....http://dblp.org/pers/hd/z/Zhang:Ce", "Feng Niu.....http://dblp.org/pers/hd/n/Niu:Feng", "Christopher R\u00e9.....http://dblp.org/pers/hd/r/R=eacute=:Christopher", "Jude W. Shavlik.....http://dblp.org/pers/hd/s/Shavlik:Jude_W="], "pages": 10}