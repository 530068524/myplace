{"title": "Attention-over-Attention Neural Networks for Reading Comprehension.", "fields": ["reading comprehension", "machine learning", "architecture", "artificial neural network", "natural language processing"], "abstract": "Cloze-style queries are representative problems in reading comprehension. Over the past few months, we have seen much progress that utilizing neural network approach to solve Cloze-style questions. In this paper, we present a novel model called attention-over-attention reader for the Cloze-style reading comprehension task. Our model aims to place another attention mechanism over the document-level attention, and induces \"attended attention\" for final predictions. Unlike the previous works, our neural network model requires less pre-defined hyper-parameters and uses an elegant architecture for modeling. Experimental results show that the proposed attention-over-attention model significantly outperforms various state-of-the-art systems by a large margin in public datasets, such as CNN and Children's Book Test datasets.", "citation": "Citations (95)", "year": "2017", "departments": ["Harbin Institute of Technology", "Harbin Institute of Technology"], "conf": "acl", "authors": ["Yiming Cui.....http://dblp.org/pers/hd/c/Cui:Yiming", "Zhipeng Chen.....http://dblp.org/pers/hd/c/Chen:Zhipeng", "Si Wei.....http://dblp.org/pers/hd/w/Wei:Si", "Shijin Wang.....http://dblp.org/pers/hd/w/Wang:Shijin", "Ting Liu.....http://dblp.org/pers/hd/l/Liu_0001:Ting", "Guoping Hu.....http://dblp.org/pers/hd/h/Hu:Guoping"], "pages": 10}