{"title": "Learning the Reward Function for a Misspecified Model.", "fields": ["computer science", "pattern recognition", "reinforcement learning", "artificial intelligence", "machine learning"], "abstract": "In model-based reinforcement learning it is typical to treat the problems of learning the dynamics model and learning the reward function separately. However, when the dynamics model is flawed, it may generate erroneous states that would never occur in the true environment. A reward function trained only to map environment states to rewards (as is typical) would have little guidance in such states. This paper presents a novel error bound that accounts for the reward model's behavior in states sampled from the model. This bound is used to extend the existing Hallucinated DAgger-MC algorithm, which offers theoretical performance guarantees in deterministic MDPs that do not assume a perfect model can be learned. Empirically, this approach to reward learning can yield dramatic improvements in control performance when the dynamics model is flawed.", "citation": "Not cited", "departments": ["Franklin & Marshall College"], "authors": ["Erik Talvitie.....http://dblp.org/pers/hd/t/Talvitie:Erik"], "conf": "icml", "year": "2018", "pages": 10}