{"title": "On the Linear Convergence of the Proximal Gradient Method for Trace Norm Regularization.", "fields": ["proximal gradient methods", "convexity", "sublinear function", "regular polygon", "proximal gradient methods for learning"], "abstract": "Motivated by various applications in machine learning, the problem of minimizing a convex smooth loss function with trace norm regularization has received much attention lately. Currently, a popular method for solving such problem is the proximal gradient method (PGM), which is known to have a sublinear rate of convergence. In this paper, we show that for a large class of loss functions, the convergence rate of the PGM is in fact linear. Our result is established without any strong convexity assumption on the loss function. A key ingredient in our proof is a new Lipschitzian error bound for the aforementioned trace norm-regularized problem, which may be of independent interest.", "citation": "Citations (33)", "departments": ["The Chinese University of Hong Kong", "The Chinese University of Hong Kong", "The Chinese University of Hong Kong", "University of Minnesota"], "authors": ["Ke Hou.....http://dblp.org/pers/hd/h/Hou:Ke", "Zirui Zhou.....http://dblp.org/pers/hd/z/Zhou:Zirui", "Anthony Man-Cho So.....http://dblp.org/pers/hd/s/So:Anthony_Man=Cho", "Zhi-Quan Luo.....http://dblp.org/pers/hd/l/Luo:Zhi=Quan"], "conf": "nips", "year": "2013", "pages": 9}