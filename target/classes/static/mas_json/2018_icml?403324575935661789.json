{"title": "Regret Minimization for Partially Observable Deep Reinforcement Learning.", "fields": ["minification", "observable", "reinforcement learning", "regret", "counterfactual thinking"], "abstract": "Deep reinforcement learning algorithms that estimate state and state-action value functions have been shown to be effective in a variety of challenging domains, including learning control strategies from raw image pixels. However, algorithms that estimate state and state-action value functions typically assume a fully observed state and must compensate for partial or non-Markovian observations by using finite-length frame-history observations or recurrent networks. In this work, we propose a new deep reinforcement learning algorithm based on counterfactual regret minimization that iteratively updates an approximation to a cumulative clipped advantage function and is robust to partially observed state. We demonstrate that on several partially observed reinforcement learning tasks, this new class of algorithms can substantially outperform strong baseline methods: on Pong with single-frame observations, and on the challenging Doom (ViZDoom) and Minecraft (Malmo) first-person navigation benchmarks.", "citation": "Not cited", "departments": ["University of California, Berkeley", "University of California, Berkeley", "University of California, Berkeley"], "authors": ["Peter H. Jin.....http://dblp.org/pers/hd/j/Jin:Peter_H=", "Kurt Keutzer.....http://dblp.org/pers/hd/k/Keutzer:Kurt", "Sergey Levine.....http://dblp.org/pers/hd/l/Levine:Sergey"], "conf": "icml", "year": "2018", "pages": 10}