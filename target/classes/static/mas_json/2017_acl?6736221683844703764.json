{"title": "A Convolutional Encoder Model for Neural Machine Translation.", "fields": ["machine translation", "sentence", "architecture", "computation", "encoder"], "abstract": "The prevalent approach to neural machine translation relies on bi-directional LSTMs to encode the source sentence.\nIn this paper we present a faster and simpler architecture based on a succession of convolutional layers. \nThis allows to encode the entire source sentence simultaneously compared to recurrent networks for which computation is constrained by temporal dependencies.\nOn WMT'16 English-Romanian translation we achieve competitive accuracy to the state-of-the-art and we outperform several recently published results on the WMT'15 English-German task. \nOur models obtain almost the same accuracy as a very deep LSTM setup on WMT'14 English-French translation.\nOur convolutional encoder speeds up CPU decoding by more than two times at the same or higher accuracy as a strong bi-directional LSTM baseline.", "citation": "Citations (28)", "year": "2017", "departments": ["Facebook", "Facebook", "Facebook", "Facebook"], "conf": "acl", "authors": ["Jonas Gehring.....http://dblp.org/pers/hd/g/Gehring:Jonas", "Michael Auli.....http://dblp.org/pers/hd/a/Auli:Michael", "David Grangier.....http://dblp.org/pers/hd/g/Grangier:David", "Yann Dauphin.....http://dblp.org/pers/hd/d/Dauphin:Yann"], "pages": 13}