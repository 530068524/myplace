{"title": "Supporting x86-64 address translation for 100s of GPU lanes.", "fields": ["conventional memory", "uniform memory access", "flat memory model", "cuda pinned memory", "physical address"], "abstract": "Efficient memory sharing between CPU and GPU threads can greatly expand the effective set of GPGPU workloads. For increased programmability, this memory should be uniformly virtualized, necessitating compatible address translation support for GPU memory references. However, even a modest GPU might need 100s of translations per cycle (6 CUs * 64 lanes/CU) with memory access patterns designed for throughput more than locality.", "citation": "Citations (68)", "departments": ["University of Wisconsin-Madison", "University of Wisconsin-Madison", "University of Wisconsin-Madison"], "authors": ["Jason Power.....http://dblp.org/pers/hd/p/Power:Jason", "Mark D. Hill.....http://dblp.org/pers/hd/h/Hill:Mark_D=", "David A. Wood.....http://dblp.org/pers/hd/w/Wood:David_A="], "conf": "hpca", "year": "2014", "pages": 11}