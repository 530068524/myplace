{"title": "Threshold Bandits, With and Without Censored Feedback.", "fields": ["statistics", "mathematical optimization", "machine learning", "stochastic game", "sampling"], "abstract": "We consider the \\emph{Threshold Bandit} setting, a variant of the classical multi-armed bandit problem in which the reward on each round depends on a piece of side information known as a \\emph{threshold value}. The learner selects one of $K$ actions (arms), this action generates a random sample from a fixed distribution, and the action then receives a unit payoff in the event that this sample exceeds the threshold value. We consider two versions of this problem, the \\emph{uncensored} and \\emph{censored} case, that determine whether the sample is always observed or only when the threshold is not met. Using new tools to understand the popular UCB algorithm, we show that the uncensored case is essentially no more difficult than the classical multi-armed bandit setting. Finally we show that the censored case exhibits more challenges, but we give guarantees in the event that the sequence of threshold values is generated optimistically.", "citation": "Citations (1)", "departments": ["University of Michigan", "Massachusetts Institute of Technology", "Electrical Engi ... and Engineering"], "authors": ["Jacob D. Abernethy.....http://dblp.org/pers/hd/a/Abernethy:Jacob_D=", "Kareem Amin.....http://dblp.org/pers/hd/a/Amin:Kareem", "Ruihao Zhu.....http://dblp.org/pers/hd/z/Zhu:Ruihao"], "conf": "nips", "year": "2016", "pages": 9}