{"title": "Enhancing Recurrent Neural Networks with Positional Attention for Question Answering.", "fields": ["sentence", "question answering", "recurrent neural network", "artificial neural network", "interrogative word"], "abstract": "Attention based recurrent neural networks (RNN) have shown a great success for question answering (QA) in recent years. Although significant improvements have been achieved over the non-attentive models, the position information is not well studied within the attention-based framework. Motivated by the effectiveness of using the word positional context to enhance information retrieval, we assume that if a word in the question (i.e.,  question word ) occurs in an answer sentence, the neighboring words should be given more attention since they intuitively contain more valuable information for question answering than those far away. Based on this assumption, we propose a positional attention based RNN model, which incorporates the positional context of the question words into the answers' attentive representations. Experiments on two benchmark datasets show the great advantages of our proposed model. Specifically, we achieve a maximum improvement of 8.83% over the classical attention based RNN model in terms of mean average precision. Furthermore, our model is comparable to if not better than the state-of-the-art approaches for question answering.", "citation": "Citations (4)", "departments": ["East China Normal University", "East China Normal University", "York University", "East China Normal University", "East China Normal University"], "authors": ["Qin Chen.....http://dblp.org/pers/hd/c/Chen:Qin", "Qinmin Hu.....http://dblp.org/pers/hd/h/Hu:Qinmin", "Jimmy Xiangji Huang.....http://dblp.org/pers/hd/h/Huang:Jimmy_Xiangji", "Liang He.....http://dblp.org/pers/hd/h/He_0001:Liang", "Weijie An.....http://dblp.org/pers/hd/a/An:Weijie"], "conf": "sigir", "year": "2017", "pages": 4}