{"title": "Discovering Different Types of Topics: Factored Topic Models.", "fields": ["computer science", "machine learning", "topic model", "artificial intelligence", "natural language processing"], "abstract": "In traditional topic models such as LDA, a word is generated by choosing a topic from a collection. However, existing topic models do not identify different types of topics in a document, such as topics that represent the content and topics that represent the sentiment. In this paper, our goal is to discover such different types of topics, if they exist. We represent our model as several parallel topic models (called topic factors), where each word is generated from topics from these factors jointly. Since the latent membership of the word is now a vector, the learning algorithms become challenging. We show that using a variational approximation still allows us to keep the algorithm tractable. Our experiments over several datasets show that our approach consistently outperforms many classic topic models while also discovering fewer, more meaningful, topics.", "citation": "Citations (5)", "year": "2013", "departments": ["Cornell University", "Cornell University"], "conf": "ijcai", "authors": ["Yun Jiang.....http://dblp.org/pers/hd/j/Jiang:Yun", "Ashutosh Saxena.....http://dblp.org/pers/hd/s/Saxena:Ashutosh"], "pages": 8}