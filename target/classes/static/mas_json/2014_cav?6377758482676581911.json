{"title": "ICE: A Robust Framework for Learning Invariants.", "fields": ["counterexample", "invariant", "scalar", "scalability", "monotonic function"], "abstract": "Invariant generation lies at the heart of automated program verification, and the learning paradigm for synthesizing invariants is a new promising approach to solve this important problem. Unlike white-box techniques that try to generate an invariant by analyzing the program, learning approaches try to synthesize the invariant given concrete configurations that the invariant must include and exclude, and are algorithmically based on learning theory and scalable machine learning algorithms. In this paper we argue that traditional learning paradigms that use concrete examples and counterexamples are inherently non-robust for synthesizing invariants. We introduce a more general learning paradigm, called ICE-learning, that learns using examples, counter-examples, and implications, and show that this paradigm allows building honest teachers and convergent mechanisms for invariant synthesis. We study the new paradigm of ICE learning, develop several monotonic ICE-learning algorithms, and two classes of non-monotonic domains for learning numerical invariants for scalar variables as well as quantified invariants for arrays and dynamic lists, and establish convergence results for them. We implement these ICE algorithms in a prototype verifier and show that the robustness of ICE-learning is practical and effective by evaluating them on a class of programs.", "citation": "Citations (7)", "departments": [], "authors": ["Pranav Garg.....http://dblp.org/pers/hd/g/Garg_0001:Pranav", "Christof L\u00f6ding.....http://dblp.org/pers/hd/l/L=ouml=ding:Christof", "P. Madhusudan.....http://dblp.org/pers/hd/m/Madhusudan:P=", "Daniel Neider.....http://dblp.org/pers/hd/n/Neider:Daniel"], "conf": "cav", "year": "2014", "pages": 19}