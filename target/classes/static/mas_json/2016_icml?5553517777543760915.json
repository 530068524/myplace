{"title": "On the Power and Limits of Distance-Based Learning.", "fields": ["wish", "computer science", "artificial intelligence", "machine learning"], "abstract": "We initiate the study of low-distortion finite metric embeddings in multi-class (and multi-label) classification where (i) both the space of input instances and the space of output classes have combinatorial metric structure, and (ii) the concepts we wish to learn are low-distortion embeddings. We develop new geometric techniques and prove strong learning lower bounds. These provable limits hold even when we allow learners and classifiers to get advice by one or more experts. Our study overwhelmingly indicates that post-geometry assumptions are necessary in multi-class classification, as in natural language processing (NLP). Technically, the mathematical tools we developed in this work could be of independent interest to NLP. To the best of our knowledge, this is the first work which formally studies classification problems in combinatorial spaces and where the concepts are low-distortion embeddings.", "citation": "Not cited", "year": "2016", "departments": ["Rutgers University", "The Graduate Center, CUNY", "Chinese Academy of Sciences"], "conf": "icml", "authors": ["Periklis A. Papakonstantinou.....http://dblp.org/pers/hd/p/Papakonstantinou:Periklis_A=", "Jia Xu.....http://dblp.org/pers/hd/x/Xu:Jia", "Guang Yang.....http://dblp.org/pers/hd/y/Yang:Guang"], "pages": 9}