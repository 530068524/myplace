{"title": "Fully distributed EM for very large datasets.", "fields": ["data set", "artificial intelligence", "topic model", "computation", "machine learning"], "abstract": "In EM and related algorithms, E-step computations distribute easily, because data items are independent given parameters. For very large data sets, however, even storing all of the parameters in a single node for the M-step can be impractical. We present a framework that fully distributes the entire EM procedure. Each node interacts only with parameters relevant to its data, sending messages to other nodes along a junction-tree topology. We demonstrate improvements over a MapReduce topology, on two tasks: word alignment and topic modeling.", "citation": "Citations (100)", "year": "2008", "departments": ["University of California, Berkeley", "University of California, Berkeley", "University of California, Berkeley"], "conf": "icml", "authors": ["Jason Andrew Wolfe.....http://dblp.org/pers/hd/w/Wolfe:Jason_Andrew", "Aria Haghighi.....http://dblp.org/pers/hd/h/Haghighi:Aria", "Dan Klein.....http://dblp.org/pers/hd/k/Klein:Dan"], "pages": 8}