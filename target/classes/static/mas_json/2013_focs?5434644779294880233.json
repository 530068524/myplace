{"title": "OSNAP: Faster Numerical Linear Algebra Algorithms via Sparser Subspace Embeddings.", "fields": ["johnson lindenstrauss lemma", "singular value", "low rank approximation", "subspace topology", "sparse matrix"], "abstract": "An oblivious subspace embedding (OSE) given some parameters e, d is a distribution D over matrices \u03a0 \u2208 R m\u00d7n  such that for any linear subspace W \u2286 R n  with dim(W) = d, P \u03a0~D (\u2200x \u2208 W ||\u03a0x|| 2  \u2208 (1 \u00b1 e)||x|| 2 ) > 2/3. We show that a certain class of distributions, Oblivious Sparse Norm-Approximating Projections (OSNAPs), provides OSE's with m = O(d 1+\u03b3 /e 2 ), and where every matrix \u03a0 in the support of the OSE has only s = O \u03b3 (1/e) non-zero entries per column, for \u03b3 > 0 any desired constant. Plugging OSNAPs into known algorithms for approximate least squares regression, l p  regression, low rank approximation, and approximating leverage scores implies faster algorithms for all these problems. Our main result is essentially a Bai-Yin type theorem in random matrix theory and is likely to be of independent interest: we show that for any fixed U \u2208 R n\u00d7d  with orthonormal columns and random sparse \u03a0, all singular values of \u03a0U lie in [1 - e, 1 + e] with good probability. This can be seen as a generalization of the sparse Johnson-Lindenstrauss lemma, which was concerned with d = 1. Our methods also recover a slightly sharper version of a main result of [Clarkson-Woodruff, STOC 2013], with a much simpler proof. That is, we show that OSNAPs give an OSE with m = O(d 2 /e 2 ), s = 1.", "citation": "Citations (156)", "year": "2013", "departments": ["Harvard University", "Princeton University"], "conf": "focs", "authors": ["Jelani Nelson.....http://dblp.org/pers/hd/n/Nelson:Jelani", "Huy L. Nguyen.....http://dblp.org/pers/hd/n/Nguyen:Huy_L="], "pages": 10}