{"title": "SeqGAN: Sequence Generative Adversarial Nets with Policy Gradient.", "fields": ["generative model", "deep learning", "generative topographic map", "generative design", "complete sequence"], "abstract": "As a new way of training generative models, Generative Adversarial\nNet (GAN) that uses a discriminative model to guide\nthe training of the generative model has enjoyed considerable\nsuccess in generating real-valued data. However, it has limitations\nwhen the goal is for generating sequences of discrete\ntokens. A major reason lies in that the discrete outputs from\nthe generative model make it difficult to pass the gradient update\nfrom the discriminative model to the generative model.\nAlso, the discriminative model can only assess a complete\nsequence, while for a partially generated sequence, it is nontrivial\nto balance its current score and the future one once\nthe entire sequence has been generated. In this paper, we propose\na sequence generation framework, called SeqGAN, to\nsolve the problems. Modeling the data generator as a stochastic\npolicy in reinforcement learning (RL), SeqGAN bypasses\nthe generator differentiation problem by directly performing\ngradient policy update. The RL reward signal comes from\nthe GAN discriminator judged on a complete sequence, and\nis passed back to the intermediate state-action steps using\nMonte Carlo search. Extensive experiments on synthetic data\nand real-world tasks demonstrate significant improvements\nover strong baselines.", "citation": "Citations (227)", "departments": ["Shanghai Jiao Tong University", "University College London", "East China Normal University", "Shanghai Jiao Tong University"], "authors": ["Lantao Yu.....http://dblp.org/pers/hd/y/Yu:Lantao", "Weinan Zhang.....http://dblp.org/pers/hd/z/Zhang:Weinan", "Jun Wang.....http://dblp.org/pers/hd/w/Wang_0012:Jun", "Yong Yu.....http://dblp.org/pers/hd/y/Yu:Yong"], "conf": "aaai", "year": "2017", "pages": 7}