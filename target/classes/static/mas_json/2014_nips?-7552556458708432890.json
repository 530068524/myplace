{"title": "Deep Networks with Internal Selective Attention through Feedback Connections.", "fields": ["parameter space", "convolutional neural network", "architecture", "scalability", "feed forward"], "abstract": "Traditional convolutional neural networks (CNN) are stationary and feedforward. They neither change their parameters during evaluation nor use feedback from higher to lower layers. Real brains, however, do. So does our Deep Attention Selective Network (dasNet) architecture. DasNet's feedback structure can dynamically alter its convolutional filter sensitivities during classification. It harnesses the power of sequential processing to improve classification performance, by allowing the network to iteratively focus its internal attention on some of its convolutional filters. Feedback is trained through direct policy search in a huge million-dimensional parameter space, through scalable natural evolution strategies (SNES). On the CIFAR-10 and CIFAR-100 datasets, dasNet outperforms the previous state-of-the-art model on unaugmented datasets.", "citation": "Citations (112)", "year": "2014", "departments": ["Dalle Molle Institute for Artificial Intelligence Research", "Dalle Molle Institute for Artificial Intelligence Research", "Dalle Molle Institute for Artificial Intelligence Research", "Dalle Molle Institute for Artificial Intelligence Research"], "conf": "nips", "authors": ["Marijn F. Stollenga.....http://dblp.org/pers/hd/s/Stollenga:Marijn_F=", "Jonathan Masci.....http://dblp.org/pers/hd/m/Masci:Jonathan", "Faustino J. Gomez.....http://dblp.org/pers/hd/g/Gomez:Faustino_J=", "J\u00fcrgen Schmidhuber.....http://dblp.org/pers/hd/s/Schmidhuber:J=uuml=rgen"], "pages": 9}