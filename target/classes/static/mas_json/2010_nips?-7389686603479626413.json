{"title": "Feature Construction for Inverse Reinforcement Learning.", "fields": ["reward based selection", "inverse", "well defined", "markov decision process", "q learning"], "abstract": "The goal of inverse reinforcement learning is to find a reward function for a Markov decision process, given example traces from its optimal policy. Current IRL techniques generally rely on user-supplied features that form a concise basis for the reward. We present an algorithm that instead constructs reward features from a large collection of component features, by building logical conjunctions of those component features that are relevant to the example policy. Given example traces, the algorithm returns a reward function as well as the constructed features. The reward function can be used to recover a full, deterministic, stationary policy, and the features can be used to transplant the reward function into any novel environment on which the component features are well defined.", "citation": "Citations (55)", "departments": ["Stanford University", "University of Washington", "Stanford University"], "authors": ["Sergey Levine.....http://dblp.org/pers/hd/l/Levine:Sergey", "Zoran Popovic.....http://dblp.org/pers/hd/p/Popovic:Zoran", "Vladlen Koltun.....http://dblp.org/pers/hd/k/Koltun:Vladlen"], "conf": "nips", "year": "2010", "pages": 9}