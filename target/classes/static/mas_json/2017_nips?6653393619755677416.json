{"title": "On the Power of Truncated SVD for General High-rank Matrix Estimation Problems.", "fields": ["matrix norm", "estimation of covariance matrices", "matrix completion", "rank", "singular value decomposition"], "abstract": "We show that given an estimate $\\widehat{\\mat A}$ that is close to a general high-rank positive semi-definite (PSD) matrix $\\mat A$ in spectral norm (i.e., $\\|\\widehat{\\mat A}-\\mat A\\|_2 \\leq \\delta$), the simple truncated Singular Value Decomposition of $\\widehat{\\mat A}$ produces a multiplicative approximation of $\\mat A$ in Frobenius norm. This observation leads to many interesting results on general high-rank matrix estimation problems: 1.High-rank matrix completion: we show that it is possible to recover a {general high-rank matrix} $\\mat A$ up to $(1+\\varepsilon)$ relative error in Frobenius norm from partial observations, with sample complexity independent of the spectral gap of $\\mat A$. 2.High-rank matrix denoising: we design algorithms that recovers a matrix $\\mat A$ with relative error in Frobenius norm from its noise-perturbed observations, without assuming $\\mat A$ is exactly low-rank. 3.Low-dimensional estimation of high-dimensional covariance: given $N$ i.i.d.~samples of dimension $n$ from $\\mathcal N_n(\\mat 0,\\mat A)$, we show that it is possible to estimate the covariance matrix $\\mat A$ with relative error in Frobenius norm with $N\\approx n$,improving over classical covariance estimation results which requires $N\\approx n^2$.", "citation": "Citations (1)", "year": "2017", "departments": ["Carnegie Mellon University", "Carnegie Mellon University", "Carnegie Mellon University"], "conf": "nips", "authors": ["Simon S. Du.....http://dblp.org/pers/hd/d/Du:Simon_S=", "Yining Wang.....http://dblp.org/pers/hd/w/Wang:Yining", "Aarti Singh.....http://dblp.org/pers/hd/s/Singh:Aarti"], "pages": 11}