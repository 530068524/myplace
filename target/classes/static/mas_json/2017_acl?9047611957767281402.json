{"title": "Topically Driven Neural Language Model.", "fields": ["perplexity", "language model", "architecture", "topic model", "sentence"], "abstract": "Language models are typically applied at the sentence level, without access to the broader document context. We present a neural language model that incorporates document context in the form of a topic model-like architecture, thus providing a succinct representation of the broader document context outside of the current sentence. Experiments over a range of datasets demonstrate that our model outperforms a pure sentence-based model in terms of language model perplexity, and leads to topics that are potentially more coherent than those produced by a standard LDA topic model. Our model also has the ability to generate related sentences for a topic, providing another way to interpret topics.", "citation": "Citations (1)", "year": "2017", "departments": ["King's College London", "University of Melbourne", "University of Melbourne"], "conf": "acl", "authors": ["Jey Han Lau.....http://dblp.org/pers/hd/l/Lau:Jey_Han", "Timothy Baldwin.....http://dblp.org/pers/hd/b/Baldwin:Timothy", "Trevor Cohn.....http://dblp.org/pers/hd/c/Cohn:Trevor"], "pages": 11}