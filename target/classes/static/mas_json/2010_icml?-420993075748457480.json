{"title": "Non-Local Contrastive Objectives.", "fields": ["divergence", "maximum likelihood", "trade off", "log linear model", "inference"], "abstract": "Pseudo-likelihood and contrastive divergence are two well-known examples of contrastive methods. These algorithms trade off the probability of the correct label with the probabilities of other \"nearby\" instantiations. In this paper we explore more general types of contrastive objectives, which trade off the probability of the correct label against an arbitrary set of other instantiations. We prove that a large class of contrastive objectives are consistent with maximum likelihood, even for finite amounts of data. This result generalizes asymptotic consistency for pseudo-likelihood. The proof gives significant insight into contrastive objectives, suggesting that they enforce (soft) probability-ratio constraints between pairs of instantiations. Based on this insight, we propose Contrastive Constraint Generation (CCG), an iterative constraint-generation style algorithm that allows us to learn a log-linear model using only MAP inference. We evaluate CCG on a scene classification task, showing that it significantly outperforms pseudo-likelihood, contrastive divergence, and a well-known margin-based method.", "citation": "Citations (14)", "departments": ["Stanford University", "Stanford University", "Stanford University"], "authors": ["David Vickrey.....http://dblp.org/pers/hd/v/Vickrey:David", "Cliff Chiung-Yu Lin.....http://dblp.org/pers/hd/l/Lin:Cliff_Chiung=Yu", "Daphne Koller.....http://dblp.org/pers/hd/k/Koller:Daphne"], "conf": "icml", "year": "2010", "pages": 8}