{"title": "Input Switched Affine Networks: An RNN Architecture Designed for Interpretability.", "fields": ["interpretability", "architecture", "software deployment", "linear subspace", "affine transformation"], "abstract": "There exist many problem domains where the interpretability of neural network models is essential for deployment. Here we introduce a recurrent architecture composed of input-switched affine transformations - in other words an RNN without any explicit nonlinearities, but with input-dependent recurrent weights. This simple form allows the RNN to be analyzed via straightforward linear methods: we can exactly characterize the linear contribution of each input to the model predictions; we can use a change-of-basis to disentangle input, output, and computational hidden unit subspaces; we can fully reverse-engineer the architecture's solution to a simple task. Despite this ease of interpretation, the input switched affine network achieves reasonable performance on a text modeling tasks, and allows greater computational efficiency than networks with standard nonlinearities.", "citation": "Citations (2)", "year": "2017", "departments": ["University of Oxford", "Google", "University of California, Berkeley", "University of Wroc\u0142aw", "Google"], "conf": "icml", "authors": ["Jakob N. Foerster.....http://dblp.org/pers/hd/f/Foerster:Jakob_N=", "Justin Gilmer.....http://dblp.org/pers/hd/g/Gilmer:Justin", "Jascha Sohl-Dickstein.....http://dblp.org/pers/hd/s/Sohl=Dickstein:Jascha", "Jan Chorowski.....http://dblp.org/pers/hd/c/Chorowski:Jan", "David Sussillo.....http://dblp.org/pers/hd/s/Sussillo:David"], "pages": 10}