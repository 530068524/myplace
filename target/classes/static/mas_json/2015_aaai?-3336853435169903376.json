{"title": "Solving Games with Functional Regret Estimation.", "fields": ["extensive form game", "corollary", "function approximation", "regret", "abstraction"], "abstract": "We propose a novel online learning method for minimizing regret in large extensive-form games. The approach learns a function approximator online to estimate the regret for choosing a particular action. A no-regret algorithm uses these estimates in place of the true regrets to define a sequence of policies.\n\nWe prove the approach sound by providing a bound relating the quality of the function approximation and regret of the algorithm. A corollary being that the method is guaranteed to converge to a Nash equilibrium in self-play so long as the regrets are ultimately realizable by the function approximator. Our technique can be understood as a principled generalization of existing work on abstraction in large games; in our work, both the abstraction as well as the equilibrium are learned during self-play. We demonstrate empirically the method achieves higher quality strategies than state-of-the-art abstraction techniques given the same resources.", "citation": "Citations (11)", "departments": ["Carnegie Mellon University", "University of Alberta", "Carnegie Mellon University", "University of Alberta"], "authors": ["Kevin Waugh.....http://dblp.org/pers/hd/w/Waugh:Kevin", "Dustin Morrill.....http://dblp.org/pers/hd/m/Morrill:Dustin", "James Andrew Bagnell.....http://dblp.org/pers/hd/b/Bagnell:James_Andrew", "Michael H. Bowling.....http://dblp.org/pers/hd/b/Bowling:Michael_H="], "conf": "aaai", "year": "2015", "pages": 8}