{"title": "Agnostic Learning of Monomials by Halfspaces Is Hard.", "fields": ["long line", "list decoding", "decision list", "monomial", "machine learning", "mathematics", "corollary", "existential quantification", "discrete mathematics", "concept class", "mathematical proof", "artificial intelligence"], "abstract": "We prove the following strong hardness result for learning: Given a distribution of labeled examples from the hypercube such that there exists a monomial consistent with $(1-\\epsilon)$ of the examples it is NP-hard to find a halfspace that is correct on $(1/2+\\epsilon)$ of the examples for arbitrary constants $\\epsilon>0$. In learning theory terms, weak agnostic learning of monomials is hard, even if one is allowed to output a hypothesis from the much bigger concept class of halfspaces. This hardness result subsumes a long line of previous results, including two recent hardness results for the proper learning of monomials and halfspaces. As an immediate corollary of our result we show that weak agnostic learning of decision lists is NP-hard. Our techniques are quite different from previous hardness proofs for learning. We define distributions on positive and negative examples for monomials whose first few moments match. We use the invariance principle to argue that regular halfspaces (all of whose coeffic...", "citation": "Citations (90)", "departments": ["IBM", "Carnegie Mellon University", "Microsoft", "Carnegie Mellon University"], "authors": ["Vitaly Feldman.....http://dblp.org/pers/hd/f/Feldman:Vitaly", "Venkatesan Guruswami.....http://dblp.org/pers/hd/g/Guruswami:Venkatesan", "Prasad Raghavendra.....http://dblp.org/pers/hd/r/Raghavendra:Prasad", "Yi Wu.....http://dblp.org/pers/hd/w/Wu_0002:Yi"], "conf": "focs", "year": "2009", "pages": 10}