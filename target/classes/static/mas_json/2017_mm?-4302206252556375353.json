{"title": "Enhancing Micro-video Understanding by Harnessing External Sounds.", "fields": ["computer vision", "feature learning", "mobile device", "semantics", "machine learning"], "abstract": "Different from traditional long videos, micro-videos are much shorter and usually recorded at a specific place with mobile devices. To better understand the semantics of a micro-video and facilitate downstream applications, it is crucial to estimate the venue where the micro-video is recorded, for example, in a concert or on a beach. However, according to our statistics over two million micro-videos, only $1.22%$ of them were labeled with location information. For the remaining large number of micro-videos without location information, we have to rely on their content to estimate their venue categories. This is a highly challenging task, as micro-videos are naturally multi-modal (with textual, visual and, acoustic content), and more importantly, the quality of each modality varies greatly for different micro-videos.   In this work, we focus on enhancing the acoustic modality for the venue category estimation task. This is motivated by our finding that although the acoustic signal can well complement the visual and textual signal in reflecting a micro-video's venue, its quality is usually relatively lower. As such, simply integrating acoustic features with visual and textual features only leads to suboptimal results, or even adversely degrades the overall performance ( cf  the barrel theory). To address this, we propose to compensate the shortest board --- the acoustic modality --- via harnessing the external sound knowledge. We develop a deep transfer model which can jointly enhance the concept-level representation of micro-videos and the venue category prediction. To alleviate the sparsity problem of unpopular categories, we further regularize the representation learning of micro-videos of the same venue category. Through extensive experiments on a real-world dataset, we show that our model significantly outperforms the state-of-the-art method in terms of both Micro-F1 and Macro-F1 scores by leveraging the external acoustic knowledge.", "citation": "Citations (2)", "departments": ["Shandong University", "National University of Singapore", "Communication University of China", "National University of Singapore", "Columbia University"], "authors": ["Liqiang Nie.....http://dblp.org/pers/hd/n/Nie:Liqiang", "Xiang Wang.....http://dblp.org/pers/hd/w/Wang:Xiang", "Jianglong Zhang.....http://dblp.org/pers/hd/z/Zhang:Jianglong", "Xiangnan He.....http://dblp.org/pers/hd/h/He_0001:Xiangnan", "Hanwang Zhang.....http://dblp.org/pers/hd/z/Zhang:Hanwang", "Richang Hong.....http://dblp.org/pers/hd/h/Hong:Richang", "Qi Tian.....http://dblp.org/pers/hd/t/Tian_0001:Qi"], "conf": "mm", "year": "2017", "pages": 9}