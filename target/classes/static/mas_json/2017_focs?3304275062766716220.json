{"title": "On Learning Mixtures of Well-Separated Gaussians.", "fields": ["exponential growth", "k means clustering", "diagonally dominant matrix", "mixture model", "boosting"], "abstract": "We consider the problem of efficiently learning mixtures of a large number of spherical Gaussians, when the components of the mixture are well separated. In the most basic form of this problem, we are given samples from a uniform mixture of k standard spherical Gaussians with means mu_1,...,mu_k in R^d, and the goal is to estimate the means up to accuracy \u03b4 using poly(k,d, 1/\u03b4) samples.In this work, we study the following question: what is the minimum separation needed between the means for solving this task? The best known algorithm due to Vempala and Wang [JCSS 2004] requires a separation of roughly min{k,d}^{1/4}. On the other hand, Moitra and Valiant [FOCS 2010] showed that with separation o(1), exponentially many samples are required. We address the significant gap between these two bounds, by showing the following results.\u2022 We show that with separation o(\u221alog k), super-polynomially many samples are required. In fact, this holds even when the k means of the Gaussians are picked at random in d=O(log k) dimensions.\u2022 We show that with separation \u03a9(\u221alog k), poly(k,d,1/\u03b4) samples suffice. Notice that the bound on the separation is independent of \u03b4. This result is based on a new and efficient accuracy boosting algorithm that takes as input coarse estimates of the true means and in time (and samples) poly(k,d, 1\u03b4) outputs estimates of the means up to arbitrarily good accuracy \u03b4 assuming the separation between the means is \u03a9min \u221a(log k),\u221ad) (independently of \u03b4). The idea of the algorithm is to iteratively solve a diagonally dominant system of non-linear equations.We also (1) present a computationally efficient algorithm in d=O(1) dimensions with only \u03a9(\u221a{d}) separation, and (2) extend our results to the case that components might have different weights and variances. These results together essentially characterize the optimal order of separation between components that is needed to learn a mixture of k spherical Gaussians with polynomial samples.", "citation": "Citations (2)", "departments": ["New York University", "Electrical Engi ... omputer Science"], "authors": ["Oded Regev.....http://dblp.org/pers/hd/r/Regev_0001:Oded", "Aravindan Vijayaraghavan.....http://dblp.org/pers/hd/v/Vijayaraghavan:Aravindan"], "conf": "focs", "year": "2017", "pages": 12}