{"title": "Sharing Residual Units Through Collective Tensor Factorization To Improve Deep Neural Networks.", "fields": ["knowledge sharing", "tensor", "residual", "factorization", "cru"], "abstract": "Residual units are wildly used for alleviating optimization difficulties when building deep neural networks. However, the performance gain does not well compensate the model size increase, indicating low parameter efficiency in these residual units. In this work, we first revisit the residual function in several variations of residual units and demonstrate that these residual functions can actually be explained with a unified framework based on generalized block term decomposition. Then, based on the new explanation, we propose a new architecture, Collective Residual Unit (CRU), which enhances the parameter efficiency of deep neural networks through collective tensor factorization. CRU enables knowledge sharing across different residual units using shared factors. Experimental results show that our proposed CRU Network demonstrates outstanding parameter efficiency, achieving comparable classification performance to ResNet-200 with the model size of ResNet-50. By building a deeper network using CRU, we can achieve state-of-the-art single model classification accuracy on ImageNet-1k and Places365-Standard benchmark datasets. (Code and trained models are available on GitHub)", "citation": "Citations (6)", "departments": ["National University of Singapore", "National University of Singapore", "National University of Singapore", "National University of Singapore", "National University of Singapore"], "authors": ["Yunpeng Chen.....http://dblp.org/pers/hd/c/Chen:Yunpeng", "Xiaojie Jin.....http://dblp.org/pers/hd/j/Jin:Xiaojie", "Bingyi Kang.....http://dblp.org/pers/hd/k/Kang:Bingyi", "Jiashi Feng.....http://dblp.org/pers/hd/f/Feng:Jiashi", "Shuicheng Yan.....http://dblp.org/pers/hd/y/Yan:Shuicheng"], "conf": "ijcai", "year": "2018", "pages": 7}