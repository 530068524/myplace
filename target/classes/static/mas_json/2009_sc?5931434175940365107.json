{"title": "Minimizing communication in sparse matrix solvers.", "fields": ["cache", "generalized minimal residual method", "shared memory", "sparse matrix", "kernel method"], "abstract": "Data communication within the memory system of a single processor node and between multiple nodes in a system is the bottleneck in many iterative sparse matrix solvers like CG and GMRES. Here  k  iterations of a conventional implementation perform  k  sparse-matrix-vector-multiplications and \u03a9( k ) vector operations like dot products, resulting in communication that grows by a factor of \u03a9( k ) in both the memory and network. By reorganizing the sparse-matrix kernel to compute a set of matrix-vector products at once and reorganizing the rest of the algorithm accordingly, we can perform  k  iterations by sending  O (log  P ) messages instead of  O ( k  \u00b7 log  P ) messages on a parallel machine, and reading the matrix  A  from DRAM to cache just once, instead of  k  times on a sequential machine. This reduces communication to the minimum possible. We combine these techniques to form a new variant of GMRES. Our shared-memory implementation on an 8-core Intel Clovertown gets speedups of up to 4.3x over standard GMRES, without sacrificing convergence rate or numerical stability.", "citation": "Citations (141)", "year": "2009", "departments": ["University of California, Berkeley", "University of California, Berkeley", "University of California, Berkeley", "University of California, Berkeley"], "conf": "sc", "authors": ["Marghoob Mohiyuddin.....http://dblp.org/pers/hd/m/Mohiyuddin:Marghoob", "Mark Hoemmen.....http://dblp.org/pers/hd/h/Hoemmen:Mark", "James Demmel.....http://dblp.org/pers/hd/d/Demmel:James", "Katherine A. Yelick.....http://dblp.org/pers/hd/y/Yelick:Katherine_A="], "pages": -1}