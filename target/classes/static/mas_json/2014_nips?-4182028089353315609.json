{"title": "Sparse PCA via Covariance Thresholding.", "fields": ["kernel", "sparse pca", "random matrix", "thresholding", "diagonal", "covariance"], "abstract": "In sparse principal component analysis we are given noisy observations of a low-rank matrix of dimension n x p and seek to reconstruct it under additional sparsity assumptions. In particular, we assume here that the principal components v1,..., vr have at most k1, \u00b7 \u00b7 \u00b7 , kq non-zero entries respectively, and study the high-dimensional regime in which p is of the same order as n.\n\nIn an influential paper, Johnstone and Lu [JL04] introduced a simple algorithm that estimates the support of the principal vectors v1,..., vr by the largest entries in the diagonal of the empirical covariance. This method can be shown to succeed with high probability if kq \u2264 C1 \u221an/ log p, and to fail with high probability if kq \u2265 C2 \u221an/ log p for two constants 0 < C1, C2 < \u221e. Despite a considerable amount of work over the last ten years, no practical algorithm exists with provably better support recovery guarantees.\n\nHere we analyze a covariance thresholding algorithm that was recently proposed by Krauthgamer, Nadler and Vilenchik [KNV13]. We confirm empirical evidence presented by these authors and rigorously prove that the algorithm succeeds with high probability for k of order \u221an. Recent conditional lower bounds [BR13] suggest that it might be impossible to do significantly better.\n\nThe key technical component of our analysis develops new bounds on the norm of kernel random matrices, in regimes that were not considered before.", "citation": "Citations (26)", "year": "2014", "departments": ["Stanford University", "Stanford University", "Stanford University", "Stanford University"], "conf": "nips", "authors": ["Yash Deshpande.....http://dblp.org/pers/hd/d/Deshpande:Yash", "Andrea Montanari.....http://dblp.org/pers/hd/m/Montanari:Andrea"], "pages": 9}