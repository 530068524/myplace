{"title": "A POMDP Extension with Belief-dependent Rewards.", "fields": ["regular polygon", "observable", "partially observable markov decision process", "markov decision process", "proper convex function"], "abstract": "Partially Observable Markov Decision Processes (POMDPs) model sequential decision-making problems under uncertainty and partial observability. Unfortunately, some problems cannot be modeled with state-dependent reward functions, e.g., problems whose objective explicitly implies reducing the uncertainty on the state. To that end, we introduce \u03c1POMDPs, an extension of POMDPs where the reward function \u03c1 depends on the belief state. We show that, under the common assumption that \u03c1 is convex, the value function is also convex, what makes it possible to (1) approximate \u03c1 arbitrarily well with a piecewise linear and convex (PWLC) function, and (2) use state-of-the-art exact or approximate solving algorithms with limited changes.", "citation": "Citations (68)", "year": "2010", "departments": ["Nancy-Universit\u00e9", "Nancy-Universit\u00e9", "Nancy-Universit\u00e9", "Laboratoire Lor ... es Applications"], "conf": "nips", "authors": ["Mauricio Araya-L\u00f3pez.....http://dblp.org/pers/hd/a/Araya=L=oacute=pez:Mauricio", "Olivier Buffet.....http://dblp.org/pers/hd/b/Buffet:Olivier", "Vincent Thomas.....http://dblp.org/pers/hd/t/Thomas:Vincent", "Fran\u00e7ois Charpillet.....http://dblp.org/pers/hd/c/Charpillet:Fran=ccedil=ois"], "pages": 9}