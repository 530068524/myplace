{"title": "Architecture-Adaptive Code Variant Tuning.", "fields": ["implementation", "offline learning", "benchmarking", "cuda", "multi task learning"], "abstract": "Code variants represent alternative implementations of a computation, and are common in high-performance libraries and applications to facilitate selecting the most appropriate implementation for a specific execution context (target architecture and input dataset). Automating code variant selection typically relies on machine learning to construct a model during an offline learning phase that can be quickly queried at runtime once the execution context is known. In this paper, we define a new approach called architecture-adaptive code variant tuning, where the variant selection model is learned on a set of source architectures, and then used to predict variants on a new target architecture without having to repeat the training process. We pose this as a multi-task learning problem, where each source architecture corresponds to a task; we use device features in the construction of the variant selection model. This work explores the effectiveness of multi-task learning and the impact of different strategies for device feature selection. We evaluate our approach on a set of benchmarks and a collection of six NVIDIA GPU architectures from three distinct generations. We achieve performance results that are mostly comparable to the previous approach of tuning for a single GPU architecture without having to repeat the learning phase.", "citation": "Citations (7)", "year": "2016", "departments": ["University of Utah", "University of Utah", "University of Utah", "Nvidia", "Indian Institute of Technology Kanpur"], "conf": "asplos", "authors": ["Saurav Muralidharan.....http://dblp.org/pers/hd/m/Muralidharan:Saurav", "Amit Roy.....http://dblp.org/pers/hd/r/Roy:Amit", "Mary W. Hall.....http://dblp.org/pers/hd/h/Hall:Mary_W=", "Michael Garland.....http://dblp.org/pers/hd/g/Garland:Michael", "Piyush Rai.....http://dblp.org/pers/hd/r/Rai:Piyush"], "pages": 14}