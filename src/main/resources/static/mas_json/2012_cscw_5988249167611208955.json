{"title": "Shepherding the crowd yields better work.", "fields": ["crowdsourcing", "human computer interaction", "massively parallel", "computer science", "knowledge management"], "abstract": "Micro-task platforms provide massively parallel, on-demand labor. However, it can be difficult to reliably achieve high-quality work because online workers may behave irresponsibly, misunderstand the task, or lack necessary skills. This paper investigates whether timely, task-specific feedback helps crowd workers learn, persevere, and produce better results. We investigate this question through Shepherd, a feedback system for crowdsourced work. In a between-subjects study with three conditions, crowd workers wrote consumer reviews for six products they own. Participants in the None condition received no immediate feedback, consistent with most current crowdsourcing practices. Participants in the Self-assessment condition judged their own work. Participants in the External assessment condition received expert feedback. Self-assessment alone yielded better overall work than the None condition and helped workers improve over time. External assessment also yielded these benefits. Participants who received external assessment also revised their work more. We conclude by discussing interaction and infrastructure approaches for integrating real-time assessment into online work.", "citation": "Citations (236)", "departments": ["Carnegie Mellon University", "University of California, Berkeley", "Stanford University", "University of California, Berkeley"], "authors": ["Steven Dow.....http://dblp.org/pers/hd/d/Dow:Steven", "Anand Pramod Kulkarni.....http://dblp.org/pers/hd/k/Kulkarni:Anand_Pramod", "Scott R. Klemmer.....http://dblp.org/pers/hd/k/Klemmer:Scott_R=", "Bj\u00f6rn Hartmann.....http://dblp.org/pers/hd/h/Hartmann:Bj=ouml=rn"], "conf": "cscw", "year": "2012", "pages": 10}