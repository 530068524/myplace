{"title": "Sparse Prediction with the $k$-Support Norm.", "fields": ["regular polygon", "elastic net regularization", "mathematical optimization", "norm", "machine learning"], "abstract": "We derive a novel norm that corresponds to the tightest convex relaxation of sparsity combined with an l1 penalty. We show that this new k-support norm provides a tighter relaxation than the elastic net and can thus be advantageous in in sparse prediction problems. We also bound the looseness of the elastic net, thus shedding new light on it and providing justification for its use.", "citation": "Citations (101)", "departments": ["Stanford University", "Toyota Technological Institute at Chicago", "\u00c9ole Centrale Paris"], "authors": ["Andreas Argyriou.....http://dblp.org/pers/hd/a/Argyriou:Andreas", "Rina Foygel.....http://dblp.org/pers/hd/f/Foygel:Rina", "Nathan Srebro.....http://dblp.org/pers/hd/s/Srebro:Nathan"], "conf": "nips", "year": "2012", "pages": 9}