{"title": "Reducing the Unlabeled Sample Complexity of Semi-Supervised Multi-View Learning.", "fields": ["stability", "probably approximately correct learning", "sampling distribution", "learning classifier system", "active learning"], "abstract": "In semi-supervised multi-view learning, unlabeled sample complexity (u.s.c.) specifies the size of unlabeled training sample that guarantees a desired learning error. In this paper, we improve the state-of-art u.s.c. from  O (1/e) to  O (log 1/e) for small error e, under mild conditions. To obtain the improved result, as a primary step we prove a connection between the generalization error of a classifier and its incompatibility, which measures the fitness between the classifier and the sample distribution. We then prove that with a sufficiently large unlabeled sample, one is able to find classifiers with low incompatibility. Combining the two observations, we manage to prove a probably approximately correct (PAC) style learning bound for semi-supervised multi-view learning. We empirically verified our theory by designing two proof-of-concept multi-view learning algorithms, one based on active view sensing and the other based on online co-regularization, with real-world data sets.", "citation": "Citations (6)", "departments": ["University of Kansas", "University of Kansas"], "authors": ["Chao Lan.....http://dblp.org/pers/hd/l/Lan:Chao", "Jun Huan.....http://dblp.org/pers/hd/h/Huan:Jun"], "conf": "kdd", "year": "2015", "pages": 8}