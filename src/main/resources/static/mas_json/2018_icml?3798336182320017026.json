{"title": "Obfuscated Gradients Give a False Sense of Security: Circumventing Defenses to Adversarial Examples.", "fields": ["obfuscation", "machine learning", "phenomenon", "artificial intelligence", "adversarial system"], "abstract": "We identify obfuscated gradients as a phenomenon that leads to a false sense of security in defenses against adversarial examples. While defenses that cause obfuscated gradients appear to defeat optimization-based attacks, we find defenses relying on this effect can be circumvented. \nFor each of the three types of obfuscated gradients we discover, we describe indicators of defenses exhibiting this effect and develop attack techniques to overcome it. In a case study, examining all defenses accepted to ICLR 2018, we find obfuscated gradients are a common occurrence, with 7 of 8 defenses relying on obfuscated gradients. Using our new attack techniques, we successfully circumvent all 7 of them.", "citation": "Citations (64)", "departments": ["Massachusetts Institute of Technology", "University of California, Berkeley", "University of California, Berkeley"], "authors": ["Anish Athalye.....http://dblp.org/pers/hd/a/Athalye:Anish", "Nicholas Carlini.....http://dblp.org/pers/hd/c/Carlini:Nicholas", "David A. Wagner.....http://dblp.org/pers/hd/w/Wagner_0001:David_A="], "conf": "icml", "year": "2018", "pages": 10}