{"title": "Learning Recurrent Neural Networks with Hessian-Free Optimization.", "fields": ["machine learning", "pattern recognition", "recurrent neural network", "hessian matrix", "matrix"], "abstract": "In this work we resolve the long-outstanding problem of how to effectively train recurrent neural networks (RNNs) on complex and difficult sequence modeling problems which may contain long-term data dependencies. Utilizing recent advances in the Hessian-free optimization approach (Martens, 2010), together with a novel damping scheme, we successfully train RNNs on two sets of challenging problems. First, a collection of pathological synthetic datasets which are known to be impossible for standard optimization approaches (due to their extremely long-term dependencies), and second, on three natural and highly complex real-world sequence datasets where we find that our method significantly outperforms the previous state-of-the-art method for training neural sequence models: the Long Short-term Memory approach of Hochreiter and Schmidhuber (1997). Additionally, we offer a new interpretation of the generalized Gauss-Newton matrix of Schraudolph (2002) which is used within the HF approach of Martens.", "citation": "Citations (430)", "year": "2011", "departments": ["University of Toronto", "University of Toronto"], "conf": "icml", "authors": ["James Martens.....http://dblp.org/pers/hd/m/Martens:James", "Ilya Sutskever.....http://dblp.org/pers/hd/s/Sutskever:Ilya"], "pages": 8}