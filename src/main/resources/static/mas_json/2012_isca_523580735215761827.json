{"title": "Can traditional programming bridge the Ninja performance gap for parallel computing applications?", "fields": ["memory hierarchy", "speedup", "program optimization", "performance gap", "simd", "computer science", "multi core processor", "theoretical computer science"], "abstract": "Current processor trends of integrating more cores with wider SIMD units, along with a deeper and complex memory hierarchy, have made it increasingly more challenging to extract performance from applications. It is believed by some that traditional approaches to programming do not apply to these modern processors and hence radical new languages must be discovered. In this paper, we question this thinking and offer evidence in support of traditional programming methods and the performance-vs-programming effort effectiveness of common multi-core processors and upcoming many-core architectures in delivering significant speedup, and close-to-optimal performance for commonly used parallel computing workloads.   We first quantify the extent of the \" Ninja gap \", which is the performance gap between naively written C/C++ code that is parallelism unaware (often serial) and best-optimized code on modern multi-/many-core processors. Using a set of representative throughput computing benchmarks, we show that  there is an average   Ninja gap   of 24X (up to 53X ) for a recent 6-core Intel\u00ae Core\u2122 i7 X980 Westmere CPU, and that this gap if left unaddressed will inevitably increase. We show how a set of well-known algorithmic changes coupled with advancements in modern compiler technology can bring down the Ninja gap to an average of  just 1.3X . These changes typically require low programming effort, as compared to the very high effort in producing Ninja code. We also discuss hardware support for programmability that can reduce the impact of these changes and even further increase programmer productivity. We show equally encouraging results for the upcoming Intel\u00ae Many Integrated Core architecture (Intel\u00ae MIC) which has more cores and wider SIMD. We thus demonstrate that we can contain the otherwise uncontrolled growth of the Ninja gap and offer  a more stable and predictable performance growth  over future architectures, offering strong evidence that radical language changes are not required.", "citation": "Citations (95)", "year": "2012", "departments": ["Intel", "Intel", "Intel", "Intel", "Intel", "Intel", "Google", "eBay", "Intel", "Intel"], "conf": "isca", "authors": ["Nadathur Satish.....http://dblp.org/pers/hd/s/Satish:Nadathur", "Changkyu Kim.....http://dblp.org/pers/hd/k/Kim:Changkyu", "Jatin Chhugani.....http://dblp.org/pers/hd/c/Chhugani:Jatin", "Hideki Saito.....http://dblp.org/pers/hd/s/Saito:Hideki", "Rakesh Krishnaiyer.....http://dblp.org/pers/hd/k/Krishnaiyer:Rakesh", "Mikhail Smelyanskiy.....http://dblp.org/pers/hd/s/Smelyanskiy:Mikhail", "Milind Girkar.....http://dblp.org/pers/hd/g/Girkar:Milind", "Pradeep Dubey.....http://dblp.org/pers/hd/d/Dubey:Pradeep"], "pages": 12}