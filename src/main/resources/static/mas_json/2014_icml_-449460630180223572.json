{"title": "A Convergence Rate Analysis for LogitBoost, MART and Their Variant.", "fields": ["learnability", "rate of convergence", "gradient descent", "logitboost", "boosting"], "abstract": "LogitBoost, MART and their variant can be viewed as additive tree regression using logistic loss and boosting style optimization. We analyze their convergence rates based on a new weak learnability formulation. We show that it has O(1/T) rate when using gradient descent only, while a linear rate is achieved when using Newton descent. Moreover, introducing Newton descent when growing the trees, as LogitBoost does, leads to a faster linear rate. Empirical results on UCI datasets support our analysis.", "citation": "Citations (1)", "year": "2014", "departments": ["Tsinghua University", "Baidu", "Rutgers University", "Tsinghua University"], "conf": "icml", "authors": ["Peng Sun.....http://dblp.org/pers/hd/s/Sun:Peng", "Tong Zhang.....http://dblp.org/pers/hd/z/Zhang:Tong", "Jie Zhou.....http://dblp.org/pers/hd/z/Zhou:Jie"], "pages": 9}