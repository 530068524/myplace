{"title": "Non-Stationary Approximate Modified Policy Iteration.", "fields": ["power iteration", "interpolation", "parameterized complexity", "markov decision process", "integer"], "abstract": "We consider the infinite-horizon \u03b3-discounted optimal control problem formalized by Markov Decision Processes. Running any instance of Modified Policy Iteration--a family of algorithms that can interpolate between Value and Policy Iteration--with an error e at each iteration is known to lead to stationary policies that are at least 2\u03b3e/(1-\u03b3)2-optimal. Variations of Value and Policy Iteration, that build l-periodic nonstationary policies, have recently been shown to display a better 2\u03b3e/(1-\u03b3)(1-\u03b3l)-optimality guarantee. We describe a new algorithmic scheme, Non-Stationary Modified Policy Iteration, a family of algorithms parameterized by two integers m \u2265 0 and l \u2265 1 that generalizes all the above mentionned algorithms. While m allows one to interpolate between Value-Iteration-style and Policy-Iteration-style updates, l specifies the period of the non-stationary policy that is output. We show that this new family of algorithms also enjoys the improved 2\u03b3e/(1-\u03b3)(1-\u03b3l)-optimality guarantee. Perhaps more importantly, we show, by exhibiting an original problem instance, that this guarantee is tight for all m and l; this tightness was to our knowledge only known in two specific cases, Value Iteration (m = 0,l = 1) and Policy Iteration (m = \u221e,l = 1).", "citation": "Not cited", "year": "2015", "departments": ["French Institute for Research in Computer Science and Automation", "French Institute for Research in Computer Science and Automation"], "conf": "icml", "authors": ["Boris Lesner.....http://dblp.org/pers/hd/l/Lesner:Boris", "Bruno Scherrer.....http://dblp.org/pers/hd/s/Scherrer:Bruno"], "pages": 9}