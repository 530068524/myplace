{"title": "Generative Adversarial Imitation Learning.", "fields": ["robot learning", "cognitive imitation", "active learning", "learning classifier system", "temporal difference learning"], "abstract": "Consider learning a policy from example expert behavior, without interaction with the expert or access to a reinforcement signal. One approach is to recover the expert's cost function with inverse reinforcement learning, then extract a policy from that cost function with reinforcement learning. This approach is indirect and can be slow. We propose a new general framework for directly extracting a policy from data as if it were obtained by reinforcement learning following inverse reinforcement learning. We show that a certain instantiation of our framework draws an analogy between imitation learning and generative adversarial networks, from which we derive a model-free imitation learning algorithm that obtains significant performance gains over existing model-free methods in imitating complex behaviors in large, high-dimensional environments.", "citation": "Citations (177)", "departments": ["OpenAI", "Stanford University"], "authors": ["Jonathan Ho.....http://dblp.org/pers/hd/h/Ho:Jonathan", "Stefano Ermon.....http://dblp.org/pers/hd/e/Ermon:Stefano"], "conf": "nips", "year": "2016", "pages": 9}