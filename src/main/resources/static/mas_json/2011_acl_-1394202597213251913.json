{"title": "Integrating surprisal and uncertain-input models in online sentence comprehension: formal techniques and empirical results.", "fields": ["machine learning", "natural language processing", "syntax", "comprehension", "sentence"], "abstract": "A system making optimal use of available information in incremental language comprehension might be expected to use linguistic knowledge together with current input to revise beliefs about previous input. Under some circumstances, such an error-correction capability might induce comprehenders to adopt grammatical analyses that are inconsistent with the true input. Here we present a formal model of how such input-unfaithful garden paths may be adopted and the difficulty incurred by their subsequent disconfirmation, combining a rational noisy-channel model of syntactic comprehension under uncertain input with the surprisal theory of incremental processing difficulty. We also present a behavioral experiment confirming the key empirical predictions of the theory.", "citation": "Citations (76)", "year": "2011", "departments": ["University of California, San Diego"], "conf": "acl", "authors": ["Roger Levy.....http://dblp.org/pers/hd/l/Levy:Roger"], "pages": 11}