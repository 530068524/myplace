{"title": "CSE: Conceptual Sentence Embeddings based on Attention Model.", "fields": ["text corpus", "polysemy", "homonym", "topic model", "sentence"], "abstract": "Most sentence embedding models typically represent each sentence only using word surface, which makes these models indiscriminative for ubiquitous homonymy and polysemy. In order to enhance representation capability of sentence, we employ conceptualization model to assign associated concepts for each sentence in the text corpus, and then learn conceptual sentence embedding (CSE). Hence, this semantic representation is more expressive than some widely-used text representation models such as latent topic model, especially for short-text. Moreover, we further extend CSE models by utilizing a local attention-based model that select relevant words within the context to make more efficient prediction. In the experiments, we evaluate the CSE models on two tasks, text classification and information retrieval. The experimental results show that the proposed models outperform typical sentence embed-ding models.", "citation": "Citations (5)", "year": "2016", "departments": ["Beijing Institute of Technology", "Beijing Institute of Technology", "Beijing Institute of Technology", "Beijing Institute of Technology", "Beijing Institute of Technology"], "conf": "acl", "authors": ["Yashen Wang.....http://dblp.org/pers/hd/w/Wang:Yashen", "Heyan Huang.....http://dblp.org/pers/hd/h/Huang:Heyan", "Chong Feng.....http://dblp.org/pers/hd/f/Feng:Chong", "Qiang Zhou.....http://dblp.org/pers/hd/z/Zhou:Qiang", "Jiahui Gu.....http://dblp.org/pers/hd/g/Gu:Jiahui", "Xiong Gao.....http://dblp.org/pers/hd/g/Gao:Xiong"], "pages": -1}