{"title": "Communication avoiding successive band reduction.", "fields": ["shared memory", "multi core processor", "speedup", "sequential algorithm", "band matrix"], "abstract": "The running time of an algorithm depends on both arithmetic and communication (i.e., data movement) costs, and the relative costs of communication are growing over time. In this work, we present both theoretical and practical results for tridiagonalizing a symmetric band matrix: we present an algorithm that asymptotically reduces communication, and we show that it indeed performs well in practice.   The tridiagonalization of a symmetric band matrix is a key kernel in solving the symmetric eigenvalue problem for both full and band matrices. In order to preserve sparsity, tridiagonalization routines use annihilate-and-chase procedures that previously have suffered from poor data locality. We improve data locality by reorganizing the computation, asymptotically reducing communication costs compared to existing algorithms. Our sequential implementation demonstrates that avoiding communication improves runtime even at the expense of extra arithmetic: we observe a 2x speedup over Intel MKL while doing 43% more floating point operations.   Our parallel implementation targets shared-memory multicore platforms. It uses pipelined parallelism and a static scheduler while retaining the locality properties of the sequential algorithm. Due to lightweight synchronization and effective data reuse, we see 9.5x scaling over our serial code and up to 6x speedup over the PLASMA library, comparing parallel performance on a ten-core processor.", "citation": "Citations (6)", "year": "2012", "departments": ["University of California, Berkeley", "University of California, Berkeley", "University of California, Berkeley"], "conf": "ppopp", "authors": ["Grey Ballard.....http://dblp.org/pers/hd/b/Ballard:Grey", "James Demmel.....http://dblp.org/pers/hd/d/Demmel:James", "Nicholas Knight.....http://dblp.org/pers/hd/k/Knight:Nicholas"], "pages": 10}