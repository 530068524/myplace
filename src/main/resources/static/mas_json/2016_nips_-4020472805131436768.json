{"title": "Path-Normalized Optimization of Recurrent Neural Networks with ReLU Activations.", "fields": ["initialization", "artificial intelligence", "recurrent neural network", "normalization", "machine learning"], "abstract": "We investigate the parameter-space geometry of recurrent neural networks (RNNs), and develop an adaptation of path-SGD optimization method, attuned to this geometry, that can learn plain RNNs with ReLU activations. On several datasets that require capturing long-term dependency structure, we show that path-SGD can significantly improve trainability of ReLU RNNs compared to RNNs trained with SGD, even with various recently suggested initialization schemes.", "citation": "Citations (3)", "departments": ["Toyota Technological Institute at Chicago", "University of Toronto", "University of Toronto", "University of Chicago"], "authors": ["Behnam Neyshabur.....http://dblp.org/pers/hd/n/Neyshabur:Behnam", "Yuhuai Wu.....http://dblp.org/pers/hd/w/Wu:Yuhuai", "Ruslan Salakhutdinov.....http://dblp.org/pers/hd/s/Salakhutdinov:Ruslan", "Nati Srebro.....http://dblp.org/pers/hd/s/Srebro:Nati"], "conf": "nips", "year": "2016", "pages": 9}