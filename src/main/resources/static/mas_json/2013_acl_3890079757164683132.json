{"title": "Fast and Adaptive Online Training of Feature-Rich Translation Models.", "fields": ["overfitting", "discriminative model", "stochastic gradient descent", "adaptive learning", "machine translation"], "abstract": "We present a fast and scalable online method for tuning statistical machine translation models with large feature sets. The standard tuning algorithm\u2014MERT\u2014only scales to tens of features. Recent discriminative algorithms that accommodate sparse features have produced smaller than expected translation quality gains in large systems. Our method, which is based on stochastic gradient descent with an adaptive learning rate, scales to millions of features and tuning sets with tens of thousands of sentences, while still converging after only a few epochs. Large-scale experiments on Arabic-English and Chinese-English show that our method produces significant translation quality gains by exploiting sparse features. Equally important is our analysis, which suggests techniques for mitigating overfitting and domain mismatch, and applies to other recent discriminative methods for machine translation.", "citation": "Citations (24)", "year": "2013", "departments": ["Stanford University", "Stanford University", "Stanford University", "Stanford University"], "conf": "acl", "authors": ["Spence Green.....http://dblp.org/pers/hd/g/Green:Spence", "Sida I. Wang.....http://dblp.org/pers/hd/w/Wang:Sida_I=", "Daniel M. Cer.....http://dblp.org/pers/hd/c/Cer:Daniel_M=", "Christopher D. Manning.....http://dblp.org/pers/hd/m/Manning:Christopher_D="], "pages": 11}