{"title": "Natural Supervised Hashing.", "fields": ["feature hashing", "dynamic perfect hashing", "k independent hashing", "double hashing", "universal hashing"], "abstract": "Among learning-based hashing methods, supervised hashing tries to find hash codes which preserve semantic similarities of original data. Recent years have witnessed much efforts devoted to design objective functions and optimization methods for supervised hashing learning, in order to improve search accuracy and reduce training cost. In this paper, we propose a very straightforward supervised hashing algorithm and demonstrate its superiority over several state-of-the-art methods. The key idea of our approach is to treat label vectors as binary codes and to learn target codes which have similar structure to label vectors. To circumvent direct optimization on large n \u00d7 n Gram matrices, we identify an inner-product-preserving transformation and use it to bring close label vectors and hash codes without changing the structure. The optimization process is very efficient and scales well. In our experiment, training 16-bit and 96-bit code on NUS-WIDE cost respectively only 3 and 6 minutes.", "citation": "Citations (3)", "year": "2016", "departments": ["Shanghai Jiao Tong University", "Shanghai Jiao Tong University"], "conf": "ijcai", "authors": ["Qi Liu.....http://dblp.org/pers/hd/l/Liu:Qi", "Hongtao Lu.....http://dblp.org/pers/hd/l/Lu:Hongtao"], "pages": 7}