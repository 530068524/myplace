{"title": "Exploration in Model-based Reinforcement Learning by Empirically Estimating Learning Progress.", "fields": ["sanity", "mean squared prediction error", "certainty", "reinforcement learning", "bayesian probability"], "abstract": "Formal exploration approaches in model-based reinforcement learning estimate the accuracy of the currently learned model without consideration of the empirical prediction error. For example, PAC-MDP approaches such as R-MAX base their model certainty on the amount of collected data, while Bayesian approaches assume a prior over the transition dynamics. We propose extensions to such approaches which drive exploration solely based on empirical estimates of the learner's accuracy and learning progress. We provide a \"sanity check\" theoretical analysis, discussing the behavior of our extensions in the standard stationary finite state-action case. We then provide experimental studies demonstrating the robustness of these exploration measures in cases of non-stationary environments or where original approaches are misled by wrong domain assumptions.", "citation": "Citations (58)", "departments": ["French Institute for Research in Computer Science and Automation", "Free University of Berlin", "Free University of Berlin", "French Institute for Research in Computer Science and Automation"], "authors": ["Manuel Lopes.....http://dblp.org/pers/hd/l/Lopes_0001:Manuel", "Tobias Lang.....http://dblp.org/pers/hd/l/Lang_0001:Tobias", "Marc Toussaint.....http://dblp.org/pers/hd/t/Toussaint:Marc", "Pierre-Yves Oudeyer.....http://dblp.org/pers/hd/o/Oudeyer:Pierre=Yves"], "conf": "nips", "year": "2012", "pages": 9}