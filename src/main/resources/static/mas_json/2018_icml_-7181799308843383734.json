{"title": "Asynchronous Decentralized Parallel Stochastic Gradient Descent.", "fields": ["deep learning", "gradient descent", "stochastic gradient descent", "speedup", "asynchronous communication"], "abstract": "Recent work shows that decentralized parallel stochastic gradient decent (D-PSGD) can outperform its centralized counterpart both theoretically and practically. While asynchronous parallelism is a powerful technology to improve the efficiency of parallelism in distributed machine learning platforms and has been widely used in many popular machine learning softwares and solvers based on centralized parallel protocols such as Tensorflow, it still remains unclear how to apply the asynchronous parallelism to improve the efficiency of decentralized parallel algorithms. This paper proposes an asynchronous decentralize parallel stochastic gradient descent algorithm to apply the asynchronous parallelism technology to decentralized algorithms. Our theoretical analysis provides the convergence rate or equivalently the computational complexity, which is consistent with many special cases and indicates we can achieve nice linear speedup when we increase the number of nodes or the batchsize. Extensive experiments in deep learning validate the proposed algorithm.", "citation": "Citations (4)", "departments": ["University of Rochester", "IBM", "University of Rochester", "ETH Zurich"], "authors": ["Xiangru Lian.....http://dblp.org/pers/hd/l/Lian:Xiangru", "Wei Zhang.....http://dblp.org/pers/hd/z/Zhang:Wei", "Ce Zhang.....http://dblp.org/pers/hd/z/Zhang:Ce", "Ji Liu.....http://dblp.org/pers/hd/l/Liu:Ji"], "conf": "icml", "year": "2018", "pages": 10}