{"title": "Bayesian Hierarchical Reinforcement Learning.", "fields": ["prior probability", "recursion", "reinforcement learning", "hierarchy", "bayesian probability"], "abstract": "We describe an approach to incorporating Bayesian priors in the MAXQ framework for hierarchical reinforcement learning (HRL). We define priors on the primitive environment model and on task pseudo-rewards. Since models for composite tasks can be complex, we use a mixed model-based/model-free learning approach to find an optimal hierarchical policy. We show empirically that (i) our approach results in improved convergence over non-Bayesian baselines, (ii) using both task hierarchies and Bayesian priors is better than either alone, (iii) taking advantage of the task hierarchy reduces the computational cost of Bayesian reinforcement learning and (iv) in this framework, task pseudo-rewards can be learned instead of being manually specified, leading to hierarchically optimal rather than recursively optimal policies.", "citation": "Citations (17)", "departments": ["Case Western Reserve University", "Case Western Reserve University"], "authors": ["Feng Cao.....http://dblp.org/pers/hd/c/Cao:Feng", "Soumya Ray.....http://dblp.org/pers/hd/r/Ray:Soumya"], "conf": "nips", "year": "2012", "pages": 9}