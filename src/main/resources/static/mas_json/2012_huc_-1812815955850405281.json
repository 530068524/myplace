{"title": "Modeling ontology for multimodal interaction in ubiquitous computing systems.", "fields": ["multimodal interaction", "autonomous system", "ontology", "gesture", "modalities"], "abstract": "People communicate with each other using different ways, such as words, gestures, etc. to give information about their status, emotions and intentions. But how may this information be described in a way that autonomous systems (e.g. Robots) can react with a human being in a given environment?   A multimodal interface allows a more flexible and natural interaction between a user and a computing system. This paper presents a methodological approach for designing an architecture that facilitates the work of a fusion engine. The selection of modalities and the fusion of events invoked by the fusion engine are based upon the definition of an ontology that describes the environment where a multimodal interaction system exists.", "citation": "Not cited", "departments": ["Universit\u00e9 du Qu\u00e9bec", "University of V ...  V\u00e9lizy, France", "University of V ...  V\u00e9lizy, France"], "authors": ["Ahmad Wehbi.....http://dblp.org/pers/hd/w/Wehbi:Ahmad", "Amar Ramdane-Cherif.....http://dblp.org/pers/hd/r/Ramdane=Cherif:Amar", "Chakib Tadj.....http://dblp.org/pers/hd/t/Tadj:Chakib"], "conf": "huc", "year": "2012", "pages": 8}