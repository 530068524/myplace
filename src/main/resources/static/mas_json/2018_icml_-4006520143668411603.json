{"title": "Scalable Bilinear Learning Using State and Action Features.", "fields": ["scalability", "pi", "bilinear interpolation", "reinforcement learning", "markov decision process"], "abstract": "Approximate linear programming (ALP) represents one of the major algorithmic families to solve large-scale Markov decision processes (MDP). In this work, we study a primal-dual formulation of the ALP, and develop a scalable, model-free algorithm called bilinear $\\pi$ learning for reinforcement learning when a sampling oracle is provided. This algorithm enjoys a number of advantages. First, it adopts (bi)linear models to represent the high-dimensional value function and state-action distributions, using given state and action features. Its run-time complexity depends on the number of features, not the size of the underlying MDPs. Second, it operates in a fully online fashion without having to store any sample, thus having minimal memory footprint. Third, we prove that it is sample-efficient, solving for the optimal policy to high precision with a sample complexity linear in the dimension of the parameter space.", "citation": "Not cited", "departments": ["Princeton University", "Microsoft", "Princeton University"], "authors": ["Yichen Chen.....http://dblp.org/pers/hd/c/Chen:Yichen", "Lihong Li.....http://dblp.org/pers/hd/l/Li_0001:Lihong", "Mengdi Wang.....http://dblp.org/pers/hd/w/Wang:Mengdi"], "conf": "icml", "year": "2018", "pages": 10}