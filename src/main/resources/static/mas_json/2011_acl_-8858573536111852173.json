{"title": "Crowdsourcing Translation: Professional Quality from Non-Professionals.", "fields": ["perplexity", "crowdsourcing", "calibration", "nist", "total cost"], "abstract": "Naively collecting translations by crowd-sourcing the task to non-professional translators yields disfluent, low-quality results if no quality control is exercised. We demonstrate a variety of mechanisms that increase the translation quality to near professional levels. Specifically, we solicit redundant translations and edits to them, and automatically select the best output among them. We propose a set of features that model both the translations and the translators, such as country of residence, LM perplexity of the translation, edit rate from the other translations, and (optionally) calibration against professional translators. Using these features to score the collected translations, we are able to discriminate between acceptable and unacceptable translations. We recreate the NIST 2009 Urdu-to-English evaluation set with Mechanical Turk, and quantitatively show that our models are able to select translations within the range of quality that we expect from professional translators. The total cost is more than an order of magnitude lower than professional translation.", "citation": "Citations (339)", "year": "2011", "departments": ["Johns Hopkins University", "Johns Hopkins University"], "conf": "acl", "authors": ["Omar Zaidan.....http://dblp.org/pers/hd/z/Zaidan:Omar", "Chris Callison-Burch.....http://dblp.org/pers/hd/c/Callison=Burch:Chris"], "pages": 10}