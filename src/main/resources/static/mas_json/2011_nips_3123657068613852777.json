{"title": "Better Mini-Batch Algorithms via Accelerated Gradient Methods.", "fields": ["proximal gradient methods", "gradient descent", "stochastic gradient descent", "gradient method", "proximal gradient methods for learning"], "abstract": "Mini-batch algorithms have been proposed as a way to speed-up stochastic convex optimization problems. We study how such algorithms can be improved using accelerated gradient methods. We provide a novel analysis, which shows how standard gradient methods may sometimes be insufficient to obtain a significant speed-up and propose a novel accelerated gradient algorithm, which deals with this deficiency, enjoys a uniformly superior guarantee and works well in practice.", "citation": "Citations (158)", "departments": ["Toyota Technological Institute at Chicago", "Microsoft", "Toyota Technological Institute at Chicago", "Toyota Technological Institute at Chicago"], "authors": ["Andrew Cotter.....http://dblp.org/pers/hd/c/Cotter:Andrew", "Ohad Shamir.....http://dblp.org/pers/hd/s/Shamir:Ohad", "Nati Srebro.....http://dblp.org/pers/hd/s/Srebro:Nati", "Karthik Sridharan.....http://dblp.org/pers/hd/s/Sridharan:Karthik"], "conf": "nips", "year": "2011", "pages": 9}