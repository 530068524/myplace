{"title": "On optimization methods for deep learning.", "fields": ["stochastic gradient descent", "broyden fletcher goldfarb shanno algorithm", "mnist database", "convolutional neural network", "deep learning"], "abstract": "The predominant methodology in training deep learning advocates the use of stochastic gradient descent methods (SGDs). Despite its ease of implementation, SGDs are difficult to tune and parallelize. These problems make it challenging to develop, debug and scale up deep learning algorithms with SGDs. In this paper, we show that more sophisticated off-the-shelf optimization methods such as Limited memory BFGS (L-BFGS) and Conjugate gradient (CG) with line search can significantly simplify and speed up the process of pretraining deep algorithms. In our experiments, the difference between L-BFGS/CG and SGDs are more pronounced if we consider algorithmic extensions (e.g., sparsity regularization) and hardware extensions (e.g., GPUs or computer clusters). Our experiments with distributed optimization support the use of L-BFGS with locally connected networks and convolutional neural networks. Using L-BFGS, our convolutional network model achieves 0.69% on the standard MNIST dataset. This is a state-of-the-art result on MNIST among algorithms that do not use distortions or pretraining.", "citation": "Citations (548)", "year": "2011", "departments": ["Stanford University", "Stanford University", "Stanford University", "Stanford University", "Stanford University"], "conf": "icml", "authors": ["Quoc V. Le.....http://dblp.org/pers/hd/l/Le:Quoc_V=", "Jiquan Ngiam.....http://dblp.org/pers/hd/n/Ngiam:Jiquan", "Adam Coates.....http://dblp.org/pers/hd/c/Coates:Adam", "Ahbik Lahiri.....http://dblp.org/pers/hd/l/Lahiri:Ahbik", "Bobby Prochnow.....http://dblp.org/pers/hd/p/Prochnow:Bobby", "Andrew Y. Ng.....http://dblp.org/pers/hd/n/Ng:Andrew_Y="], "pages": 8}