{"title": "Mixed batches and symmetric discriminators for GAN training.", "fields": ["discriminator", "machine learning", "generative grammar", "symmetric function", "architecture"], "abstract": "Generative adversarial networks (GANs) are powerful generative models based on providing feedback to a generative network via a discriminator network. However, the discriminator usually assesses individual samples. This prevents the dis-criminator from accessing global distributional statistics of generated samples, and often leads to mode dropping: the generator models only part of the target distribution. We propose to feed the discriminator with mixed batches of true and fake samples, and train it to predict the ratio of true samples in the batch. The latter score does not depend on the order of samples in a batch. Rather than learning this invariance, we introduce a generic permutation-invariant discriminator architecture. This architecture is provably a universal approximator of all symmetric functions. Experimentally, our approach reduces mode collapse in GANs on two synthetic datasets, and obtains good results on the CIFAR10 and CelebA datasets, both qualitatively and quantitatively.", "citation": "Not cited", "departments": ["French Institute for Research in Computer Science and Automation", "\u00c9cole Polytechnique", "Facebook", "French Institute for Research in Computer Science and Automation"], "authors": ["Thomas Lucas.....http://dblp.org/pers/hd/l/Lucas:Thomas", "Corentin Tallec.....http://dblp.org/pers/hd/t/Tallec:Corentin", "Yann Ollivier.....http://dblp.org/pers/hd/o/Ollivier:Yann", "Jakob Verbeek.....http://dblp.org/pers/hd/v/Verbeek:Jakob"], "conf": "icml", "year": "2018", "pages": 10}