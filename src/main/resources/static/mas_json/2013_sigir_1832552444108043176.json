{"title": "Assessor disagreement and text classifier accuracy.", "fields": ["binary classification", "ranking", "data mining", "classifier", "information retrieval"], "abstract": "Text classifiers are frequently used for high-yield retrieval from large corpora, such as in e-discovery. The classifier is trained by annotating example documents for relevance. These examples may, however, be assessed by people other than those whose conception of relevance is authoritative. In this paper, we examine the impact that disagreement between actual and authoritative assessor has upon classifier effectiveness, when evaluated against the authoritative conception. We find that using alternative assessors leads to a significant decrease in binary classification quality, though less so ranking quality. A ranking consumer would have to go on average 25% deeper in the ranking produced by alternative-assessor training to achieve the same yield as for authoritative-assessor training.", "citation": "Citations (12)", "departments": ["University of Maryland, College Park", "Catalyst Reposi ... Denver, CO, USA"], "authors": ["William Webber.....http://dblp.org/pers/hd/w/Webber:William", "Jeremy Pickens.....http://dblp.org/pers/hd/p/Pickens:Jeremy"], "conf": "sigir", "year": "2013", "pages": 4}