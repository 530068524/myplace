{"title": "A Deeper Look at Planning as Learning from Replay.", "fields": ["equivalence", "training set", "function approximation", "ranging", "linear function"], "abstract": "In reinforcement learning, the notions of experience replay, and of planning as learning from replayed experience, have long been used to find good policies with minimal training data. Replay can be seen either as model-based reinforcement learning, where the store of past experiences serves as the model, or as a way to avoid a conventional model of the environment altogether. In this paper, we look more deeply at how replay blurs the line between model-based and model-free methods. First, we show for the first time an exact equivalence between the sequence of value functions found by a model-based policy-evaluation method and by a model-free method with replay. Second, we present a general replay method that can mimic a spectrum of methods ranging from the explicitly model-free (TD(0)) to the explicitly model-based (linear Dyna). Finally, we use insights gained from these relationships to design a new model-based reinforcement learning algorithm for linear function approximation. This method, which we call forgetful LSTD(\u03bb), improves upon regular LSTD(\u03bb) because it extends more naturally to online control, and improves upon linear Dyna because it is a multi-step method, enabling it to perform well even in non-Markov problems or, equivalently, in problems with significant function approximation.", "citation": "Citations (5)", "year": "2015", "departments": ["University of Alberta", "University of Alberta"], "conf": "icml", "authors": ["Harm Vanseijen.....http://dblp.org/pers/hd/v/Vanseijen:Harm", "Rich Sutton.....http://dblp.org/pers/hd/s/Sutton:Rich"], "pages": 9}