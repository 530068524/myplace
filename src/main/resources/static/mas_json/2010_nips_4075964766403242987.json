{"title": "Learning Efficient Markov Networks.", "fields": ["bounded function", "subspace topology", "probabilistic logic", "linear subspace", "inference"], "abstract": "We present an algorithm for learning high-treewidth Markov networks where inference is still tractable. This is made possible by exploiting context-specific independence and determinism in the domain. The class of models our algorithm can learn has the same desirable properties as thin junction trees: polynomial inference, closed-form weight learning, etc., but is much broader. Our algorithm searches for a feature that divides the state space into subspaces where the remaining variables decompose into independent subsets (conditioned on the feature and its negation) and recurses on each subspace/subset of variables until no useful new features can be found. We provide probabilistic performance guarantees for our algorithm under the assumption that the maximum feature length is bounded by a constant k (the treewidth can be much larger) and dependences are of bounded strength. We also propose a greedy version of the algorithm that, while forgoing these guarantees, is much more efficient. Experiments on a variety of domains show that our approach outperforms many state-of-the-art Markov network structure learners.", "citation": "Citations (25)", "departments": ["University of Washington", "University of Washington", "University of Washington"], "authors": ["Vibhav Gogate.....http://dblp.org/pers/hd/g/Gogate:Vibhav", "William Austin Webb.....http://dblp.org/pers/hd/w/Webb:William_Austin", "Pedro M. Domingos.....http://dblp.org/pers/hd/d/Domingos:Pedro_M="], "conf": "nips", "year": "2010", "pages": 9}