{"title": "Understanding Dropout.", "fields": ["error function", "formalism", "stochastic gradient descent", "normalization", "recursion"], "abstract": "Dropout is a relatively new algorithm for training neural networks which relies on stochastically \"dropping out\" neurons during training in order to avoid the co-adaptation of feature detectors. We introduce a general formalism for studying dropout on either units or connections, with arbitrary probability values, and use it to analyze the averaging and regularizing properties of dropout in both linear and non-linear networks. For deep neural networks, the averaging properties of dropout are characterized by three recursive equations, including the approximation of expectations by normalized weighted geometric means. We provide estimates and bounds for these approximations and corroborate the results with simulations. Among other results, we also show how dropout performs stochastic gradient descent on a regularized error function.", "citation": "Citations (132)", "departments": ["University of California, Irvine", "University of California, Irvine"], "authors": ["Pierre Baldi.....http://dblp.org/pers/hd/b/Baldi:Pierre", "Peter J. Sadowski.....http://dblp.org/pers/hd/s/Sadowski:Peter_J="], "conf": "nips", "year": "2013", "pages": 9}