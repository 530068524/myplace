{"title": "Fast sparse representation with prototypes.", "fields": ["small set", "sparse approximation", "convex optimization", "neural coding", "sparse matrix"], "abstract": "Sparse representation has found applications in numerous domains and recent developments have been focused on the convex relaxation of the lo-norm minimization for sparse coding (i.e., the l\\-norm minimization). Nevertheless, the time and space complexities of these algorithms remain significantly high for large-scale problems. As signals in most problems can be modeled by a small set of prototypes, we propose an algorithm that exploits this property and show that the l\\-norm minimization problem can be reduced to a much smaller problem, thereby gaining significant speed-ups with much less memory requirements. Experimental results demonstrate that our algorithm is able to achieve double-digit gain in speed with much less memory requirement than the state-of-the-art algorithms.", "citation": "Citations (28)", "year": "2010", "departments": ["University of California, Merced", "University of California, Merced"], "conf": "cvpr", "authors": ["Jia-Bin Huang.....http://dblp.org/pers/hd/h/Huang:Jia=Bin", "Ming-Hsuan Yang.....http://dblp.org/pers/hd/y/Yang_0001:Ming=Hsuan"], "pages": 8}