{"title": "Accelerated Gradient Methods for Stochastic Optimization and Online Learning.", "fields": ["proximal gradient methods", "random optimization", "hinge loss", "stochastic gradient descent", "proximal gradient methods for learning"], "abstract": "Regularized risk minimization often involves non-smooth optimization, either because of the loss function (e.g., hinge loss) or the regularizer (e.g., l1-regularizer). Gradient methods, though highly scalable and easy to implement, are known to converge slowly. In this paper, we develop a novel accelerated gradient method for stochastic optimization while still preserving their computational simplicity and scalability. The proposed algorithm, called SAGE (Stochastic Accelerated GradiEnt), exhibits fast convergence rates on stochastic composite optimization with convex or strongly convex objectives. Experimental results show that SAGE is faster than recent (sub)gradient methods including FOLOS, SMIDAS and SCD. Moreover, SAGE can also be extended for online learning, resulting in a simple algorithm but with the best regret bounds currently known for these problems.", "citation": "Citations (135)", "year": "2009", "departments": ["Hong Kong University of Science and Technology", "Hong Kong University of Science and Technology", "Hong Kong University of Science and Technology"], "conf": "nips", "authors": ["Chonghai Hu.....http://dblp.org/pers/hd/h/Hu:Chonghai", "James T. Kwok.....http://dblp.org/pers/hd/k/Kwok:James_T=", "Weike Pan.....http://dblp.org/pers/hd/p/Pan:Weike"], "pages": 9}