{"title": "Fixed-Point Factorized Networks.", "fields": ["matrix decomposition", "fixed point", "artificial neural network", "computational complexity theory", "theoretical computer science"], "abstract": "In recent years, Deep Neural Networks (DNN) based methods have achieved remarkable performance in a wide range of tasks and have been among the most powerful and widely used techniques in computer vision. However, DNN-based methods are both computational-intensive and resource-consuming, which hinders the application of these methods on embedded systems like smart phones. To alleviate this problem, we introduce a novel Fixed-point Factorized Networks (FFN) for pretrained models to reduce the computational complexity as well as the storage requirement of networks. The resulting networks have only weights of -1, 0 and 1, which significantly eliminates the most resource-consuming multiply-accumulate operations (MACs). Extensive experiments on large-scale ImageNet classification task show the proposed FFN only requires one-thousandth of multiply operations with comparable accuracy.", "citation": "Citations (3)", "year": "2017", "departments": ["Chinese Academy of Sciences", "Chinese Academy of Sciences"], "conf": "cvpr", "authors": ["Peisong Wang.....http://dblp.org/pers/hd/w/Wang:Peisong", "Jian Cheng.....http://dblp.org/pers/hd/c/Cheng_0001:Jian"], "pages": 9}