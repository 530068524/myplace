{"title": "Global Convergence of Stochastic Gradient Descent for Some Non-convex Matrix Problems.", "fields": ["power iteration", "subspace topology", "stochastic gradient descent", "speedup", "matrix completion"], "abstract": "Stochastic gradient descent (SGD) on a low-rank factorization (Burer & Monteiro, 2003) is commonly employed to speed up matrix problems including matrix completion, subspace tracking, and SDP relaxation. In this paper, we exhibit a step size scheme for SGD on a low-rank least-squares problem, and we prove that, under broad sampling conditions, our method converges globally from a random starting point within O(e-1nlog n) steps with constant probability for constant-rank problems. Our modification of SGD relates it to stochastic power iteration. We also show experiments to illustrate the runtime and convergence of the algorithm.", "citation": "Citations (81)", "year": "2015", "departments": ["Stanford University", "Stanford University", "Stanford University"], "conf": "icml", "authors": ["Christopher De Sa.....http://dblp.org/pers/hd/s/Sa:Christopher_De", "Christopher R\u00e9.....http://dblp.org/pers/hd/r/R=eacute=:Christopher", "Kunle Olukotun.....http://dblp.org/pers/hd/o/Olukotun:Kunle"], "pages": 10}