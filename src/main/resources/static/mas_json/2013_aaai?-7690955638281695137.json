{"title": "Multiagent Stochastic Planning With Bayesian Policy Recognition.", "fields": ["recursion", "bayesian inference", "posterior probability", "partially observable markov decision process", "observable"], "abstract": "A rational, autonomous decision maker operating in a partially observable, multiagent setting must accurately predict the actions of other entities. For this purpose, some kind of model of the other agents needs to be maintained and updated. One option is to consider intentional models, and simulate their decision making process. For instance, a POMDP-based agent might consider other decision makers to be POMDP-based themselves, and maintain a probability distribution over the collection of POMDP specifications it considers possible, as in the case of interactive POMDPs (Gmytrasiewicz and Doshi 2005). In general, this approach involves maintaining a probability distribution over all possible agents\u2019 specifications, including their own beliefs and structure. Given the complexity of such space, this is clearly an impractical task, even without considering the additional complication that the other agents might themselves model other entities, including our agent; this recursion gives rise to an infinite hierarchy of nested beliefs. An alternative is to consider subintentional models, which provide a predictive distribution over other agents\u2019 actions without explicitly simulating their decision-making algorithm. Within this class, stochastic finite state controllers (FSCs) provide a good tradeoff between expressivity and compactness. An FSC is a collection of nodes, each associated with a probability distribution over the agent\u2019s actions, and transition probabilities for each possible observation. Each node can be thought of as an abstract memory state of the modeled agent, a statistic that summarizes its past history. Learning accurate FSC models of other agents from their observed behavior yields an implicit marginalization over the set of possible structures, particularly their reward functions, when predicting their actions. It also has the benefit of flattening the belief hierarchy, since all the information the modeled agents use to take their decisions is implicitly contained in the inferred FSCs, regardless of their actual mental process. From a Bayesian learning standpoint, let m \u2208 M be an FSC, and D be the observed data about the behavior of an agent being modeled. We want to compute the posterior probability p(m|D) \u221d p(D|m)p(m). The complexity (i.e.", "citation": "Not cited", "departments": ["University of Illinois at Chicago"], "authors": ["Alessandro Panella.....http://dblp.org/pers/hd/p/Panella:Alessandro"], "conf": "aaai", "year": "2013", "pages": -1}