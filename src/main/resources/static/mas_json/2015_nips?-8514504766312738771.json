{"title": "Learning Continuous Control Policies by Stochastic Value Gradients.", "fields": ["mathematical optimization", "backpropagation", "bellman equation", "stochastic control", "machine learning"], "abstract": "We present a unified framework for learning continuous control policies using backpropagation. It supports stochastic control by treating stochasticity in the Bellman equation as a deterministic function of exogenous noise. The product is a spectrum of general policy gradient algorithms that range from model-free methods with value functions to model-based methods without value functions. We use learned models but only require observations from the environment instead of observations from model-predicted trajectories, minimizing the impact of compounded model errors. We apply these algorithms first to a toy stochastic control problem and then to several physics-based control problems in simulation. One of these variants, SVG(1), shows the effectiveness of learning models, value functions, and policies simultaneously in continuous domains.", "citation": "Citations (124)", "year": "2015", "departments": ["Google", "Google", "Google", "Google", "Google"], "conf": "nips", "authors": ["Nicolas Heess.....http://dblp.org/pers/hd/h/Heess:Nicolas", "Gregory Wayne.....http://dblp.org/pers/hd/w/Wayne:Gregory", "David Silver.....http://dblp.org/pers/hd/s/Silver:David", "Timothy P. Lillicrap.....http://dblp.org/pers/hd/l/Lillicrap:Timothy_P=", "Tom Erez.....http://dblp.org/pers/hd/e/Erez:Tom", "Yuval Tassa.....http://dblp.org/pers/hd/t/Tassa:Yuval"], "pages": 9}