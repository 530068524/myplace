{"title": "A majorization-minimization algorithm for (multiple) hyperparameter learning.", "fields": ["hyperparameter optimization", "semi supervised learning", "local optimum", "supervised learning", "hyperparameter"], "abstract": "We present a general Bayesian framework for hyperparameter tuning in  L  2 -regularized supervised learning models. Paradoxically, our algorithm works by first analytically integrating out the hyperparameters from the model. We find a local optimum of the resulting non-convex optimization problem efficiently using a majorization-minimization (MM) algorithm, in which the non-convex problem is reduced to a series of convex  L  2 -regularized parameter estimation tasks. The principal appeal of our method is its simplicity: the updates for choosing the  L  2 -regularized subproblems in each step are trivial to implement (or even perform by hand), and each subproblem can be efficiently solved by adapting existing solvers. Empirical results on a variety of supervised learning models show that our algorithm is competitive with both grid-search and gradient-based algorithms, but is more efficient and far easier to implement.", "citation": "Citations (18)", "departments": ["Stanford University", "Stanford University", "Institute for I ... arch, Singapore"], "authors": ["Chuan-Sheng Foo.....http://dblp.org/pers/hd/f/Foo:Chuan=Sheng", "Chuong B. Do.....http://dblp.org/pers/hd/d/Do:Chuong_B=", "Andrew Y. Ng.....http://dblp.org/pers/hd/n/Ng:Andrew_Y="], "conf": "icml", "year": "2009", "pages": 8}