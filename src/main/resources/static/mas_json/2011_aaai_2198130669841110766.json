{"title": "Scaling Up Reinforcement Learning through Targeted Exploration.", "fields": ["scaling", "state space", "small number", "reinforcement learning", "machine learning"], "abstract": "Recent Reinforcement Learning (RL) algorithms, such as R-MAX, make (with high probability) only a small number of poor decisions. In practice, these algorithms do not scale well as the number of states grows because the algorithms spend too much effort exploring. We introduce an RL algorithm State TArgeted R-MAX (STAR-MAX) that explores a subset of the state space, called the exploration envelope \u03be. When \u03be equals the total state space, STAR-MAX behaves identically to R-MAX. When \u03be is a subset of the state space, to keep exploration within \u03be, a recovery rule \u03b2 is needed. We compared existing algorithms with our algorithm employing various exploration envelopes. With an appropriate choice of \u03be, STAR-MAX scales far better than existing RL algorithms as the number of states increases. A possible drawback of our algorithm is its dependence on a good choice of \u03be and \u03b2. However, we show that an effective recovery rule \u03b2 can be learned on-line and \u03be can be learned from demonstrations. We also find that even randomly sampled exploration envelopes can improve cumulative rewards compared to R-MAX. We expect these results to lead to more efficient methods for RL in large-scale problems.", "citation": "Citations (3)", "departments": ["Texas A&M University", "Texas A&M University"], "authors": ["Timothy Arthur Mann.....http://dblp.org/pers/hd/m/Mann:Timothy_Arthur", "Yoonsuck Choe.....http://dblp.org/pers/hd/c/Choe:Yoonsuck"], "conf": "aaai", "year": "2011", "pages": -1}