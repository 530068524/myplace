{"title": "PAC Subset Selection in Stochastic Multi-armed Bandits.", "fields": ["gibbs sampling", "upper and lower bounds", "sample complexity", "regret", "sampling"], "abstract": "We consider the problem of selecting, from among the arms of a stochastic n-armed bandit, a subset of size m of those arms with the highest expected rewards, based on efficiently sampling the arms. This \"subset selection\" problem finds application in a variety of areas. In the authors' previous work (Kalyanakrishnan & Stone, 2010), this problem is framed under a PAC setting (denoted \"Explore-m\"), and corresponding sampling algorithms are analyzed. Whereas the formal analysis therein is restricted to the worst case sample complexity of algorithms, in this paper, we design and analyze an algorithm (\"LUCB\") with improved expected sample complexity. Interestingly LUCB bears a close resemblance to the well-known UCB algorithm for regret minimization. The expected sample complexity bound we show for LUCB is novel even for single-arm selection (Explore-1). We also give a lower bound on the worst case sample complexity of PAC algorithms for Explore-m.", "citation": "Citations (129)", "departments": ["Yahoo!", "University of Texas at Austin", "Information Technology University", "University of Texas at Austin"], "authors": ["Shivaram Kalyanakrishnan.....http://dblp.org/pers/hd/k/Kalyanakrishnan:Shivaram", "Ambuj Tewari.....http://dblp.org/pers/hd/t/Tewari:Ambuj", "Peter Auer.....http://dblp.org/pers/hd/a/Auer:Peter", "Peter Stone.....http://dblp.org/pers/hd/s/Stone:Peter"], "conf": "icml", "year": "2012", "pages": -1}