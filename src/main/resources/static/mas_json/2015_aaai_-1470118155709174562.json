{"title": "Expressing Arbitrary Reward Functions as Potential-Based Advice.", "fields": ["reward based selection", "mathematical optimization", "machine learning", "bellman equation", "reinforcement learning"], "abstract": "Effectively incorporating external advice is an important problem in reinforcement learning, especially as it moves into the real world. Potential-based reward shaping is a way to provide the agent with a specific form of additional reward, with the guarantee of policy invariance. In this work we give a novel way to incorporate an arbitrary reward function with the same guarantee, by implicitly translating it into the specific form of dynamic advice potentials, which are maintained as an auxiliary value function learnt at the same time. We show that advice provided in this way captures the input reward function in expectation, and demonstrate its efficacy empirically.", "citation": "Citations (10)", "departments": ["Vrije Universiteit Brussel", "University of York", "Vrije Universiteit Brussel", "Vrije Universiteit Brussel"], "authors": ["Anna Harutyunyan.....http://dblp.org/pers/hd/h/Harutyunyan:Anna", "Sam Devlin.....http://dblp.org/pers/hd/d/Devlin:Sam", "Peter Vrancx.....http://dblp.org/pers/hd/v/Vrancx:Peter", "Ann Now\u00e9.....http://dblp.org/pers/hd/n/Now=eacute=:Ann"], "conf": "aaai", "year": "2015", "pages": 7}