{"title": "Learning by Playing Solving Sparse Reward Tasks from Scratch.", "fields": ["scheduling", "scratch", "reinforcement learning", "artificial intelligence", "machine learning"], "abstract": "We propose Scheduled Auxiliary Control (SAC-X), a new learning paradigm in the context of Reinforcement Learning (RL). SAC-X enables learning of complex behaviors - from scratch - in the presence of multiple sparse reward signals. To this end, the agent is equipped with a set of general auxiliary tasks, that it attempts to learn simultaneously via off-policy RL. The key idea behind our method is that active (learned) scheduling and execution of auxiliary policies allows the agent to efficiently explore its environment - enabling it to excel at sparse reward RL. Our experiments in several challenging robotic manipulation settings demonstrate the power of our approach.", "citation": "Citations (3)", "departments": ["Google", "Google", "DeepMind", "DeepMind", "DeepMind"], "authors": ["Martin A. Riedmiller.....http://dblp.org/pers/hd/r/Riedmiller:Martin_A=", "Roland Hafner.....http://dblp.org/pers/hd/h/Hafner:Roland", "Thomas Lampe.....http://dblp.org/pers/hd/l/Lampe:Thomas", "Michael Neunert.....http://dblp.org/pers/hd/n/Neunert:Michael", "Jonas Degrave.....http://dblp.org/pers/hd/d/Degrave:Jonas", "Tom Van de Wiele.....http://dblp.org/pers/hd/w/Wiele:Tom_Van_de", "Vlad Mnih.....http://dblp.org/pers/hd/m/Mnih:Vlad", "Nicolas Heess.....http://dblp.org/pers/hd/h/Heess:Nicolas", "Jost Tobias Springenberg.....http://dblp.org/pers/hd/s/Springenberg:Jost_Tobias"], "conf": "icml", "year": "2018", "pages": 10}