{"title": "SCOPE: Scalable Composite Optimization for Learning on Spark.", "fields": ["rate of convergence", "spark", "distributed learning", "composite number", "scalability"], "abstract": "Many machine learning models, such as logistic regression~(LR) and support vector machine~(SVM), can be formulated as composite optimization problems. Recently, many distributed stochastic optimization~(DSO) methods have been proposed to solve the large-scale composite optimization problems, which have shown better performance than traditional batch methods. However, most of these DSO methods are not scalable enough. In this paper, we propose a novel DSO method, called \\underline{s}calable \\underline{c}omposite \\underline{op}timization for l\\underline{e}arning~({SCOPE}), and implement it on the fault-tolerant distributed platform \\mbox{Spark}. SCOPE is both computation-efficient and communication-efficient. Theoretical analysis shows that SCOPE is convergent with linear convergence rate when the objective function is convex. Furthermore, empirical results on real datasets show that SCOPE can outperform other state-of-the-art distributed learning methods on Spark, including both batch learning methods and DSO methods.", "citation": "Citations (1)", "departments": ["Nanjing University", "Nanjing University", "Nanjing University", "Nanjing University", "Nanjing University"], "authors": ["Shen-Yi Zhao.....http://dblp.org/pers/hd/z/Zhao:Shen=Yi", "Ru Xiang.....http://dblp.org/pers/hd/x/Xiang:Ru", "Ying-Hao Shi.....http://dblp.org/pers/hd/s/Shi:Ying=Hao", "Peng Gao.....http://dblp.org/pers/hd/g/Gao:Peng", "Wu-Jun Li.....http://dblp.org/pers/hd/l/Li:Wu=Jun"], "conf": "aaai", "year": "2017", "pages": 7}