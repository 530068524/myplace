{"title": "Co-Training for Domain Adaptation.", "fields": ["domain adaptation", "coda", "training set", "feature vector", "co training"], "abstract": "Domain adaptation algorithms seek to generalize a model trained in a source domain to a new target domain. In many practical cases, the source and target distributions can differ substantially, and in some cases crucial target features may not have support in the source domain. In this paper we introduce an algorithm that bridges the gap between source and target domains by slowly adding to the training set both the target features and instances in which the current algorithm is the most confident. Our algorithm is a variant of co-training [7], and we name it CODA (Co-training for domain adaptation). Unlike the original co-training work, we do not assume a particular feature split. Instead, for each iteration of co-training, we formulate a single optimization problem which simultaneously learns a target predictor, a split of the feature space into views, and a subset of source and target features to include in the predictor. CODA significantly out-performs the state-of-the-art on the 12-domain benchmark data set of Blitzer et al. [4]. Indeed, over a wide range (65 of 84 comparisons) of target supervision CODA achieves the best performance.", "citation": "Citations (215)", "departments": ["Washington University in St. Louis", "Washington University in St. Louis", "Google"], "authors": ["Minmin Chen.....http://dblp.org/pers/hd/c/Chen:Minmin", "Kilian Q. Weinberger.....http://dblp.org/pers/hd/w/Weinberger:Kilian_Q=", "John Blitzer.....http://dblp.org/pers/hd/b/Blitzer:John"], "conf": "nips", "year": "2011", "pages": 9}