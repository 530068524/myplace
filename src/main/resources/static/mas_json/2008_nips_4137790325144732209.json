{"title": "Goal-directed decision making in prefrontal cortex: a computational framework.", "fields": ["action selection", "orbitofrontal cortex", "decision problem", "probabilistic logic", "dorsolateral prefrontal cortex"], "abstract": "Research in animal learning and behavioral neuroscience has distinguished between two forms of action control: a habit-based form, which relies on stored action values, and a goal-directed form, which forecasts and compares action outcomes based on a model of the environment. While habit-based control has been the subject of extensive computational research, the computational principles underlying goal-directed control in animals have so far received less attention. In the present paper, we advance a computational framework for goal-directed control in animals and humans. We take three empirically motivated points as founding premises: (1) Neurons in dorsolateral prefrontal cortex represent action policies, (2) Neurons in orbitofrontal cortex represent rewards, and (3) Neural computation, across domains, can be appropriately understood as performing structured probabilistic inference. On a purely computational level, the resulting account relates closely to previous work using Bayesian inference to solve Markov decision problems, but extends this work by introducing a new algorithm, which provably converges on optimal plans. On a cognitive and neuroscientific level, the theory provides a unifying framework for several different forms of goal-directed action selection, placing emphasis on a novel form, within which orbitofrontal reward representations directly drive policy selection.", "citation": "Citations (53)", "year": "2008", "departments": ["Princeton University", "Princeton University"], "conf": "nips", "authors": ["Matthew Botvinick.....http://dblp.org/pers/hd/b/Botvinick:Matthew", "James An.....http://dblp.org/pers/hd/a/An:James"], "pages": 8}