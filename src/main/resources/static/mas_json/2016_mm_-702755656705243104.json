{"title": "Describing Videos using Multi-modal Fusion.", "fields": ["fusion", "natural language", "recurrent neural network", "video tracking", "encoder"], "abstract": "Describing videos with natural language is one of the ultimate goals of video understanding. Video records multi-modal information including image, motion, aural, speech and so on. MSR Video to Language Challenge provides a good chance to study multi-modality fusion in caption task. In this paper, we propose the multi-modal fusion encoder and integrate it with text sequence decoder into an end-to-end video caption framework. Features from visual, aural, speech and meta modalities are fused together to represent the video contents. Long Short-Term Memory Recurrent Neural Networks (LSTM-RNNs) are then used as the decoder to generate natural language sentences. Experimental results show the effectiveness of multi-modal fusion encoder trained in the end-to-end framework, which achieved top performance in both common metrics evaluation and human evaluation.", "citation": "Citations (18)", "departments": ["Renmin University of China", "Carnegie Mellon University", "Renmin University of China", "Renmin University of China", "Carnegie Mellon University"], "authors": ["Qin Jin.....http://dblp.org/pers/hd/j/Jin:Qin", "Jia Chen.....http://dblp.org/pers/hd/c/Chen:Jia", "Shizhe Chen.....http://dblp.org/pers/hd/c/Chen:Shizhe", "Yifan Xiong.....http://dblp.org/pers/hd/x/Xiong:Yifan", "Alexander G. Hauptmann.....http://dblp.org/pers/hd/h/Hauptmann:Alexander_G="], "conf": "mm", "year": "2016", "pages": 5}