{"title": "Accelerated Training for Matrix-norm Regularization: A Boosting Approach.", "fields": ["matrix norm", "sparse approximation", "interlacing", "gradient boosting", "boosting"], "abstract": "Sparse learning models typically combine a smooth loss with a nonsmooth penalty, such as trace norm. Although recent developments in sparse approximation have offered promising solution methods, current approaches either apply only to matrix-norm constrained problems or provide suboptimal convergence rates. In this paper, we propose a boosting method for regularized learning that guarantees e accuracy within O(1 /e) iterations. Performance is further accelerated by interlacing boosting with fixed-rank local optimization\u2014exploiting a simpler local objective than previous work. The proposed method yields state-of-the-art performance on large-scale problems. We also demonstrate an application to latent multiview learning for which we provide the first efficient weak-oracle.", "citation": "Citations (92)", "departments": ["University of Alberta", "University of Alberta", "University of Alberta"], "authors": ["Xinhua Zhang.....http://dblp.org/pers/hd/z/Zhang:Xinhua", "Yaoliang Yu.....http://dblp.org/pers/hd/y/Yu:Yaoliang", "Dale Schuurmans.....http://dblp.org/pers/hd/s/Schuurmans:Dale"], "conf": "nips", "year": "2012", "pages": 9}