{"title": "Value Pursuit Iteration.", "fields": ["power iteration", "reinforcement learning", "bellman equation", "markov decision process", "sparse approximation"], "abstract": "Value Pursuit Iteration (VPI) is an approximate value iteration algorithm that finds a close to optimal policy for reinforcement learning problems with large state spaces. VPI has two main features: First, it is a nonparametric algorithm that finds a good sparse approximation of the optimal value function given a dictionary of features. The algorithm is almost insensitive to the number of irrelevant features. Second, after each iteration of VPI, the algorithm adds a set of functions based on the currently learned value function to the dictionary. This increases the representation power of the dictionary in a way that is directly relevant to the goal of having a good approximation of the optimal value function. We theoretically study VPI and provide a finite-sample error upper bound for it.", "citation": "Citations (8)", "departments": ["McGill University", "McGill University"], "authors": ["Amir Massoud Farahmand.....http://dblp.org/pers/hd/f/Farahmand:Amir_Massoud", "Doina Precup.....http://dblp.org/pers/hd/p/Precup:Doina"], "conf": "nips", "year": "2012", "pages": 9}