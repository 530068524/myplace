{"title": "Optimal Binary Classifier Aggregation for General Losses.", "fields": ["sigmoid function", "binary classification", "minimax", "ensemble learning", "decision rule"], "abstract": "We address the problem of aggregating an ensemble of predictors with known loss bounds in a semi-supervised binary classification setting, to minimize prediction loss incurred on the unlabeled data. We find the minimax optimal predictions for a very general class of loss functions including all convex and many non-convex losses, extending a recent analysis of the problem for misclassification error. The result is a family of semi-supervised ensemble aggregation algorithms which are as efficient as linear learning by convex optimization, but are minimax optimal without any relaxations. Their decision rules take a form familiar in decision theory -- applying sigmoid functions to a notion of ensemble margin -- without the assumptions typically made in margin-based learning.", "citation": "Citations (3)", "year": "2016", "departments": ["University of California, San Diego", "University of California, San Diego"], "conf": "nips", "authors": ["Akshay Balsubramani.....http://dblp.org/pers/hd/b/Balsubramani:Akshay", "Yoav Freund.....http://dblp.org/pers/hd/f/Freund:Yoav"], "pages": 8}