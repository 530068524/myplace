{"title": "Multimodal Learning and Reasoning for Visual Question Answering.", "fields": ["convolutional neural network", "multimodal learning", "feature learning", "artificial general intelligence", "modular neural network"], "abstract": "Reasoning about entities and their relationships from multimodal data is a key goal of Artificial General Intelligence. The visual question answering (VQA) problem is an excellent way to test such reasoning capabilities of an AI model and its multimodal representation learning. However, the current VQA models are over-simplified deep neural networks, comprised of a long short-term memory (LSTM) unit for question comprehension and a convolutional neural network (CNN) for learning single image representation. We argue that the single visual representation contains a limited and general information about the image contents and thus limits the model reasoning capabilities. In this work we introduce a modular neural network model that learns a multimodal and multifaceted representation of the image and the question. The proposed model learns to use the multimodal representation to reason about the image entities and achieves a new state-of-the-art performance on both VQA benchmark datasets, VQA v1.0 and v2.0, by a wide margin.", "citation": "Not cited", "year": "2017", "departments": ["National University of Singapore", "National University of Singapore"], "conf": "nips", "authors": ["Ilija Ilievski.....http://dblp.org/pers/hd/i/Ilievski:Ilija", "Jiashi Feng.....http://dblp.org/pers/hd/f/Feng:Jiashi"], "pages": 12}