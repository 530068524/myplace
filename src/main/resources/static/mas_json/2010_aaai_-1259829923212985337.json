{"title": "Robust Policy Computation in Reward-Uncertain MDPs Using Nondominated Policies.", "fields": ["robust optimization", "exploit", "partially observable markov decision process", "speedup", "markov decision process"], "abstract": "The precise specification of reward functions for Markov decision processes (MDPs) is often extremely difficult, motivating research into both reward elicitation and the robust solution of MDPs with imprecisely specified reward (IRMDPs). We develop new techniques for the robust optimization of IR-MDPs, using the minimax regret decision criterion, that exploit the set of nondominated policies, i.e., policies that are optimal for some instantiation of the imprecise reward function. Drawing parallels to POMDP value functions, wedevise a Witness-style algorithm for identifying nondominated policies. We also examine several new algorithms for computing minimax regret using the nondominated set, and examine both practically and theoretically the impact of approximating this set. Our results suggest that a small subset of the nondominated set can greatly speed up computation, yet yield very tight approximations to minimax regret.", "citation": "Citations (26)", "departments": ["University of Toronto", "University of Toronto"], "authors": ["Kevin Regan.....http://dblp.org/pers/hd/r/Regan:Kevin", "Craig Boutilier.....http://dblp.org/pers/hd/b/Boutilier:Craig"], "conf": "aaai", "year": "2010", "pages": -1}