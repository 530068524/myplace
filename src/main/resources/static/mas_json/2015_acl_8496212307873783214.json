{"title": "Sparse Overcomplete Word Vector Representations.", "fields": ["machine learning", "lexical semantics", "natural language processing", "synonym", "binary number"], "abstract": "Current distributed representations of words show little resemblance to theories of lexical semantics. The former are dense and uninterpretable, the latter largely based on familiar, discrete classes (e.g., supersenses) and relations (e.g., synonymy and hypernymy). We propose methods that transform word vectors into sparse (and optionally binary) vectors. The resulting representations are more similar to the interpretable features typically used in NLP, though they are discovered automatically from raw corpora. Because the vectors are highly sparse, they are computationally easy to work with. Most importantly, we find that they outperform the original vectors on benchmark tasks.", "citation": "Citations (60)", "year": "2015", "departments": ["Carnegie Mellon University", "Carnegie Mellon University", "Carnegie Mellon University", "Carnegie Mellon University", "Carnegie Mellon University"], "conf": "acl", "authors": ["Manaal Faruqui.....http://dblp.org/pers/hd/f/Faruqui:Manaal", "Yulia Tsvetkov.....http://dblp.org/pers/hd/t/Tsvetkov:Yulia", "Dani Yogatama.....http://dblp.org/pers/hd/y/Yogatama:Dani", "Chris Dyer.....http://dblp.org/pers/hd/d/Dyer:Chris", "Noah A. Smith.....http://dblp.org/pers/hd/s/Smith:Noah_A="], "pages": 10}