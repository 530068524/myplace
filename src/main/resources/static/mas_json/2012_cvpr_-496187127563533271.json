{"title": "Discovering important people and objects for egocentric video summarization.", "fields": ["wearable computer", "object detection", "storyboard", "salience", "automatic summarization"], "abstract": "We present a video summarization approach for egocentric or \u201cwearable\u201d camera data. Given hours of video, the proposed method produces a compact storyboard summary of the camera wearer's day. In contrast to traditional keyframe selection techniques, the resulting summary focuses on the most important objects and people with which the camera wearer interacts. To accomplish this, we develop region cues indicative of high-level saliency in egocentric video \u2014 such as the nearness to hands, gaze, and frequency of occurrence \u2014 and learn a regressor to predict the relative importance of any new region based on these cues. Using these predictions and a simple form of temporal event detection, our method selects frames for the storyboard that reflect the key object-driven happenings. Critically, the approach is neither camera-wearer-specific nor object-specific; that means the learned importance metric need not be trained for a given user or context, and it can predict the importance of objects and people that have never been seen previously. Our results with 17 hours of egocentric data show the method's promise relative to existing techniques for saliency and summarization.", "citation": "Citations (402)", "year": "2012", "departments": ["University of Texas at Austin", "University of Texas at Austin", "University of Texas at Austin"], "conf": "cvpr", "authors": ["Yong Jae Lee.....http://dblp.org/pers/hd/l/Lee:Yong_Jae", "Joydeep Ghosh.....http://dblp.org/pers/hd/g/Ghosh:Joydeep", "Kristen Grauman.....http://dblp.org/pers/hd/g/Grauman:Kristen"], "pages": 8}