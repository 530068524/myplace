{"title": "BROOF: Exploiting Out-of-Bag Errors, Boosting and Random Forests for Effective Automated Classification.", "fields": ["additive model", "microarray analysis techniques", "random forest", "exploit", "brownboost"], "abstract": "Random Forests (RF) and Boosting are two of the most successful supervised learning paradigms for automatic classification. In this work we propose to combine both strategies in order to exploit their strengths while simultaneously solving some of their drawbacks, especially when applied to high-dimensional and noisy classification tasks. More specifically, we propose a boosted version of the RF classifier (BROOF), which fits an additive model composed by several random forests (as weak learners). Differently from traditional boosting methods which exploit the training error estimate, we here use the stronger out-of-bag (OOB) error estimate which is an out-of-the-box estimate naturally produced by the bagging method used in RFs. The influence of each weak learner in the fitted additive model is inversely proportional to their OOB error. Moreover, the probability of selecting an out-of-bag training example is increased if misclassified by the simpler weak learners, in order to enable the boosted model to focus on complex regions of the input space. We also adopt a selective weight updating procedure, whereas only the out-of-bag examples are updated as the boosting iterations go by. This serves the purpose of slowing down the tendency to focus on just a few hard-to-classify examples. By mitigating this undesired bias known to affect boosting algorithms under high dimensional and noisy scenarios - due to both the selective weighting schema and a proper weak-learner effectiveness assessment - we greatly improve classification effectiveness. Our experiments with several datasets in three representative high-dimensional and noisy domains - topic, sentiment and microarray data classification - an up to ten state-of-the-art classifiers (covering almost 500 results), show that BROOF is the only classifier to be among the top performers in all tested datasets from the topic classification domain, and in the vast majority of cases in sentiment and microarray domains, a surprising result given the knowledge that there is no single top-notch classifier for all datasets.", "citation": "Citations (8)", "departments": ["Universidade Federal de Minas Gerais", "Universidade Federal de Minas Gerais", "Universidade Federal de Minas Gerais", "Federal Univers ... Del-Rei, Brazil"], "authors": ["Thiago Salles.....http://dblp.org/pers/hd/s/Salles:Thiago", "Marcos Andr\u00e9 Gon\u00e7alves.....http://dblp.org/pers/hd/g/Gon=ccedil=alves:Marcos_Andr=eacute=", "Victor Rodrigues.....http://dblp.org/pers/hd/r/Rodrigues:Victor", "Leonardo C. da Rocha.....http://dblp.org/pers/hd/r/Rocha:Leonardo_C=_da"], "conf": "sigir", "year": "2015", "pages": 10}