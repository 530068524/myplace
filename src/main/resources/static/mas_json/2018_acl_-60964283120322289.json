{"title": "Universal Language Model Fine-tuning for Text Classification.", "fields": ["inductive transfer", "fine tuning", "universal language", "transfer of learning", "scratch"], "abstract": "Inductive transfer learning has greatly impacted computer vision, but existing approaches in NLP still require task-specific modifications and training from scratch. We propose Universal Language Model Fine-tuning (ULMFiT), an effective transfer learning method that can be applied to any task in NLP, and introduce techniques that are key for fine-tuning a language model. Our method significantly outperforms the state-of-the-art on six text classification tasks, reducing the error by 18-24% on the majority of datasets. Furthermore, with only 100 labeled examples, it matches the performance of training from scratch on 100x more data. We open-source our pretrained models and code.", "citation": "Not cited", "departments": [], "authors": ["Sebastian Ruder.....http://dblp.org/pers/hd/r/Ruder:Sebastian", "Jeremy Howard.....http://dblp.org/pers/hd/h/Howard:Jeremy"], "conf": "acl", "year": "2018", "pages": 12}