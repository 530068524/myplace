{"title": "Code-specific policy gradient rules for spiking neurons.", "fields": ["natural exponential family", "machine learning", "neural coding", "learning rule", "reinforcement learning"], "abstract": "Although it is widely believed that reinforcement learning is a suitable tool for describing behavioral learning, the mechanisms by which it can be implemented in networks of spiking neurons are not fully understood. Here, we show that different learning rules emerge from a policy gradient approach depending on which features of the spike trains are assumed to influence the reward signals, i.e., depending on which neural code is in effect. We use the framework of Williams (1992) to derive learning rules for arbitrary neural codes. For illustration, we present policy-gradient rules for three different example codes - a spike count code, a spike timing code and the most general \"full spike train\" code - and test them on simple model problems. In addition to classical synaptic learning, we derive learning rules for intrinsic parameters that control the excitability of the neuron. The spike count learning rule has structural similarities with established Bienenstock-Cooper-Munro rules. If the distribution of the relevant spike train features belongs to the natural exponential family, the learning rules have a characteristic shape that raises interesting prediction problems.", "citation": "Citations (6)", "year": "2009", "departments": ["\u00c9cole Polytechnique F\u00e9d\u00e9rale de Lausanne", "\u00c9cole Polytechnique F\u00e9d\u00e9rale de Lausanne", "\u00c9cole Polytechnique F\u00e9d\u00e9rale de Lausanne"], "conf": "nips", "authors": ["Henning Sprekeler.....http://dblp.org/pers/hd/s/Sprekeler:Henning", "Guillaume Hennequin.....http://dblp.org/pers/hd/h/Hennequin:Guillaume", "Wulfram Gerstner.....http://dblp.org/pers/hd/g/Gerstner:Wulfram"], "pages": 9}