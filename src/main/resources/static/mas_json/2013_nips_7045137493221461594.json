{"title": "Online learning in episodic Markovian decision processes by relative entropy policy search.", "fields": ["kullback leibler divergence", "state space", "search algorithm", "regret", "markov decision process"], "abstract": "We study the problem of online learning in finite episodic Markov decision processes (MDPs) where the loss function is allowed to change between episodes. The natural performance measure in this learning problem is the regret defined as the difference between the total loss of the best stationary policy and the total loss suffered by the learner. We assume that the learner is given access to a finite action space A and the state space X has a layered structure with L layers, so that state transitions are only possible between consecutive layers. We describe a variant of the recently proposed Relative Entropy Policy Search algorithm and show that its regret after T episodes is 2\u221aL|X||A|T log (|X||A|/L) in the bandit setting and 2L \u221aT log(|X||A|/L) in the full information setting, given that the learner has perfect knowledge of the transition probabilities of the underlying MDP. These guarantees largely improve previously known results under much milder assumptions and cannot be significantly improved under general assumptions.", "citation": "Citations (12)", "departments": ["Institute of Science and Technology Austria", "French Institute for Research in Computer Science and Automation"], "authors": ["Alexander Zimin.....http://dblp.org/pers/hd/z/Zimin:Alexander", "Gergely Neu.....http://dblp.org/pers/hd/n/Neu:Gergely"], "conf": "nips", "year": "2013", "pages": 9}