{"title": "Stochastic Optimization with Importance Sampling for Regularized Loss Minimization.", "fields": ["stochastic approximation", "continuous time stochastic process", "estimator", "importance sampling", "stochastic universal sampling"], "abstract": "Uniform sampling of training data has been commonly used in traditional stochastic optimization algorithms such as Proximal Stochastic Mirror Descent (prox-SMD) and Proximal Stochastic Dual Coordinate Ascent (prox-SDCA). Although uniform sampling can guarantee that the sampled stochastic quantity is an unbiased estimate of the corresponding true quantity, the resulting estimator may have a rather high variance, which negatively affects the convergence of the underlying optimization procedure. In this paper we study stochastic optimization, including prox-SMD and prox-SDCA, with importance sampling, which improves the convergence rate by reducing the stochastic variance. We theoretically analyze the algorithms and empirically validate their effectiveness.", "citation": "Citations (107)", "year": "2015", "departments": ["Baidu", "Rutgers University", "Agency for Science, Technology and Research", "Baidu", "Rutgers University"], "conf": "icml", "authors": ["Peilin Zhao.....http://dblp.org/pers/hd/z/Zhao:Peilin", "Tong Zhang.....http://dblp.org/pers/hd/z/Zhang:Tong"], "pages": 9}