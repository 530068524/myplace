{"title": "Multimodal Deep Learning.", "fields": ["modalities", "deep learning", "multimodal learning", "feature learning", "multi task learning"], "abstract": "Deep networks have been successfully applied to unsupervised feature learning for single modalities (e.g., text, images or audio). In this work, we propose a novel application of deep networks to learn features over multiple modalities. We present a series of tasks for multimodal learning and show how to train deep networks that learn features to address these tasks. In particular, we demonstrate cross modality feature learning, where better features for one modality (e.g., video) can be learned if multiple modalities (e.g., audio and video) are present at feature learning time. Furthermore, we show how to learn a shared representation between modalities and evaluate it on a unique task, where the classifier is trained with audio-only data but tested with video-only data and vice-versa. Our models are validated on the CUAVE and AVLetters datasets on audio-visual speech classification, demonstrating best published visual speech classification on AVLetters and effective shared representation learning.", "citation": "Citations (1,319)", "year": "2011", "departments": ["Stanford University", "Stanford University", "Stanford University", "Stanford University", "University of Michigan"], "conf": "icml", "authors": ["Jiquan Ngiam.....http://dblp.org/pers/hd/n/Ngiam:Jiquan", "Aditya Khosla.....http://dblp.org/pers/hd/k/Khosla:Aditya", "Mingyu Kim.....http://dblp.org/pers/hd/k/Kim:Mingyu", "Juhan Nam.....http://dblp.org/pers/hd/n/Nam:Juhan", "Honglak Lee.....http://dblp.org/pers/hd/l/Lee:Honglak", "Andrew Y. Ng.....http://dblp.org/pers/hd/n/Ng:Andrew_Y="], "pages": 8}