{"title": "Increasingly Cautious Optimism for Practical PAC-MDP Exploration.", "fields": ["upper and lower bounds", "sample complexity", "reinforcement learning", "optimism", "machine learning"], "abstract": "Exploration strategy is an essential part of learning agents in model-based Reinforcement Learning. R-MAX and V-MAX are PAC-MDP strategies proved to have polynomial sample complexity; yet, their exploration behavior tend to be overly cautious in practice. We propose the principle of Increasingly Cautious Optimism (ICO) to automatically cut off unnecessarily cautious exploration, and apply ICO to R-MAX and V-MAX, yielding two new strategies, namely Increasingly Cautious R-MAX (ICR) and Increasingly Cautious V-MAX (ICV). We prove that both ICR and ICV are PACMDP, and show that their improvement is guaranteed by a tighter sample complexity upper bound. Then, we demonstrate their significantly improved performance through empirical results.", "citation": "Citations (1)", "departments": ["University of Science and Technology of China", "University of Science and Technology of China", "University of Birmingham"], "authors": ["Liangpeng Zhang.....http://dblp.org/pers/hd/z/Zhang:Liangpeng", "Ke Tang.....http://dblp.org/pers/hd/t/Tang:Ke", "Xin Yao.....http://dblp.org/pers/hd/y/Yao_0001:Xin"], "conf": "ijcai", "year": "2015", "pages": 8}