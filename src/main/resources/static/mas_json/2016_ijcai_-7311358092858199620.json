{"title": "Cost-Aware Pre-Training for Multiclass Cost-Sensitive Deep Learning.", "fields": ["deep learning", "semi supervised learning", "feature extraction", "artificial intelligence", "machine learning"], "abstract": "Deep learning has been one of the most prominent machine learning techniques nowadays, being the state-of-the-art on a broad range of applications where automatic feature extraction is needed. Many such applications also demand varying costs for different types of mis-classification errors, but it is not clear whether or how such cost information can be incorporated into deep learning to improve performance. In this work, we first design a novel loss function that embeds the cost information for the training stage of cost-sensitive deep learning. We then show that the loss function can also be integrated into the pre-training stage to conduct cost-aware feature extraction more effectively. Extensive experimental results justify the validity of the novel loss function for making existing deep learning models cost-sensitive, and demonstrate that our proposed model with cost-aware pre-training and training outperforms non-deep models and other deep models that digest the cost information in other stages.", "citation": "Citations (5)", "year": "2016", "departments": ["National Taiwan University", "National Taiwan University", "Intel"], "conf": "ijcai", "authors": ["Yu-An Chung.....http://dblp.org/pers/hd/c/Chung:Yu=An", "Hsuan-Tien Lin.....http://dblp.org/pers/hd/l/Lin:Hsuan=Tien", "Shao-Wen Yang.....http://dblp.org/pers/hd/y/Yang:Shao=Wen"], "pages": 7}