{"title": "Long Short-Term Memory Over Recursive Structures.", "fields": ["memory cell", "long short term memory", "machine translation", "recursion", "natural language understanding"], "abstract": "The chain-structured long short-term memory (LSTM) has showed to be effective in a wide range of problems such as speech recognition and machine translation. In this paper, we propose to extend it to tree structures, in which a memory cell can reflect the history memories of multiple child cells or multiple descendant cells in a recursive process. We call the model S-LSTM, which provides a principled way of considering long-distance interaction over hierarchies, e.g., language or image parse structures. We leverage the models for semantic composition to understand the meaning of text, a fundamental problem in natural language understanding, and show that it outperforms a state-of-the-art recursive model by replacing its composition layers with the S-LSTM memory blocks. We also show that utilizing the given structures is helpful in achieving a performance better than that without considering the structures.", "citation": "Citations (133)", "year": "2015", "departments": ["National Research Council", "University of Ottawa", "National Research Council"], "conf": "icml", "authors": ["Xiao-Dan Zhu.....http://dblp.org/pers/hd/z/Zhu:Xiao=Dan", "Parinaz Sobhani.....http://dblp.org/pers/hd/s/Sobhani:Parinaz", "Hongyu Guo.....http://dblp.org/pers/hd/g/Guo:Hongyu"], "pages": 9}