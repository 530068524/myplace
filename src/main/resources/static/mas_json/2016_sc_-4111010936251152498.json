{"title": "dCUDA: hardware supported overlap of computation and communication.", "fields": ["thread", "distributed memory", "science technology and society", "instruction set", "imagination", "hardware architecture", "cuda", "computation", "search engine", "data science"], "abstract": "Over the last decade, CUDA and the underlying GPU hardware architecture have continuously gained popularity in various high-performance computing application domains such as climate modeling, computational chemistry, or machine learning. Despite this popularity, we lack a single coherent programming model for GPU clusters. We therefore introduce the dCUDA programming model, which implements device-side remote memory access with target notification. To hide instruction pipeline latencies, CUDA programs over-decompose the problem and over-subscribe the device by running many more threads than there are hardware execution units. Whenever a thread stalls, the hardware scheduler immediately proceeds with the execution of another thread ready for execution. This latency hiding technique is key to make best use of the available hardware resources. With dCUDA, we apply latency hiding at cluster scale to automatically overlap computation and communication. Our benchmarks demonstrate perfect overlap for memory bandwidth-bound tasks and good overlap for compute-bound tasks.", "citation": "Not cited", "departments": ["ETH Zurich", "ETH Zurich", "ETH Zurich"], "authors": ["Tobias Gysi.....http://dblp.org/pers/hd/g/Gysi:Tobias", "Jeremia B\u00e4r.....http://dblp.org/pers/hd/b/B=auml=r:Jeremia", "Torsten Hoefler.....http://dblp.org/pers/hd/h/Hoefler:Torsten"], "conf": "sc", "year": "2016", "pages": 12}