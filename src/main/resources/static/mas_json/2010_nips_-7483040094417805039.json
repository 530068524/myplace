{"title": "Error Propagation for Approximate Policy and Value Iteration.", "fields": ["infimum and supremum", "square", "markov decision process", "lp space", "residual"], "abstract": "We address the question of how the approximation error/Bellman residual at each iteration of the Approximate Policy/Value Iteration algorithms influences the quality of the resulted policy. We quantify the performance loss as the Lp norm of the approximation error/Bellman residual at each iteration. Moreover, we show that the performance loss depends on the expectation of the squared Radon-Nikodym derivative of a certain distribution rather than its supremum - as opposed to what has been suggested by the previous results. Also our results indicate that the contribution of the approximation/Bellman error to the performance loss is more prominent in the later iterations of API/AVI, and the effect of an error term in the earlier iterations decays exponentially fast.", "citation": "Citations (80)", "departments": ["University of Alberta", "University of Alberta", "French Institute for Research in Computer Science and Automation"], "authors": ["Amir Massoud Farahmand.....http://dblp.org/pers/hd/f/Farahmand:Amir_Massoud", "R\u00e9mi Munos.....http://dblp.org/pers/hd/m/Munos:R=eacute=mi", "Csaba Szepesv\u00e1ri.....http://dblp.org/pers/hd/s/Szepesv=aacute=ri:Csaba"], "conf": "nips", "year": "2010", "pages": 9}