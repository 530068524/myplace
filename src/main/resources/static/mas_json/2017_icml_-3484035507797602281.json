{"title": "Forward and Reverse Gradient-Based Hyperparameter Optimization.", "fields": ["iterative learning control", "hyperparameter optimization", "stochastic gradient descent", "recurrent neural network", "deep learning"], "abstract": "We study two procedures (reverse-mode and\nforward-mode) for computing the gradient of the\nvalidation error with respect to the hyperparameters\nof any iterative learning algorithm such as\nstochastic gradient descent. These procedures\nmirror two methods of computing gradients for\nrecurrent neural networks and have different\ntrade-offs in terms of running time and space requirements.\nOur formulation of the reverse-mode\nprocedure is linked to previous work by Maclaurin\net al. (2015) but does not require reversible\ndynamics. The forward-mode procedure is suitable\nfor real-time hyperparameter updates, which\nmay significantly speed up hyperparameter optimization\non large datasets. We present experiments\non data cleaning and on learning task interactions.\nWe also present one large-scale experiment\nwhere the use of previous gradient-based\nmethods would be prohibitive.", "citation": "Citations (3)", "year": "2017", "departments": ["Istituto Italiano di Tecnologia", "University of Padua", "University of Florence", "University College London"], "conf": "icml", "authors": ["Luca Franceschi.....http://dblp.org/pers/hd/f/Franceschi:Luca", "Michele Donini.....http://dblp.org/pers/hd/d/Donini:Michele", "Paolo Frasconi.....http://dblp.org/pers/hd/f/Frasconi:Paolo", "Massimiliano Pontil.....http://dblp.org/pers/hd/p/Pontil:Massimiliano"], "pages": 9}