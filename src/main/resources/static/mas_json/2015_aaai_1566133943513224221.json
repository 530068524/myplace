{"title": "Reward Shaping for Model-Based Bayesian Reinforcement Learning.", "fields": ["heuristic", "machine learning", "reinforcement learning", "artificial intelligence", "bayesian probability"], "abstract": "Bayesian reinforcement learning (BRL) provides a formal framework for optimal exploration-exploitation tradeoff in reinforcement learning. Unfortunately, it is generally intractable to find the Bayes-optimal behavior except for restricted cases. As a consequence, many BRL algorithms, model-based approaches in particular, rely on approximated models or real-time search methods. In this paper, we present potential-based shaping for improving the learning performance in model-based BRL. We propose a number of potential functions that are particularly well suited for BRL, and are domain-independent in the sense that they do not require any prior knowledge about the actual environment. By incorporating the potential function into real-time heuristic search, we show that we can significantly improve the learning performance in standard benchmark domains.", "citation": "Citations (2)", "departments": ["KAIST", "KAIST", "KAIST", "KAIST", "KAIST"], "authors": ["Hyeoneun Kim.....http://dblp.org/pers/hd/k/Kim:Hyeoneun", "Woosang Lim.....http://dblp.org/pers/hd/l/Lim:Woosang", "Kanghoon Lee.....http://dblp.org/pers/hd/l/Lee:Kanghoon", "Yung-Kyun Noh.....http://dblp.org/pers/hd/n/Noh:Yung=Kyun", "Kee-Eung Kim.....http://dblp.org/pers/hd/k/Kim:Kee=Eung"], "conf": "aaai", "year": "2015", "pages": 8}