{"title": "Probit Classifiers with a Generalized Gaussian Scale Mixture Prior.", "fields": ["kernel", "feature selection", "probit", "gaussian", "maximum a posteriori estimation"], "abstract": "Most of the existing probit classifiers are based on sparsity-oriented modeling. However, we show that sparsity is not always desirable in practice, and only an appropriate degree of sparsity is profitable. In this work, we propose a flexible probabilistic model using a generalized Gaussian scale mixture prior that can promote an appropriate degree of sparsity for its model parameters, and yield either sparse or non-sparse estimates according to the intrinsic sparsity of features in a dataset. Model learning is carried out by an efficient modified maximum a posteriori (MAP) estimate. We also show relationships of the proposed model to existing probit classifiers as well as iteratively re-weighted l1 and l2 minimizations. Experiments demonstrate that the proposed method has better or comparable performances in feature selection for linear classifiers as well as in kernel-based classification.", "citation": "Citations (3)", "year": "2011", "departments": ["Nanyang Technological University", "Nanyang Technological University", "Teesside University"], "conf": "ijcai", "authors": ["Guoqing Liu.....http://dblp.org/pers/hd/l/Liu:Guoqing", "Jianxin Wu.....http://dblp.org/pers/hd/w/Wu:Jianxin", "Suiping Zhou.....http://dblp.org/pers/hd/z/Zhou:Suiping"], "pages": 6}