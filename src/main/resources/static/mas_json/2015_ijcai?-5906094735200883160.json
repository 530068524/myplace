{"title": "Adaptive Dropout Rates for Learning with Corrupted Features.", "fields": ["overfitting", "bayesian probability", "pattern recognition", "feature", "machine learning"], "abstract": "Feature noising is an effective mechanism on reducing the risk of overfitting. To avoid an explosive searching space, existing work typically assumes that all features share a single noise level, which is often cross-validated. In this paper, we present a Bayesian feature noising model that flexibly allows for dimension-specific or group-specific noise levels, and we derive a learning algorithm that adaptively updates these noise levels. Our adaptive rule is simple and interpretable, by drawing a direct connection to the fitness of each individual feature or feature group. Empirical results on various datasets demonstrate the effectiveness on avoiding extensive tuning and sometimes improving the performance due to its flexibility.", "citation": "Citations (6)", "departments": ["Tsinghua University", "Tsinghua University", "Tsinghua University"], "authors": ["Jingwei Zhuo.....http://dblp.org/pers/hd/z/Zhuo:Jingwei", "Jun Zhu.....http://dblp.org/pers/hd/z/Zhu_0001:Jun", "Bo Zhang.....http://dblp.org/pers/hd/z/Zhang_0010:Bo"], "conf": "ijcai", "year": "2015", "pages": 8}