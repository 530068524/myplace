{"title": "Stick-Breaking Policy Learning in Dec-POMDPs.", "fields": ["machine learning", "bayesian probability", "controller", "expectation maximization algorithm", "policy learning"], "abstract": "Expectation maximization (EM) has recently been shown to be an efficient algorithm for learning finite-state controllers (FSCs) in large decentralized POMDPs (Dec-POMDPs). However, current methods use fixed-size FSCs and often converge to maxima that are far from the optimal value. This paper represents the local policy of each agent using variable-sized FSCs that are constructed using a stick-breaking prior, leading to a new framework called decentralized stick-breaking policy representation (Dec-SBPR). This approach learns the controller parameters with a variational Bayesian algorithm without having to assume that the Dec-POMDP model is available. The performance of Dec-SBPR is demonstrated on several benchmark problems, showing that the algorithm scales to large problems while outperforming other state-of-the-art methods.", "citation": "Citations (8)", "departments": ["Massachusetts Institute of Technology", "University of New Hampshire", "Duke University", "Duke University", "Massachusetts Institute of Technology"], "authors": ["Miao Liu.....http://dblp.org/pers/hd/l/Liu:Miao", "Christopher Amato.....http://dblp.org/pers/hd/a/Amato:Christopher", "Xuejun Liao.....http://dblp.org/pers/hd/l/Liao:Xuejun", "Lawrence Carin.....http://dblp.org/pers/hd/c/Carin:Lawrence", "Jonathan P. How.....http://dblp.org/pers/hd/h/How:Jonathan_P="], "conf": "ijcai", "year": "2015", "pages": 8}