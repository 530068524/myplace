{"title": "Weighted Sums of Random Kitchen Sinks: Replacing minimization with randomization in learning.", "fields": ["minification", "randomization", "inequality", "random neural network", "concentration of measure"], "abstract": "Randomized neural networks are immortalized in this AI Koan:\n\nIn the days when Sussman was a novice, Minsky once came to him as he sat hacking at the PDP-6.\n\n\"What are you doing?\" asked Minsky. \"I am training a randomly wired neural net to play tic-tac-toe,\" Sussman replied. \"Why is the net wired randomly?\" asked Minsky. Sussman replied, \"I do not want it to have any preconceptions of how to play.\"\n\nMinsky then shut his eyes. \"Why do you close your eyes?\" Sussman asked his teacher. \"So that the room will be empty,\" replied Minsky. At that moment, Sussman was enlightened.\n\nWe analyze shallow random networks with the help of concentration of measure inequalities. Specifically, we consider architectures that compute a weighted sum of their inputs after passing them through a bank of arbitrary randomized nonlinearities. We identify conditions under which these networks exhibit good classification performance, and bound their test error in terms of the size of the dataset and the number of random nonlinearities.", "citation": "Citations (308)", "year": "2008", "departments": ["University of California, Berkeley", "California Institute of Technology"], "conf": "nips", "authors": ["Ali Rahimi.....http://dblp.org/pers/hd/r/Rahimi:Ali", "Benjamin Recht.....http://dblp.org/pers/hd/r/Recht:Benjamin"], "pages": 8}