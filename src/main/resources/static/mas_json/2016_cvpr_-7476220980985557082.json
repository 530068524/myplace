{"title": "Where to Look: Focus Regions for Visual Question Answering.", "fields": ["pattern recognition", "information retrieval", "question answering", "artificial intelligence", "machine learning"], "abstract": "We present a method that learns to answer visual questions by selecting image regions relevant to the text-based query. Our method maps textual queries and visual features from various regions into a shared space where they are compared for relevance with an inner product. Our method exhibits significant improvements in answering questions such as \"what color,\" where it is necessary to evaluate a specific location, and \"what room,\" where it selectively identifies informative image regions. Our model is tested on the recently released VQA [1] dataset, which features free-form human-annotated questions and answers.", "citation": "Citations (153)", "year": "2016", "departments": ["University of Illinois at Urbana\u2013Champaign", "University of Illinois at Urbana\u2013Champaign", "University of Illinois at Urbana\u2013Champaign"], "conf": "cvpr", "authors": ["Kevin J. Shih.....http://dblp.org/pers/hd/s/Shih:Kevin_J=", "Saurabh Singh.....http://dblp.org/pers/hd/s/Singh:Saurabh", "Derek Hoiem.....http://dblp.org/pers/hd/h/Hoiem:Derek"], "pages": 9}