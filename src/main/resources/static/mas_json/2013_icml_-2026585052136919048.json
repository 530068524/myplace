{"title": "Regularization of Neural Networks using DropConnect.", "fields": ["regularization", "pattern recognition", "artificial intelligence", "artificial neural network", "machine learning"], "abstract": "We introduce DropConnect, a generalization of Dropout (Hinton et al., 2012), for regularizing large fully-connected layers within neural networks. When training with Dropout, a randomly selected subset of activations are set to zero within each layer. DropConnect instead sets a randomly selected subset of weights within the network to zero. Each unit thus receives input from a random subset of units in the previous layer. We derive a bound on the generalization performance of both Dropout and DropConnect. We then evaluate DropConnect on a range of datasets, comparing to Dropout, and show state-of-the-art results on several image recognition benchmarks by aggregating multiple DropConnect-trained models.", "citation": "Citations (1,068)", "departments": ["New York University", "New York University", "New York University", "New York University", "New York University"], "authors": ["Li Wan.....http://dblp.org/pers/hd/w/Wan:Li", "Matthew D. Zeiler.....http://dblp.org/pers/hd/z/Zeiler:Matthew_D=", "Sixin Zhang.....http://dblp.org/pers/hd/z/Zhang:Sixin", "Yann LeCun.....http://dblp.org/pers/hd/l/LeCun:Yann", "Rob Fergus.....http://dblp.org/pers/hd/f/Fergus:Rob"], "conf": "icml", "year": "2013", "pages": 9}