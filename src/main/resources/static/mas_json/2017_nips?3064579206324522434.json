{"title": "Gradient Descent Can Take Exponential Time to Escape Saddle Points.", "fields": ["almost surely", "initialization", "saddle point", "descent direction", "saddle"], "abstract": "Although gradient descent (GD) almost always escapes saddle points asymptotically [Lee et al., 2016], this paper shows that even with fairly natural random initialization schemes and non-pathological functions, GD can be significantly slowed down by saddle points, taking exponential time to escape. On the other hand, gradient descent with perturbations [Ge et al., 2015, Jin et al., 2017] is not slowed down by saddle points\u2014it can find an approximate local minimizer in polynomial time. This result implies that GD is inherently slower than perturbed GD, and justifies the importance of adding perturbations for efficient non-convex optimization. While our focus is theoretical, we also present experiments that illustrate our theoretical findings.", "citation": "Citations (13)", "year": "2017", "departments": ["Carnegie Mellon University", "University of California, Berkeley", "University of Southern California", "University of California, Berkeley", "Carnegie Mellon University"], "conf": "nips", "authors": ["Simon S. Du.....http://dblp.org/pers/hd/d/Du:Simon_S=", "Chi Jin.....http://dblp.org/pers/hd/j/Jin:Chi", "Jason D. Lee.....http://dblp.org/pers/hd/l/Lee:Jason_D=", "Michael I. Jordan.....http://dblp.org/pers/hd/j/Jordan:Michael_I=", "Aarti Singh.....http://dblp.org/pers/hd/s/Singh:Aarti", "Barnab\u00e1s P\u00f3czos.....http://dblp.org/pers/hd/p/P=oacute=czos:Barnab=aacute=s"], "pages": 11}