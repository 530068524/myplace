{"title": "Generating Text with Recurrent Neural Networks.", "fields": ["state vector", "multiplicative function", "stochastic matrix", "recurrent neural network", "language model"], "abstract": "Recurrent Neural Networks (RNNs) are very powerful sequence models that do not enjoy widespread use because it is extremely difficult to train them properly. Fortunately, recent advances in Hessian-free optimization have been able to overcome the difficulties associated with training RNNs, making it possible to apply them successfully to challenging sequence problems. In this paper we demonstrate the power of RNNs trained with the new Hessian-Free optimizer (HF) by applying them to character-level language modeling tasks. The standard RNN architecture, while effective, is not ideally suited for such tasks, so we introduce a new RNN variant that uses multiplicative (or \"gated\") connections which allow the current input character to determine the transition matrix from one hidden state vector to the next. After training the multiplicative RNN with the HF optimizer for five days on 8 high-end Graphics Processing Units, we were able to surpass the performance of the best previous single method for character-level language modeling \u2013 a hierarchical non-parametric sequence model. To our knowledge this represents the largest recurrent neural network application to date.", "citation": "Citations (693)", "year": "2011", "departments": ["University of Toronto", "University of Toronto", "University of Toronto"], "conf": "icml", "authors": ["Ilya Sutskever.....http://dblp.org/pers/hd/s/Sutskever:Ilya", "James Martens.....http://dblp.org/pers/hd/m/Martens:James", "Geoffrey E. Hinton.....http://dblp.org/pers/hd/h/Hinton:Geoffrey_E="], "pages": 8}