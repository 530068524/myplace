{"title": "Stability and Hypothesis Transfer Learning.", "fields": ["stability", "domain adaptation", "negative transfer", "generalization error", "transfer of learning"], "abstract": "We consider the transfer learning scenario, where the learner does not have access to the source domain directly, but rather operates on the basis of hypotheses induced from it - the Hypothesis Transfer Learning (HTL) problem. Particularly, we conduct a theoretical analysis of HTL by considering the algorithmic stability of a class of HTL algorithms based on Regularized Least Squares with biased regularization. We show that the relatedness of source and target domains accelerates the convergence of the Leave-One-Out error to the generalization error, thus enabling the use of the Leave-One-Out error to find the optimal transfer parameters, even in the presence of a small training set. In case of unrelated domains we also suggest a theoretically principled way to prevent negative transfer, so that in the limit we recover the performance of the algorithm not using any knowledge from the source domain.", "citation": "Citations (57)", "departments": ["Idiap Research Institute", "\u00c9cole Polytechnique", "Toyota Technological Institute at Chicago"], "authors": ["Ilja Kuzborskij.....http://dblp.org/pers/hd/k/Kuzborskij:Ilja", "Francesco Orabona.....http://dblp.org/pers/hd/o/Orabona:Francesco"], "conf": "icml", "year": "2013", "pages": 9}