{"title": "Jointly Learning to Extract and Compress.", "fields": ["fold", "automatic summarization", "inference", "sentence extraction", "rouge"], "abstract": "We learn a joint model of sentence extraction and compression for multi-document summarization. Our model scores candidate summaries according to a combined linear model whose features factor over (1) the n-gram types in the summary and (2) the compressions used. We train the model using a margin-based objective whose loss captures end summary quality. Because of the exponentially large set of candidate summaries, we use a cutting-plane algorithm to incrementally detect and add active constraints efficiently. Inference in our model can be cast as an ILP and thereby solved in reasonable time; we also present a fast approximation scheme which achieves similar performance. Our jointly extracted and compressed summaries outperform both unlearned baselines and our learned extraction-only system on both ROUGE and Pyramid, without a drop in judged linguistic quality. We achieve the highest published ROUGE results to date on the TAC 2008 data set.", "citation": "Citations (125)", "year": "2011", "departments": ["University of California, Berkeley", "University of California, Berkeley", "University of California, Berkeley"], "conf": "acl", "authors": ["Taylor Berg-Kirkpatrick.....http://dblp.org/pers/hd/b/Berg=Kirkpatrick:Taylor", "Dan Gillick.....http://dblp.org/pers/hd/g/Gillick:Dan", "Dan Klein.....http://dblp.org/pers/hd/k/Klein:Dan"], "pages": 10}