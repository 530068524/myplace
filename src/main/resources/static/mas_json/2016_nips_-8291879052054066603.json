{"title": "Accelerating Stochastic Composition Optimization.", "fields": ["oracle", "proximal gradient methods", "regularization", "stochastic optimization", "stochastic neural network"], "abstract": "Consider the stochastic composition optimization problem where the objective is a composition of two expected-value functions. We propose a new stochastic first-order method, namely the accelerated stochastic compositional proximal gradient (ASC-PG) method, which updates based on queries to the sampling oracle using two different timescales. The ASC-PG is the first proximal gradient method for the stochastic composition problem that can deal with nonsmooth regularization penalty. We show that the ASC-PG exhibits faster convergence than the best known algorithms, and that it achieves the optimal sample-error complexity in several important special cases. We further demonstrate the application of ASC-PG to reinforcement learning and conduct numerical experiments.", "citation": "Not cited", "departments": ["Princeton University", "University of Rochester", "Pennsylvania State University"], "authors": ["Mengdi Wang.....http://dblp.org/pers/hd/w/Wang:Mengdi", "Ji Liu.....http://dblp.org/pers/hd/l/Liu:Ji", "Ethan X. Fang.....http://dblp.org/pers/hd/f/Fang:Ethan_X="], "conf": "nips", "year": "2016", "pages": 9}