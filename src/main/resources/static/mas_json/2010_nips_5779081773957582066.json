{"title": "Relaxed Clipping: A Global Training Method for Robust Regression and Classification.", "fields": ["clipping", "huber loss", "robust regression", "truncate", "outlier"], "abstract": "Robust regression and classification are often thought to require non-convex loss functions that prevent scalable, global training. However, such a view neglects the possibility of reformulated training methods that can yield practically solvable alternatives. A natural way to make a loss function more robust to outliers is to truncate loss values that exceed a maximum threshold. We demonstrate that a relaxation of this form of \"loss clipping\" can be made globally solvable and applicable to any standard loss while guaranteeing robustness against outliers. We present a generic procedure that can be applied to standard loss functions and demonstrate improved robustness in regression and classification problems.", "citation": "Citations (17)", "departments": ["University of Alberta", "University of Alberta", "University of Alberta", "University of Alberta", "University of Alberta"], "authors": ["Yaoliang Yu.....http://dblp.org/pers/hd/y/Yu:Yaoliang", "Min Yang.....http://dblp.org/pers/hd/y/Yang:Min", "Linli Xu.....http://dblp.org/pers/hd/x/Xu:Linli", "Martha White.....http://dblp.org/pers/hd/w/White:Martha", "Dale Schuurmans.....http://dblp.org/pers/hd/s/Schuurmans:Dale"], "conf": "nips", "year": "2010", "pages": 9}