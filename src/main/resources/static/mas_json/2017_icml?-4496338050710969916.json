{"title": "On Calibration of Modern Neural Networks.", "fields": ["scaling", "platt scaling", "correctness", "normalization", "document classification"], "abstract": "Confidence calibration -- the problem of predicting probability estimates representative of the true correctness likelihood -- is important for classification models in many applications. We discover that modern neural networks, unlike those from a decade ago, are poorly calibrated. Through extensive experiments, we observe that depth, width, weight decay, and Batch Normalization are important factors influencing calibration. We evaluate the performance of various post-processing calibration methods on state-of-the-art architectures with image and document classification datasets. Our analysis and experiments not only offer insights into neural network learning, but also provide a simple and straightforward recipe for practical settings: on most datasets, temperature scaling -- a single-parameter variant of Platt Scaling -- is surprisingly effective at calibrating predictions.", "citation": "Citations (62)", "year": "2017", "departments": ["Cornell University", "Cornell University", "Cornell University", "Cornell University"], "conf": "icml", "authors": ["Chuan Guo.....http://dblp.org/pers/hd/g/Guo:Chuan", "Geoff Pleiss.....http://dblp.org/pers/hd/p/Pleiss:Geoff", "Yu Sun.....http://dblp.org/pers/hd/s/Sun:Yu", "Kilian Q. Weinberger.....http://dblp.org/pers/hd/w/Weinberger:Kilian_Q="], "pages": 10}