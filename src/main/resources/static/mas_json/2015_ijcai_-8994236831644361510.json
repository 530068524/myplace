{"title": "Learning Context-Sensitive Word Embeddings with Neural Tensor Skip-Gram Model.", "fields": ["polysemy", "machine learning", "tensor", "artificial intelligence", "architecture"], "abstract": "Distributed word representations have a rising interest in NLP community. Most of existing models assume only one vector for each individual word, which ignores polysemy and thus degrades their effectiveness for downstream tasks. To address this problem, some recent work adopts multiprototype models to learn multiple embeddings per word type. In this paper, we distinguish the different senses of each word by their latent topics. We present a general architecture to learn the word and topic embeddings efficiently, which is an extension to the Skip-Gram model and can model the interaction between words and topics simultaneously. The experiments on the word similarity and text classification tasks show our model outperforms state-of-the-art methods.", "citation": "Citations (24)", "departments": ["Fudan University", "Fudan University", "Fudan University"], "authors": ["Pengfei Liu.....http://dblp.org/pers/hd/l/Liu:Pengfei", "Xipeng Qiu.....http://dblp.org/pers/hd/q/Qiu:Xipeng", "Xuanjing Huang.....http://dblp.org/pers/hd/h/Huang:Xuanjing"], "conf": "ijcai", "year": "2015", "pages": 7}