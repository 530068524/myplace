{"title": "Cartesian K-Means.", "fields": ["data point", "membership function", "multipath propagation", "vector quantization", "correlation clustering", "linde buzo gray algorithm", "fuzzy clustering", "cluster", "k means clustering", "cartesian coordinate system", "impulse", "scale invariant feature transform", "gist"], "abstract": "A fundamental limitation of quantization techniques like the k-means clustering algorithm is the storage and run-time cost associated with the large numbers of clusters required to keep quantization errors small and model fidelity high. We develop new models with a compositional parameterization of cluster centers, so representational capacity increases super-linearly in the number of parameters. This allows one to effectively quantize data using billions or trillions of centers. We formulate two such models, Orthogonal k-means and Cartesian k-means. They are closely related to one another, to k-means, to methods for binary hash function optimization like ITQ (Gong and Lazebnik, 2011), and to Product Quantization for vector quantization (Jegou et al., 2011). The models are tested on large-scale ANN retrieval tasks (1M GIST, 1B SIFT features), and on codebook learning for object recognition (CIFAR-10).", "citation": "Not cited", "year": "2013", "departments": ["University of Toronto", "University of Toronto", "Tsinghua University", "University of Southern California", "Tsinghua University", "Lund University", "Tsinghua University", "National University of Computer and Emerging Sciences", "National University of Computer and Emerging Sciences", "National University of Computer and Emerging Sciences"], "conf": "cvpr", "authors": ["Mohammad Norouzi.....http://dblp.org/pers/hd/n/Norouzi_0002:Mohammad", "David J. Fleet.....http://dblp.org/pers/hd/f/Fleet:David_J="], "pages": 8}