{"title": "On primal and dual sparsity of Markov networks.", "fields": ["data set", "regularization", "equivalence", "marginal model", "laplace transform"], "abstract": "Sparsity is a desirable property in high dimensional learning. The  l  1 -norm regularization can lead to primal sparsity, while max-margin methods achieve dual sparsity. Combining these two methods, an  l  1 -norm max-margin Markov network ( l  1 -M 3 N) can achieve both types of sparsity. This paper analyzes its connections to the Laplace max-margin Markov network (LapM 3 N), which inherits the dual sparsity of max-margin models but is pseudo-primal sparse, and to a novel adaptive M 3 N (AdapM 3 N). We show that the  l  1 -M 3 N is an extreme case of the LapM 3 N, and the  l  1 -M 3 N is equivalent to an AdapM 3 N. Based on this equivalence we develop a robust EM-style algorithm for learning an  l  1 -M 3 N. We demonstrate the advantages of the simultaneously (pseudo-) primal and dual sparse models over the ones which enjoy either primal or dual sparsity on both synthetic and real data sets.", "citation": "Citations (6)", "departments": ["Carnegie Mellon University", "Carnegie Mellon University"], "authors": ["Jun Zhu.....http://dblp.org/pers/hd/z/Zhu_0001:Jun", "Eric P. Xing.....http://dblp.org/pers/hd/x/Xing:Eric_P="], "conf": "icml", "year": "2009", "pages": 8}