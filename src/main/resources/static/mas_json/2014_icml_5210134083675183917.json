{"title": "Towards End-To-End Speech Recognition with Recurrent Neural Networks.", "fields": ["phonetic representation", "trigram", "lexicon", "recurrent neural network", "rule based machine translation"], "abstract": "This paper presents a speech recognition system that directly transcribes audio data with text, without requiring an intermediate phonetic representation. The system is based on a combination of the deep bidirectional LSTM recurrent neural network architecture and the Connectionist Temporal Classification objective function. A modification to the objective function is introduced that trains the network to minimise the expectation of an arbitrary transcription loss function. This allows a direct optimisation of the word error rate, even in the absence of a lexicon or language model. The system achieves a word error rate of 27.3% on the Wall Street Journal corpus with no prior linguistic information, 21.9% with only a lexicon of allowed words, and 8.2% with a trigram language model. Combining the network with a baseline system further reduces the error rate to 6.7%.", "citation": "Citations (777)", "year": "2014", "departments": ["Google", "University of Toronto"], "conf": "icml", "authors": ["Alex Graves.....http://dblp.org/pers/hd/g/Graves:Alex", "Navdeep Jaitly.....http://dblp.org/pers/hd/j/Jaitly:Navdeep"], "pages": 9}