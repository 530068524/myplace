{"title": "Veto-Consensus Multiple Kernel Learning.", "fields": ["rademacher complexity", "multiple kernel learning", "convexity", "parameterized complexity", "parametric statistics"], "abstract": "We propose Veto-Consensus Multiple Kernel Learning (VCMKL), a novel way of combining multiple kernels such that one class of samples is described by the logical intersection (consensus) of base kernelized decision rules, whereas the other classes by the union (veto) of their complements. The proposed configuration is a natural fit for domain description and learning with hidden subgroups. We first provide generalization risk bound in terms of the Rademacher complexity of the classifier, then a large margin multi-\u03bd learning objective with tunable training error bound is formulated. Seeing that the corresponding optimization is non-convex and existing methods severely suffer from local minima, we establish a new algorithm, namely Parametric Dual Descent Procedure(PDDP), that can approach global optimum with guarantees. The bases of PDDP are two theorems that reveal the global convexity and local explicitness of the parameterized dual optimum, for which a series of new techniques for parametric program have been developed. The proposed method is evaluated on extensive set of experiments, and the results show significant improvement over the state-of-the-art approaches.", "citation": "Citations (10)", "departments": ["University of California, Berkeley", "University of Amsterdam", "University of California, Berkeley"], "authors": ["Yuxun Zhou.....http://dblp.org/pers/hd/z/Zhou:Yuxun", "Ninghang Hu.....http://dblp.org/pers/hd/h/Hu:Ninghang", "Costas J. Spanos.....http://dblp.org/pers/hd/s/Spanos:Costas_J="], "conf": "aaai", "year": "2016", "pages": 8}