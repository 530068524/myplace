{"title": "Mortal Multi-Armed Bandits.", "fields": ["ad serving", "e commerce", "online advertising", "multi armed bandit", "empirical research"], "abstract": "We formulate and study a new variant of the k-armed bandit problem, motivated by e-commerce applications. In our model, arms have (stochastic) lifetime after which they expire. In this setting an algorithm needs to continuously explore new arms, in contrast to the standard k-armed bandit model in which arms are available indefinitely and exploration is reduced once an optimal arm is identified with near-certainty. The main motivation for our setting is online-advertising, where ads have limited lifetime due to, for example, the nature of their content and their campaign budgets. An algorithm needs to choose among a large collection of ads, more than can be fully explored within the typical ad lifetime.\n\nWe present an optimal algorithm for the state-aware (deterministic reward function) case, and build on this technique to obtain an algorithm for the state-oblivious (stochastic reward function) case. Empirical studies on various reward distributions, including one derived from a real-world ad serving application, show that the proposed algorithms significantly outperform the standard multi-armed bandit approaches applied to these settings.", "citation": "Citations (103)", "year": "2008", "departments": ["Yahoo!", "Yahoo!", "Microsoft", "Brown University"], "conf": "nips", "authors": ["Deepayan Chakrabarti.....http://dblp.org/pers/hd/c/Chakrabarti:Deepayan", "Ravi Kumar.....http://dblp.org/pers/hd/k/Kumar_0001:Ravi", "Filip Radlinski.....http://dblp.org/pers/hd/r/Radlinski:Filip", "Eli Upfal.....http://dblp.org/pers/hd/u/Upfal:Eli"], "pages": 8}