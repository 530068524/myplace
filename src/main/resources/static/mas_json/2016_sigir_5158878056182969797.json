{"title": "Distributional Random Oversampling for Imbalanced Text Classification.", "fields": ["data mining", "statistical classification", "machine learning", "oversampling", "binary number"], "abstract": "The accuracy of many classification algorithms is known to suffer when the data are imbalanced (i.e., when the distribution of the examples across the classes is severely skewed). Many applications of binary text classification are of this type, with the positive examples of the class of interest far outnumbered by the negative examples. Oversampling (i.e., generating synthetic training examples of the minority class) is an often used strategy to counter this problem. We present a new oversampling method specifically designed for classifying data (such as text) for which the distributional hypothesis holds, according to which the meaning of a feature is somehow determined by its distribution in large corpora of data. Our Distributional Random Oversampling method generates new random minority-class synthetic documents by exploiting the distributional properties of the terms in the collection. We discuss results we have obtained on the Reuters-21578, OHSUMED-S, and RCV1-v2 datasets.", "citation": "Citations (2)", "departments": ["Khalifa University", "Consiglio Nazio ... he, Pisa, Italy", "Consiglio Nazio ... he, Pisa, Italy"], "authors": ["Alejandro Moreo.....http://dblp.org/pers/hd/m/Moreo:Alejandro", "Andrea Esuli.....http://dblp.org/pers/hd/e/Esuli:Andrea", "Fabrizio Sebastiani.....http://dblp.org/pers/hd/s/Sebastiani_0001:Fabrizio"], "conf": "sigir", "year": "2016", "pages": 4}