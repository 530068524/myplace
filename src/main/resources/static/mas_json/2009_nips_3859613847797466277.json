{"title": "Potential-Based Agnostic Boosting.", "fields": ["machine learning", "decision tree", "derivation", "computational learning theory", "boosting"], "abstract": "We prove strong noise-tolerance properties of a potential-based boosting algorithm, similar to MadaBoost (Domingo and Watanabe, 2000) and SmoothBoost (Servedio, 2003). Our analysis is in the agnostic framework of Kearns, Schapire and Sellie (1994), giving polynomial-time guarantees in presence of arbitrary noise. A remarkable feature of our algorithm is that it can be implemented without reweighting examples, by randomly relabeling them instead. Our boosting theorem gives, as easy corollaries, alternative derivations of two recent nontrivial results in computational learning theory: agnostically learning decision trees (Gopalan et al, 2008) and agnostically learning halfspaces (Kalai et al, 2005). Experiments suggest that the algorithm performs similarly to MadaBoost.", "citation": "Citations (14)", "year": "2009", "departments": ["Harvard University", "Microsoft"], "conf": "nips", "authors": ["Adam Kalai.....http://dblp.org/pers/hd/k/Kalai:Adam", "Varun Kanade.....http://dblp.org/pers/hd/k/Kanade:Varun"], "pages": 9}