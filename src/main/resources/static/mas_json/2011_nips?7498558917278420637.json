{"title": "High-dimensional regression with noisy and missing data: Provable guarantees with non-convexity.", "fields": ["expectation maximization algorithm", "gradient descent", "estimator", "missing data", "high dimensional statistics"], "abstract": "Although the standard formulations of prediction problems involve fully-observed and noiseless data drawn in an i.i.d. manner, many applications involve noisy and/or missing data, possibly involving dependencies. We study these issues in the context of high-dimensional sparse linear regression, and propose novel estimators for the cases of noisy, missing, and/or dependent data. Many standard approaches to noisy or missing data, such as those using the EM algorithm, lead to optimization problems that are inherently non-convex, and it is difficult to establish theoretical guarantees on practical algorithms. While our approach also involves optimizing non-convex programs, we are able to both analyze the statistical error associated with any global optimum, and prove that a simple projected gradient descent algorithm will converge in polynomial time to a small neighborhood of the set of global minimizers. On the statistical side, we provide non-asymptotic bounds that hold with high probability for the cases of noisy, missing, and/or dependent data. On the computational side, we prove that under the same types of conditions required for statistical consistency, the projected gradient descent algorithm will converge at geometric rates to a near-global minimizer. We illustrate these theoretical predictions with simulations, showing agreement with the predicted scalings.", "citation": "Citations (275)", "year": "2011", "departments": ["University of California, Berkeley", "University of California, Berkeley"], "conf": "nips", "authors": ["Po-Ling Loh.....http://dblp.org/pers/hd/l/Loh:Po=Ling", "Martin J. Wainwright.....http://dblp.org/pers/hd/w/Wainwright:Martin_J="], "pages": 9}