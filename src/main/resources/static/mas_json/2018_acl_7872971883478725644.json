{"title": "Learning to Write with Cooperative Discriminators.", "fields": ["perplexity", "autoregressive model", "discriminative model", "recurrent neural network", "language model"], "abstract": "Recurrent Neural Networks (RNNs) are powerful autoregressive sequence models, but when used to generate natural language their output tends to be overly generic, repetitive, and self-contradictory. We postulate that the objective function optimized by RNN language models, which amounts to the overall perplexity of a text, is not expressive enough to capture the notion of communicative goals described by linguistic principles such as Grice's Maxims. We propose learning a mixture of multiple discriminative models that can be used to complement the RNN generator and guide the decoding process. Human evaluation demonstrates that text generated by our system is preferred over that of baselines by a large margin and significantly enhances the overall coherence, style, and information content of the generated text.", "citation": "Not cited", "year": "2018", "departments": ["University of Washington", "University of Oxford", "University of Washington", "University of Washington"], "conf": "acl", "authors": ["Yejin Choi.....http://dblp.org/pers/hd/c/Choi:Yejin", "Jan Buys.....http://dblp.org/pers/hd/b/Buys:Jan", "David Golub.....http://dblp.org/pers/hd/g/Golub:David", "Ari Holtzman.....http://dblp.org/pers/hd/h/Holtzman:Ari", "Antoine Bosselut.....http://dblp.org/pers/hd/b/Bosselut:Antoine", "Maxwell Forbes.....http://dblp.org/pers/hd/f/Forbes:Maxwell"], "pages": 12}