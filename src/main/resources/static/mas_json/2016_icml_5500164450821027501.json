{"title": "Meta-Gradient Boosted Decision Tree Model for Weight and Target Learning.", "fields": ["stability", "hyperparameter optimization", "incremental decision tree", "gradient boosting", "learning to rank"], "abstract": "Labeled training data is an essential part of any supervised machine learning framework. In practice, there is a trade-off between the quality of a label and its cost. In this paper, we consider a problem of learning to rank on a large-scale dataset with low-quality relevance labels aiming at maximizing the quality of a trained ranker on a small validation dataset with high-quality ground truth relevance labels. Motivated by the classical Gauss-Markov theorem for the linear regression problem, we formulate the problems of (1) reweighting training instances and (2) remapping learning targets. We propose meta-gradient decision tree learning framework for optimizing weight and target functions by applying gradient-based hyperparameter optimization. Experiments on a large-scale real-world dataset demonstrate that we can significantly improve state-of-the-art machine-learning algorithms by incorporating our framework.", "citation": "Citations (1)", "year": "2016", "departments": ["Yandex", "Yandex", "Yandex", "Yandex"], "conf": "icml", "authors": ["Yury Ustinovskiy.....http://dblp.org/pers/hd/u/Ustinovskiy:Yury", "Valentina Fedorova.....http://dblp.org/pers/hd/f/Fedorova:Valentina", "Gleb Gusev.....http://dblp.org/pers/hd/g/Gusev:Gleb", "Pavel Serdyukov.....http://dblp.org/pers/hd/s/Serdyukov:Pavel"], "pages": 10}