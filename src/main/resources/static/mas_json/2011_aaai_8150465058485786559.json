{"title": "Coordinated Multi-Agent Reinforcement Learning in Networked Distributed POMDPs.", "fields": ["distributed constraint optimization", "observability", "distributed learning", "partially observable markov decision process", "reinforcement learning"], "abstract": "In many multi-agent applications such as distributed sensor nets, a network of agents act collaboratively under uncertainty and local interactions. Networked Distributed POMDP (ND-POMDP) provides a framework to model such cooperative multi-agent decision making. Existing work on ND-POMDPs has focused on offline techniques that require accurate models, which are usually costly to obtain in practice. This paper presents a model-free, scalable learning approach that synthesizes multi-agent reinforcement learning (MARL) and distributed constraint optimization (DCOP). By exploiting structured interaction in ND-POMDPs, our approach distributes the learning of the joint policy and employs DCOP techniques to coordinate distributed learning to ensure the global learning performance. Our approach can learn a globally optimal policy for ND-POMDPs with a property called groupwise observability. Experimental results show that, with communication during learning and execution, our approach significantly outperforms the nearly-optimal non-communication policies computed offline.", "citation": "Citations (36)", "departments": ["University of Massachusetts Amherst", "University of Massachusetts Amherst"], "authors": ["Chongjie Zhang.....http://dblp.org/pers/hd/z/Zhang:Chongjie", "Victor R. Lesser.....http://dblp.org/pers/hd/l/Lesser:Victor_R="], "conf": "aaai", "year": "2011", "pages": -1}