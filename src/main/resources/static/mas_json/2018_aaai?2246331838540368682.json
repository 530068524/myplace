{"title": "Multi-attention Recurrent Network for Human Communication Comprehension.", "fields": ["trait", "sentiment analysis", "human communication", "gesture", "modalities"], "abstract": "Human face-to-face communication is a complex multimodal signal. We use words (language modality), gestures (vision modality) and changes in tone (acoustic modality) to convey our intentions. Humans easily process and understand face-to-face communication, however, comprehending this form of communication remains a significant challenge for Artificial Intelligence (AI). AI must understand each modality and the interactions between them that shape human communication. In this paper, we present a novel neural architecture for understanding human communication called the Multi-attention Recurrent Network (MARN). The main strength of our model comes from discovering interactions between modalities through time using a neural component called the Multi-attention Block (MAB) and storing them in the hybrid memory of a recurrent component called the Long-short Term Hybrid Memory (LSTHM). We perform extensive comparisons on six publicly available datasets for multimodal sentiment analysis, speaker trait recognition and emotion recognition. MARN shows state-of-the-art performance on all the datasets.", "citation": "Citations (7)", "departments": ["Carnegie Mellon University", "Carnegie Mellon University", "Nanyang Technological University", "Nanyang Technological University"], "authors": ["Amir Zadeh.....http://dblp.org/pers/hd/z/Zadeh:Amir", "Paul Pu Liang.....http://dblp.org/pers/hd/l/Liang:Paul_Pu", "Soujanya Poria.....http://dblp.org/pers/hd/p/Poria:Soujanya", "Prateek Vij.....http://dblp.org/pers/hd/v/Vij:Prateek", "Erik Cambria.....http://dblp.org/pers/hd/c/Cambria:Erik", "Louis-Philippe Morency.....http://dblp.org/pers/hd/m/Morency:Louis=Philippe"], "conf": "aaai", "year": "2018", "pages": 8}