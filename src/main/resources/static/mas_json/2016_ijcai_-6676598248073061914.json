{"title": "The Dependence of Effective Planning Horizon on Model Accuracy.", "fields": ["overfitting", "phenomenon", "principles of learning", "rademacher complexity", "counting measure", "time horizon", "as is", "markov decision process"], "abstract": "For Markov decision processes with long horizons (i.e., discount factors close to one), it is common in practice to use reduced horizons during planning to speed computation. However, perhaps surprisingly, when the model available to the agent is estimated from data, as will be the case in most real-world problems, the policy found using a shorter planning horizon can actually be better than a policy learned with the true horizon. In this paper we provide a precise explanation for this phenomenon based on principles of learning theory. We show formally that the planning horizon is a complexity control parameter for the class of policies to be learned. In particular, it has an intuitive, monotonic relationship with a simple counting measure of complexity, and that a similar relationship can be observed empirically with a more general and data-dependent Rademacher complexity measure. Each complexity measure gives rise to a bound on the planning loss predicting that a planning horizon shorter than the true horizon can reduce overfitting and improve test performance, and we confirm these predictions empirically.", "citation": "Not cited", "year": "2016", "departments": ["University of Michigan", "University of Michigan", "University of Michigan", "University of Michigan", "University of Michigan", "University of Michigan", "University of Michigan", "University of Michigan"], "conf": "ijcai", "authors": ["Nan Jiang.....http://dblp.org/pers/hd/j/Jiang:Nan", "Alex Kulesza.....http://dblp.org/pers/hd/k/Kulesza:Alex", "Satinder P. Singh.....http://dblp.org/pers/hd/s/Singh:Satinder_P=", "Richard L. Lewis.....http://dblp.org/pers/hd/l/Lewis:Richard_L="], "pages": 10}