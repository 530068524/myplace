{"title": "Flexible sampling of discrete data correlations without the marginal distributions.", "fields": ["marginal distribution", "joint probability distribution", "marginal model", "markov chain monte carlo", "parametric statistics"], "abstract": "Learning the joint dependence of discrete variables is a fundamental problem in machine learning, with many applications including prediction, clustering and dimensionality reduction. More recently, the framework of copula modeling has gained popularity due to its modular parameterization of joint distributions. Among other properties, copulas provide a recipe for combining flexible models for univariate marginal distributions with parametric families suitable for potentially high dimensional dependence structures. More radically, the extended rank likelihood approach of Hoff (2007) bypasses learning marginal models completely when such information is ancillary to the learning task at hand as in, e.g., standard dimensionality reduction problems or copula parameter estimation. The main idea is to represent data by their observable rank statistics, ignoring any other information from the marginals. Inference is typically done in a Bayesian framework with Gaussian copulas, and it is complicated by the fact this implies sampling within a space where the number of constraints increases quadratically with the number of data points. The result is slow mixing when using off-the-shelf Gibbs sampling. We present an efficient algorithm based on recent advances on constrained Hamiltonian Markov chain Monte Carlo that is simple to implement and does not require paying for a quadratic cost in sample size.", "citation": "Citations (5)", "departments": ["University College London", "University College London"], "authors": ["Alfredo A. Kalaitzis.....http://dblp.org/pers/hd/k/Kalaitzis:Alfredo_A=", "Ricardo Bezerra de Andrade e Silva.....http://dblp.org/pers/hd/s/Silva:Ricardo_Bezerra_de_Andrade_e"], "conf": "nips", "year": "2013", "pages": 9}