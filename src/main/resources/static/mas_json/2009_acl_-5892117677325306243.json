{"title": "Learning Semantic Correspondences with Less Supervision.", "fields": ["weather forecasting", "utterance", "generative model", "ambiguity", "language acquisition"], "abstract": "A central problem in grounded language acquisition is learning the correspondences between a rich world state and a stream of text which references that world state. To deal with the high degree of ambiguity present in this setting, we present a generative model that simultaneously segments the text into utterances and maps each utterance to a meaning representation grounded in the world state. We show that our model generalizes across three domains of increasing difficulty---Robocup sportscasting, weather forecasts (a new domain), and NFL recaps.", "citation": "Citations (157)", "year": "2009", "departments": ["University of California, Berkeley", "University of California, Berkeley", "University of California, Berkeley"], "conf": "acl", "authors": ["Percy Liang.....http://dblp.org/pers/hd/l/Liang:Percy", "Michael I. Jordan.....http://dblp.org/pers/hd/j/Jordan:Michael_I=", "Dan Klein.....http://dblp.org/pers/hd/k/Klein:Dan"], "pages": 9}