{"title": "Policy gradients in linearly-solvable MDPs.", "fields": ["mathematics", "mathematical optimization", "artificial intelligence", "machine learning"], "abstract": "We present policy gradient results within the framework of linearly-solvable MDPs. For the first time, compatible function approximators and natural policy gradients are obtained by estimating the cost-to-go function, rather than the (much larger) state-action advantage function as is necessary in traditional MDPs. We also develop the first compatible function approximators and natural policy gradients for continuous-time stochastic systems.", "citation": "Citations (9)", "departments": ["University of Washington"], "authors": ["Emanuel Todorov.....http://dblp.org/pers/hd/t/Todorov:Emanuel"], "conf": "nips", "year": "2010", "pages": 9}