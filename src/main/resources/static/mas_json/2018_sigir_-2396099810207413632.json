{"title": "Multihop Attention Networks for Question Answer Matching.", "fields": ["ranking", "feature learning", "empirical research", "artificial neural network", "question answering"], "abstract": "Attention based neural network models have been successfully applied in answer selection, which is an important subtask of question answering (QA). These models often represent a question by a single vector and find its corresponding matches by attending to candidate answers. However, questions and answers might be related to each other in complicated ways which cannot be captured by single-vector representations. In this paper, we propose Multihop Attention Networks (MAN) which aim to uncover these complex relations for ranking question and answer pairs. Unlike previous models, we do not collapse the question into a single vector, instead we use multiple vectors which focus on different parts of the question for its overall semantic representation and apply multiple steps of attention to learn representations for the candidate answers. For each attention step, in addition to common attention mechanisms, we adopt sequential attention which utilizes context information for computing context-aware attention weights. Via extensive experiments, we show that MAN outperforms state-of-the-art approaches on popular benchmark QA datasets. Empirical studies confirm the effectiveness of sequential attention over other attention mechanisms.", "citation": "Not cited", "departments": ["Leibniz University of Hanover", "Leibniz University of Hanover"], "authors": ["Nam Khanh Tran.....http://dblp.org/pers/hd/t/Tran:Nam_Khanh", "Claudia Nieder\u00e9e.....http://dblp.org/pers/hd/n/Nieder=eacute=e:Claudia"], "conf": "sigir", "year": "2018", "pages": 10}