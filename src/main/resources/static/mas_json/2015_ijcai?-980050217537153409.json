{"title": "Thompson Sampling for Budgeted Multi-Armed Bandits.", "fields": ["bernoulli distribution", "bernoulli s principle", "bernoulli trial", "thompson sampling", "total cost"], "abstract": "Thompson sampling is one of the earliest randomized algorithms for multi-armed bandits (MAB). In this paper, we extend the Thompson sampling to Budgeted MAB, where there is random cost for pulling an arm and the total cost is constrained by a budget. We start with the case of Bernoulli bandits, in which the random rewards (costs) of an arm are independently sampled from a Bernoulli distribution. To implement the Thompson sampling algorithm in this case, at each round, we sample two numbers from the posterior distributions of the reward and cost for each arm, obtain their ratio, select the arm with the maximum ratio, and then update the posterior distributions. We prove that the distribution-dependent regret bound of this algorithm is O(lnB), where B denotes the budget. By introducing a Bernoulli trial, we further extend this algorithm to the setting that the rewards (costs) are drawn from general distributions, and prove that its regret bound remains almost the same. Our simulation results demonstrate the effectiveness of the proposed algorithm.", "citation": "Citations (9)", "departments": ["University of Science and Technology of China", "Chinese Academy of Sciences", "Microsoft", "University of Science and Technology of China", "Microsoft"], "authors": ["Yingce Xia.....http://dblp.org/pers/hd/x/Xia:Yingce", "Haifang Li.....http://dblp.org/pers/hd/l/Li:Haifang", "Tao Qin.....http://dblp.org/pers/hd/q/Qin:Tao", "Nenghai Yu.....http://dblp.org/pers/hd/y/Yu:Nenghai", "Tie-Yan Liu.....http://dblp.org/pers/hd/l/Liu:Tie=Yan"], "conf": "ijcai", "year": "2015", "pages": 7}