{"title": "Learning to Optimize via Information-Directed Sampling.", "fields": ["square", "mutual information", "regret", "thompson sampling", "bernoulli s principle", "online optimization", "gaussian", "multi armed bandit"], "abstract": "We propose information-directed sampling\u2014a new approach to online optimization problems in which a decision maker must balance between exploration and exploitation while learning from partial feedback. Each action is sampled in a manner that minimizes the ratio between squared expected single-period regret and a measure of information gain: the mutual information between the optimal action and the next observation. We establish an expected regret bound for information-directed sampling that applies across a very general class of models and scales with the entropy of the optimal action distribution. We illustrate through simple analytic examples how information-directed sampling accounts for kinds of information that alternative approaches do not adequately address and that this can lead to dramatic performance gains. For the widely studied Bernoulli, Gaussian, and linear bandit problems, we demonstrate state-of-the-art simulation performance. The electronic companion is available at https://doi.org/10.128...", "citation": "Citations (64)", "year": "2014", "departments": ["Stanford University", "Stanford University"], "conf": "nips", "authors": ["Daniel Russo.....http://dblp.org/pers/hd/r/Russo_0001:Daniel", "Benjamin Van Roy.....http://dblp.org/pers/hd/r/Roy:Benjamin_Van"], "pages": 9}