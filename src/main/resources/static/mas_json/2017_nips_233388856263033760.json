{"title": "The Reversible Residual Network: Backpropagation Without Storing Activations.", "fields": ["contextual image classification", "backpropagation", "bottleneck", "convolutional neural network", "residual"], "abstract": "Residual Networks (ResNets) have demonstrated significant improvement over traditional Convolutional Neural Networks (CNNs) on image classification, increasing in performance as networks grow both deeper and wider. However, memory consumption becomes a bottleneck as one needs to store all the intermediate activations for calculating gradients using backpropagation. In this work, we present the Reversible Residual Network (RevNet), a variant of ResNets where each layer's activations can be reconstructed exactly from the next layer's. Therefore, the activations for most layers need not be stored in memory during backprop. We demonstrate the effectiveness of RevNets on CIFAR and ImageNet, establishing nearly identical performance to equally-sized ResNets, with activation storage requirements independent of depth.", "citation": "Citations (17)", "year": "2017", "departments": ["University of Toronto", "University of Toronto", "University of Toronto", "University of Toronto"], "conf": "nips", "authors": ["Aidan N. Gomez.....http://dblp.org/pers/hd/g/Gomez:Aidan_N=", "Mengye Ren.....http://dblp.org/pers/hd/r/Ren:Mengye", "Raquel Urtasun.....http://dblp.org/pers/hd/u/Urtasun:Raquel", "Roger B. Grosse.....http://dblp.org/pers/hd/g/Grosse:Roger_B="], "pages": 11}