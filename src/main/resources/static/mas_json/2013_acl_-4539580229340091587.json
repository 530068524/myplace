{"title": "Improving Text Simplification Language Modeling Using Unsimplified Text Data.", "fields": ["perplexity", "text simplification", "semeval", "lexical simplification", "language model"], "abstract": "In this paper we examine language modeling for text simplification. Unlike some text-to-text translation tasks, text simplification is a monolingual translation task allowing for text in both the input and output domain to be used for training the language model. We explore the relationship between normal English and simplified English and compare language models trained on varying amounts of text from each. We evaluate the models intrinsically with perplexity and extrinsically on the lexical simplification task from SemEval 2012. We find that a combined model using both simplified and normal English data achieves a 23% improvement in perplexity and a 24% improvement on the lexical simplification task over a model trained only on simple data. Post-hoc analysis shows that the additional unsimplified data provides better coverage for unseen and raren-grams.", "citation": "Citations (64)", "year": "2013", "departments": ["Middlebury College"], "conf": "acl", "authors": ["David Kauchak.....http://dblp.org/pers/hd/k/Kauchak:David"], "pages": 10}