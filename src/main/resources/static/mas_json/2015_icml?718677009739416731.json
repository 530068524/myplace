{"title": "Efficient Training of LDA on a GPU by Mean-for-Mode Estimation.", "fields": ["gibbs sampling", "embarrassingly parallel", "reduction", "machine learning", "sampling"], "abstract": "We introduce Mean-for-Mode estimation, a variant of an uncollapsed Gibbs sampler that we use to train LDA on a GPU. The algorithm combines benefits of both uncollapsed and collapsed Gibbs samplers. Like a collapsed Gibbs sampler--and unlike an uncollapsed Gibbs sampler--it has good statistical performance, and can use sampling complexity reduction techniques such as sparsity. Meanwhile, like an uncollapsed Gibbs sampler--and unlike a collapsed Gibbs sampler --it is embarrassingly parallel, and can use approximate counters.", "citation": "Citations (4)", "year": "2015", "departments": ["Oracle Corporation", "Carnegie Mellon University", "Oracle Corporation"], "conf": "icml", "authors": ["Jean-Baptiste Tristan.....http://dblp.org/pers/hd/t/Tristan:Jean=Baptiste", "Joseph Tassarotti.....http://dblp.org/pers/hd/t/Tassarotti:Joseph", "Guy L. Steele Jr......http://dblp.org/pers/hd/s/Steele_Jr=:Guy_L="], "pages": 10}