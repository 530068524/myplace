{"title": "Simpler Context-Dependent Logical Forms via Model Projections.", "fields": ["equivalence class", "machine learning", "natural language processing", "artificial intelligence", "parsing"], "abstract": "We consider the task of learning a context-dependent mapping from utterances to denotations. With only denotations at training time, we must search over a combinatorially large space of logical forms, which is even larger with context-dependent utterances. To cope with this challenge, we perform successive projections of the full model onto simpler models that operate over equivalence classes of logical forms. Though less expressive, we find that these simpler models are much faster and can be surprisingly effective. Moreover, they can be used to bootstrap the full model. Finally, we collected three new context-dependent semantic parsing datasets, and develop a new left-to-right parser.", "citation": "Citations (11)", "year": "2016", "departments": ["Stanford University", "Stanford University"], "conf": "acl", "authors": ["Reginald Long.....http://dblp.org/pers/hd/l/Long:Reginald", "Panupong Pasupat.....http://dblp.org/pers/hd/p/Pasupat:Panupong", "Percy Liang.....http://dblp.org/pers/hd/l/Liang:Percy"], "pages": -1}