{"title": "Reward Design via Online Gradient Ascent.", "fields": ["convergence", "gradient descent", "leverage", "optimization problem", "machine learning"], "abstract": "Recent work has demonstrated that when artificial agents are limited in their ability to achieve their goals, the agent designer can benefit by making the agent's goals different from the designer's. This gives rise to the optimization problem of designing the artificial agent's goals\u2014in the RL framework, designing the agent's reward function. Existing attempts at solving this optimal reward problem do not leverage experience gained online during the agent's lifetime nor do they take advantage of knowledge about the agent's structure. In this work, we develop a gradient ascent approach with formal convergence guarantees for approximately solving the optimal reward problem online during an agent's lifetime. We show that our method generalizes a standard policy gradient approach, and we demonstrate its ability to improve reward functions in agents with various forms of limitations.", "citation": "Citations (17)", "departments": ["University of Michigan", "University of Michigan", "University of Michigan"], "authors": ["Jonathan Sorg.....http://dblp.org/pers/hd/s/Sorg:Jonathan", "Satinder P. Singh.....http://dblp.org/pers/hd/s/Singh:Satinder_P=", "Richard L. Lewis.....http://dblp.org/pers/hd/l/Lewis:Richard_L="], "conf": "nips", "year": "2010", "pages": 9}