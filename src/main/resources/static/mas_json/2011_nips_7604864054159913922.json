{"title": "Sparse recovery by thresholded non-negative least squares.", "fields": ["linear model", "intuition", "non negative least squares", "thresholding", "regularization"], "abstract": "Non-negative data are commonly encountered in numerous fields, making non-negative least squares regression (NNLS) a frequently used tool. At least relative to its simplicity, it often performs rather well in practice. Serious doubts about its usefulness arise for modern high-dimensional linear models. Even in this setting \u2013 unlike first intuition may suggest \u2013 we show that for a broad class of designs, NNLS is resistant to overriding and works excellently for sparse recovery when combined with thresholding, experimentally even outperforming l1-regularization. Since NNLS also circumvents the delicate choice of a regularization parameter, our findings suggest that NNLS may be the method of choice.", "citation": "Citations (28)", "departments": ["Saarland University", "Saarland University"], "authors": ["Martin Slawski.....http://dblp.org/pers/hd/s/Slawski:Martin", "Matthias Hein.....http://dblp.org/pers/hd/h/Hein_0001:Matthias"], "conf": "nips", "year": "2011", "pages": 9}