{"title": "Utterance-Level Multimodal Sentiment Analysis.", "fields": ["sentiment analysis", "word error rate", "utterance", "natural language processing", "modalities"], "abstract": "During real-life interactions, people are naturally gesturing and modulating their voice to emphasize specific points or to express their emotions. With the recent growth of social websites such as YouTube, Facebook, and Amazon, video reviews are emerging as a new source of multimodal and natural opinions that has been left almost untapped by automatic opinion analysis techniques. This paper presents a method for multimodal sentiment classification, which can identify the sentiment expressed in utterance-level visual datastreams. Using a new multimodal dataset consisting of sentiment annotated utterances extracted from video reviews, we show that multimodal sentiment analysis can be effectively performed, and that the joint use of visual, acoustic, and linguistic modalities can lead to error rate reductions of up to 10.5% as compared to the best performing individual modality.", "citation": "Citations (56)", "year": "2013", "departments": ["Robotics Institute", "University of Southern California", "Electrical Engi ... and Engineering"], "conf": "acl", "authors": ["Ver\u00f3nica P\u00e9rez-Rosas.....http://dblp.org/pers/hd/p/P=eacute=rez=Rosas:Ver=oacute=nica", "Rada Mihalcea.....http://dblp.org/pers/hd/m/Mihalcea:Rada", "Louis-Philippe Morency.....http://dblp.org/pers/hd/m/Morency:Louis=Philippe"], "pages": 10}