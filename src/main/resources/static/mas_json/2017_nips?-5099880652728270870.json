{"title": "Context Selection for Embedding Models.", "fields": ["recommender system", "data type", "inference", "conditioning", "embedding"], "abstract": "Word embeddings are an effective tool to analyze language. They have been recently extended to model other types of data beyond text, such as items in recommendation systems. Embedding models consider the probability of a target observation (a word or an item) conditioned on the elements in the context (other words or items). In this paper, we show that conditioning on all the elements in the context is not optimal. Instead, we model the probability of the target conditioned on a learned subset of the elements in the context. We use amortized variational inference to automatically choose this subset. Compared to standard embedding models, this method improves predictions and the quality of the embeddings.", "citation": "Citations (1)", "year": "2017", "departments": ["Tufts University", "Columbia University", "Columbia University"], "conf": "nips", "authors": ["Li-Ping Liu.....http://dblp.org/pers/hd/l/Liu:Li=Ping", "Francisco J. R. Ruiz.....http://dblp.org/pers/hd/r/Ruiz:Francisco_J=_R=", "Susan Athey.....http://dblp.org/pers/hd/a/Athey:Susan", "David M. Blei.....http://dblp.org/pers/hd/b/Blei:David_M="], "pages": 10}