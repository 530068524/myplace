{"title": "Performance optimization of Hadoop workflows in public clouds through adaptive task partitioning.", "fields": ["job shop scheduling", "workflow engine", "special case", "workflow technology", "workflow management system"], "abstract": "Cloud computing provides a cost-effective computing platform for big data workflows where moldable parallel computing models such as MapReduce are widely applied to meet stringent performance requirements. The granularity of task partitioning in each moldable job has a significant impact on workflow completion time and financial cost. We investigate the properties of moldable jobs and design a big-data workflow mapping model, based on which, we formulate a workflow mapping problem to minimize workflow makespan under a budget constraint in public clouds. We show this problem to be strongly NP-complete and design i) a fully polynomial-time approximation scheme (FPTAS) for a special case with a pipeline-structured workflow executed on virtual machines in a single class, and ii) a heuristic for a generalized problem with an arbitrary directed acyclic graph-structured workflow executed on virtual machines in multiple classes. The performance superiority of the proposed solution is illustrated by extensive simulation-based results in Hadoop/YARN in comparison with existing workflow mapping models and algorithms.", "citation": "Not cited", "departments": ["New Jersey Institute of Technology", "New Jersey Institute of Technology"], "authors": ["Tong Shu.....http://dblp.org/pers/hd/s/Shu:Tong", "Chase Q. Wu.....http://dblp.org/pers/hd/w/Wu:Chase_Q="], "conf": "infocom", "year": "2017", "pages": 9}