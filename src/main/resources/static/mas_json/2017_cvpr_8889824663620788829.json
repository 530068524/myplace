{"title": "Deep Visual-Semantic Quantization for Efficient Image Retrieval.", "fields": ["deep learning", "nearest neighbor search", "feature learning", "hash function", "visual word"], "abstract": "Compact coding has been widely applied to approximate nearest neighbor search for large-scale image retrieval, due to its computation efficiency and retrieval quality. This paper presents a compact coding solution with a focus on the deep learning to quantization approach, which improves retrieval quality by end-to-end representation learning and compact encoding and has already shown the superior performance over the hashing solutions for similarity retrieval. We propose Deep Visual-Semantic Quantization (DVSQ), which is the first approach to learning deep quantization models from labeled image data as well as the semantic information underlying general text domains. The main contribution lies in jointly learning deep visual-semantic embeddings and visual-semantic quantizers using carefully-designed hybrid networks and well-specified loss functions. DVSQ enables efficient and effective image retrieval by supporting maximum inner-product search, which is computed based on learned codebooks with fast distance table lookup. Comprehensive empirical evidence shows that DVSQ can generate compact binary codes and yield state-of-the-art similarity retrieval performance on standard benchmarks.", "citation": "Citations (2)", "year": "2017", "departments": ["Tsinghua University", "Tsinghua University", "Tsinghua University", "Tsinghua University"], "conf": "cvpr", "authors": ["Yue Cao.....http://dblp.org/pers/hd/c/Cao:Yue", "Mingsheng Long.....http://dblp.org/pers/hd/l/Long:Mingsheng", "Jianmin Wang.....http://dblp.org/pers/hd/w/Wang_0001:Jianmin", "Shichen Liu.....http://dblp.org/pers/hd/l/Liu:Shichen"], "pages": 10}