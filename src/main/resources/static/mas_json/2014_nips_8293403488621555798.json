{"title": "How hard is my MDP?\" The distribution-norm to the rescue\".", "fields": ["concentration inequality", "kernel", "norm", "reinforcement learning", "markov decision process"], "abstract": "In Reinforcement Learning (RL), state-of-the-art algorithms require a large number of samples per state-action pair to estimate the transition kernel p. In many problems, a good approximation of p is not needed. For instance, if from one state-action pair (s, a), one can only transit to states with the same value, learning p(\u00b7 |s, a) accurately is irrelevant (only its support matters). This paper aims at capturing such behavior by defining a novel hardness measure for Markov Decision Processes (MDPs) based on what we call the distribution-norm. The distribution-norm w.r.t. a measure v is defined on zero v-mean functions f by the standard variation of f with respect to v. We first provide a concentration inequality for the dual of the distribution-norm. This allows us to replace the problem-free, loose \u2016 \u00b7 \u20161 concentration inequalities used in most previous analysis of RL algorithms, with a tighter problem-dependent hardness measure. We then show that several common RL benchmarks have low hardness when measured using the new norm. The distribution-norm captures finer properties than the number of states or the diameter and can be used to assess the difficulty of MDPs.", "citation": "Citations (6)", "year": "2014", "departments": ["Technion \u2013 Israel Institute of Technology", "Technion \u2013 Israel Institute of Technology", "Technion \u2013 Israel Institute of Technology"], "conf": "nips", "authors": ["Odalric-Ambrym Maillard.....http://dblp.org/pers/hd/m/Maillard:Odalric=Ambrym", "Timothy Arthur Mann.....http://dblp.org/pers/hd/m/Mann:Timothy_Arthur", "Shie Mannor.....http://dblp.org/pers/hd/m/Mannor:Shie"], "pages": 9}