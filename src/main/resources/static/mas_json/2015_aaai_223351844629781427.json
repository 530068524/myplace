{"title": "Random Gradient Descent Tree: A Combinatorial Approach for SVM with Outliers.", "fields": ["support vector machine", "outlier", "stochastic gradient descent", "machine learning", "gradient descent", "time complexity", "spacetime", "curse of dimensionality"], "abstract": "Support Vector Machine (SVM) is a fundamental technique in machine learning. A long time challenge facing SVM is how to deal with outliers (caused by misla-beling), as they could make the classes in SVM non-separable. Existing techniques, such as soft margin SVM, v-SVM, and Core-SVM, can alleviate the problem to certain extent, but cannot completely resolve the issue. Recently, there are also techniques available for explicit outlier removal. But they suffer from high time complexity and cannot guarantee quality of solution. In this paper, we present a new combinatorial approach, called Random Gradient Descent Tree (or RGD-tree), to explicitly deal with outliers; this results in a new algorithm called RGD-SVM. Our technique yields prov-ably good solution and can be efficiently implemented for practical purpose. The time and space complexities of our approach only linearly depend on the input size and the dimensionality of the space, which are significantly better than existing ones. Experiments on benchmark datasets suggest that our technique considerably outperforms several popular techniques in most of the cases.", "citation": "Not cited", "departments": ["University at Buffalo", "University at Buffalo"], "authors": ["Hu Ding.....http://dblp.org/pers/hd/d/Ding:Hu", "Jinhui Xu.....http://dblp.org/pers/hd/x/Xu_0001:Jinhui"], "conf": "aaai", "year": "2015", "pages": 7}