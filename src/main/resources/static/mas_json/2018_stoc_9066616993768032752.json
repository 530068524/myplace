{"title": "List-decodable robust mean estimation and learning mixtures of spherical gaussians.", "fields": ["sample complexity", "covariance", "robust statistics", "outlier", "second moment of area"], "abstract": "We study the problem of  list-decodable (robust) Gaussian mean estimation  and the related problem of  learning mixtures of separated spherical Gaussians . In the former problem, we are given a set  T  of points in   n   with the promise that an \u03b1-fraction of points in  T , where 0 G , and no assumptions are made about the remaining points. The goal is to output a small list of candidate vectors with the guarantee that at least one of the candidates is close to the mean of  G . In the latter problem, we are given samples from a  k -mixture of spherical Gaussians on   n   and the goal is to estimate the unknown model parameters up to small accuracy. We develop a set of techniques that yield new efficient algorithms with significantly improved guarantees for these problems. Specifically, our main contributions are as follows:        List-Decodable Mean Estimation. Fix any  d  \u2208  +  and 0 O   d   (( n   d  /\u03b1)) and runtime  O   d   (( n /\u03b1)  d  ) that outputs a list of  O (1/\u03b1) many candidate vectors such that with high probability one of the candidates is within l 2 -distance  O   d  (\u03b1 \u22121/(2 d ) ) from the mean of  G . The only previous algorithm for this problem achieved error O(\u03b1 \u22121/2 ) under second moment conditions. For  d  =  O (1/), where >0 is a constant, our algorithm runs in polynomial time and achieves error  O (\u03b1). For  d  = \u0398(log(1/\u03b1)), our algorithm runs in time ( n /\u03b1)  O (log(1/\u03b1))  and achieves error  O (log 3/2 (1/\u03b1)), almost matching the information-theoretically optimal bound of \u0398(log 1/2 (1/\u03b1)) that we establish. We also give a Statistical Query (SQ) lower bound suggesting that the complexity of our algorithm is qualitatively close to best possible.        Learning Mixtures of Spherical Gaussians. We give a learning algorithm for mixtures of spherical Gaussians, with unknown spherical covariances, that succeeds under significantly weaker separation assumptions compared to prior work. For the prototypical case of a uniform  k -mixture of identity covariance Gaussians we obtain the following: For any >0, if the pairwise separation between the means is at least \u03a9( k +\u221alog(1/\u03b4)), our algorithm learns the unknown parameters within accuracy \u03b4 with sample complexity and running time ( n , 1/\u03b4, ( k /) 1/ ). Moreover, our algorithm is robust to a small dimension-independent fraction of corrupted data. The previously best known polynomial time algorithm required separation at least  k  1/4  ( k /\u03b4). Finally, our algorithm works under separation of O(log 3/2 ( k )+\u221alog(1/\u03b4)) with sample complexity and running time ( n , 1/\u03b4,  k  log k  ). This bound is close to the information-theoretically minimum separation of \u03a9(\u221alog k ).        Our main technical contribution is a new technique, using degree- d  multivariate polynomials, to remove outliers from high-dimensional datasets where the majority of the points are corrupted.", "citation": "Citations (4)", "year": "2018", "departments": ["University of Southern California", "University of California, San Diego", "University of Southern California"], "conf": "stoc", "authors": ["Ilias Diakonikolas.....http://dblp.org/pers/hd/d/Diakonikolas:Ilias", "Daniel M. Kane.....http://dblp.org/pers/hd/k/Kane:Daniel_M=", "Alistair Stewart.....http://dblp.org/pers/hd/s/Stewart:Alistair"], "pages": 14}