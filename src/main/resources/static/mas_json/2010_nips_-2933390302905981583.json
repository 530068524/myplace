{"title": "Short-term memory in neuronal networks through dynamical compressed sensing.", "fields": ["dynamical systems theory", "short term memory", "time delay and integration", "compressed sensing", "feed forward"], "abstract": "Recent proposals suggest that large, generic neuronal networks could store memory traces of past input sequences in their instantaneous state. Such a proposal raises important theoretical questions about the duration of these memory traces and their dependence on network size, connectivity and signal statistics. Prior work, in the case of gaussian input sequences and linear neuronal networks, shows that the duration of memory traces in a network cannot exceed the number of neurons (in units of the neuronal time constant), and that no network can out-perform an equivalent feedforward network. However a more ethologically relevant scenario is that of sparse input sequences. In this scenario, we show how linear neural networks can essentially perform compressed sensing (CS) of past inputs, thereby attaining a memory capacity that exceeds the number of neurons. This enhanced capacity is achieved by a class of \"orthogonal\" recurrent networks and not by feedforward networks or generic recurrent networks. We exploit techniques from the statistical physics of disordered systems to analytically compute the decay of memory traces in such networks as a function of network size, signal sparsity and integration time. Alternately, viewed purely from the perspective of CS, this work introduces a new ensemble of measurement matrices derived from dynamical systems, and provides a theoretical analysis of their asymptotic performance.", "citation": "Citations (25)", "departments": ["University of California, San Francisco", "Interdisciplinary Center for Neural Computation"], "authors": ["Surya Ganguli.....http://dblp.org/pers/hd/g/Ganguli:Surya", "Haim Sompolinsky.....http://dblp.org/pers/hd/s/Sompolinsky:Haim"], "conf": "nips", "year": "2010", "pages": 9}