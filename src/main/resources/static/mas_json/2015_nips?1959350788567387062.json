{"title": "Learning with Incremental Iterative Regularization.", "fields": ["regularization perspectives on support vector machines", "convergence of random variables", "gradient method", "iterated function", "proximal gradient methods for learning"], "abstract": "Within a statistical learning setting, we propose and study an iterative regularization algorithm for least squares defined by an incremental gradient method. In particular, we show that, if all other parameters are fixed a priori, the number of passes over the data (epochs) acts as a regularization parameter, and prove strong universal consistency, i.e. almost sure convergence of the risk, as well as sharp finite sample bounds for the iterates. Our results are a step towards understanding the effect of multiple epochs in stochastic gradient techniques in machine learning and rely on integrating statistical and optimization results.", "citation": "Citations (20)", "year": "2015", "departments": ["Massachusetts Institute of Technology", "Massachusetts Institute of Technology"], "conf": "nips", "authors": ["Lorenzo Rosasco.....http://dblp.org/pers/hd/r/Rosasco:Lorenzo", "Silvia Villa.....http://dblp.org/pers/hd/v/Villa:Silvia"], "pages": 9}