{"title": "Neural Optimizer Search with Reinforcement Learning.", "fields": ["deep learning", "domain specific language", "recurrent neural network", "learning classifier system", "string"], "abstract": "We present an approach to automate the process of discovering optimization methods, with a focus on deep learning architectures. We train a Recurrent Neural Network controller to generate a string in a domain specific language that describes a mathematical update equation based on a list of primitive functions, such as the gradient, running average of the gradient, etc. The controller is trained with Reinforcement Learning to maximize the performance of a model after a few epochs. On CIFAR-10, our method discovers several update rules that are better than many commonly used optimizers, such as Adam, RMSProp, or SGD with and without Momentum on a ConvNet model. We introduce two new optimizers, named PowerSign and AddSign, which we show transfer well and improve training on a variety of different tasks and architectures, including ImageNet classification and Google's neural machine translation system.", "citation": "Citations (14)", "departments": ["Google", "Google", "Google", "Northwestern University"], "authors": ["Irwan Bello.....http://dblp.org/pers/hd/b/Bello:Irwan", "Barret Zoph.....http://dblp.org/pers/hd/z/Zoph:Barret", "Vijay Vasudevan.....http://dblp.org/pers/hd/v/Vasudevan:Vijay", "Quoc V. Le.....http://dblp.org/pers/hd/l/Le:Quoc_V="], "conf": "icml", "year": "2017", "pages": 10}