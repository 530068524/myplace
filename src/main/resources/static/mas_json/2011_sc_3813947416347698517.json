{"title": "Optimizing symmetric dense matrix-vector multiplication on GPUs.", "fields": ["symmetric matrix", "linear algebra", "matrix multiplication", "cuda", "sparse matrix"], "abstract": "GPUs are excellent accelerators for data parallel applications with regular data access patterns. It is challenging, however, to optimize computations with irregular data access patterns on GPUs. One such computation is the Symmetric Matrix Vector product (SYMV) for dense linear algebra. Optimizing the SYMV kernel is important because it forms the basis of fundamental algorithms such as linear solvers and eigenvalue solvers on symmetric matrices. In this work, we present a new algorithm for optimizing the SYMV kernel on GPUs. Our optimized SYMV in single precision brings up to a 7\u00d7 speed up compared to the (latest) CUBLAS 4.0 NVIDIA library on the GTX 280 GPU. Our SYMV kernel tuned for Fermi C2050 is 4.5\u00d7 faster than CUBLAS 4.0 in single precision and 2\u00d7 faster than CUBLAS 4.0 in double precision. Moreover, the techniques used and described in the paper are general enough to be of interest for developing high-performance GPU kernels beyond the particular case of SYMV.", "citation": "Citations (55)", "departments": ["University of California, San Diego", "University of Tennessee", "University of Tennessee", "University of Tennessee"], "authors": ["Rajib Nath.....http://dblp.org/pers/hd/n/Nath:Rajib", "Stanimire Tomov.....http://dblp.org/pers/hd/t/Tomov:Stanimire", "Tingxing Dong.....http://dblp.org/pers/hd/d/Dong:Tingxing", "Jack J. Dongarra.....http://dblp.org/pers/hd/d/Dongarra:Jack_J="], "conf": "sc", "year": "2011", "pages": 10}