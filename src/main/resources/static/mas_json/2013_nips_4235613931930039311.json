{"title": "Direct 0-1 Loss Minimization and Margin Maximization with Boosting.", "fields": ["lpboost", "adaboost", "logitboost", "coordinate descent", "brownboost"], "abstract": "We propose a boosting method, DirectBoost, a greedy coordinate descent algorithm that builds an ensemble classifier of weak classifiers through directly minimizing empirical classification error over labeled training examples; once the training classification error is reduced to a local coordinatewise minimum, Direct-Boost runs a greedy coordinate ascent algorithm that continuously adds weak classifiers to maximize any targeted arbitrarily defined margins until reaching a local coordinatewise maximum of the margins in a certain sense. Experimental results on a collection of machine-learning benchmark datasets show that DirectBoost gives better results than AdaBoost, LogitBoost, LPBoost with column generation and BrownBoost, and is noise tolerant when it maximizes an n'th order bottom sample margin.", "citation": "Citations (9)", "departments": ["Wright State University", "Wright State University", "Wright State University", "Wright State University"], "authors": ["Shaodan Zhai.....http://dblp.org/pers/hd/z/Zhai:Shaodan", "Tian Xia.....http://dblp.org/pers/hd/x/Xia:Tian", "Ming Tan.....http://dblp.org/pers/hd/t/Tan:Ming", "Shaojun Wang.....http://dblp.org/pers/hd/w/Wang:Shaojun"], "conf": "nips", "year": "2013", "pages": 9}