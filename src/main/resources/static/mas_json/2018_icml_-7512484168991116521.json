{"title": "Dissecting Adam: The Sign, Magnitude and Variance of Stochastic Gradients.", "fields": ["deep learning", "stochastic optimization", "momentum", "magnitude", "relative standard deviation"], "abstract": "The ADAM optimizer is exceedingly popular in the deep learning community. Often it works very well, sometimes it doesn\u2019t. Why? We interpret ADAM as a combination of two aspects: for each weight, the update direction is determined by the sign of the stochastic gradient, whereas the update magnitude is solely determined by an estimate of its relative variance. We disentangle these two aspects and analyze them in isolation, shedding light on ADAM \u2019s inner workings. Transferring the \"variance adaptation\u201d to momentum- SGD gives rise to a novel method, completing the practitioner\u2019s toolbox for problems where ADAM fails.", "citation": "Citations (1)", "departments": ["Max Planck Society", "Max Planck Society"], "authors": ["Lukas Balles.....http://dblp.org/pers/hd/b/Balles:Lukas", "Philipp Hennig.....http://dblp.org/pers/hd/h/Hennig:Philipp"], "conf": "icml", "year": "2018", "pages": 10}