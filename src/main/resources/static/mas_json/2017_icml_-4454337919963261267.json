{"title": "Conditional Accelerated Lazy Stochastic Gradient Descent.", "fields": ["oracle", "rate of convergence", "mathematical optimization", "stochastic gradient descent", "machine learning"], "abstract": "In this work we introduce a conditional accelerated lazy stochastic gradient descent algorithm with optimal number of calls to a stochastic first-order oracle and convergence rate $O\\left(\\frac{1}{\\varepsilon^2}\\right)$ improving over the projection-free, Online Frank-Wolfe based stochastic gradient descent of Hazan and Kale [2012] with convergence rate $O\\left(\\frac{1}{\\varepsilon^4}\\right)$.", "citation": "Citations (2)", "year": "2017", "departments": ["Georgia Institute of Technology", "Georgia Institute of Technology", "Georgia Institute of Technology", "Georgia Institute of Technology"], "conf": "icml", "authors": ["Guanghui Lan.....http://dblp.org/pers/hd/l/Lan:Guanghui", "Sebastian Pokutta.....http://dblp.org/pers/hd/p/Pokutta:Sebastian", "Yi Zhou.....http://dblp.org/pers/hd/z/Zhou:Yi", "Daniel Zink.....http://dblp.org/pers/hd/z/Zink:Daniel"], "pages": 10}