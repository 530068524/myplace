{"title": "Speeding Up Automatic Hyperparameter Optimization of Deep Neural Networks by Extrapolation of Learning Curves.", "fields": ["hyperparameter", "hyperparameter optimization", "benchmarking", "stochastic gradient descent", "deep learning"], "abstract": "Deep neural networks (DNNs) show very strong performance on many machine learning problems, but they are very sensitive to the setting of their hyperparameters. Automated hyperparameter optimization methods have recently been shown to yield settings competitive with those found by human experts, but their widespread adoption is hampered by the fact that they require more computational resources than human experts. Humans have one advantage: when they evaluate a poor hyperparameter setting they can quickly detect (after a few steps of stochastic gradient descent) that the resulting network performs poorly and terminate the corresponding evaluation to save time. In this paper, we mimic the early termination of bad runs using a probabilistic model that extrapolates the performance from the first part of a learning curve. Experiments with a broad range of neural network architectures on various prominent object recognition benchmarks show that our resulting approach speeds up state-of-the-art hyperparameter optimization methods for DNNs roughly twofold, enabling them to find DNN settings that yield better performance than those chosen by human experts.", "citation": "Citations (96)", "departments": ["University of Freiburg", "University of Freiburg", "University of Freiburg"], "authors": ["Tobias Domhan.....http://dblp.org/pers/hd/d/Domhan:Tobias", "Jost Tobias Springenberg.....http://dblp.org/pers/hd/s/Springenberg:Jost_Tobias", "Frank Hutter.....http://dblp.org/pers/hd/h/Hutter:Frank"], "conf": "ijcai", "year": "2015", "pages": 9}