{"title": "Stein Variational Gradient Descent as Gradient Flow.", "fields": ["stochastic gradient descent", "nonlinear conjugate gradient method", "function space", "vlasov equation", "kullback leibler divergence"], "abstract": "Stein variational gradient descent (SVGD) is a deterministic sampling algorithm that iteratively transports a set of particles to approximate given distributions, based on a gradient-based update constructed to optimally decrease the KL divergence within a function space. This paper develops the first theoretical analysis on SVGD. We establish that the empirical measures of the SVGD samples weakly converge to the target distribution, and show that the asymptotic behavior of SVGD is characterized by a nonlinear Fokker-Planck equation known as Vlasov equation in physics. We develop a geometric perspective that views SVGD as a gradient flow of the KL divergence functional under a new metric structure on the space of distributions induced by Stein operator.", "citation": "Citations (8)", "year": "2017", "departments": ["Dartmouth College"], "conf": "nips", "authors": ["Qiang Liu.....http://dblp.org/pers/hd/l/Liu:Qiang"], "pages": 9}