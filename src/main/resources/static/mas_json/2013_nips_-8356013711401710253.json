{"title": "Supervised Sparse Analysis and Synthesis Operators.", "fields": ["operator", "sparse approximation", "prior probability", "bilevel optimization", "invariant"], "abstract": "In this paper, we propose a new and computationally efficient framework for learning sparse models. We formulate a unified approach that contains as particular cases models promoting sparse synthesis and analysis type of priors, and mixtures thereof. The supervised training of the proposed model is formulated as a bilevel optimization problem, in which the operators are optimized to achieve the best possible performance on a specific task, e.g., reconstruction or classification. By restricting the operators to be shift invariant, our approach can be thought as a way of learning analysis+synthesis sparsity-promoting convolutional operators. Leveraging recent ideas on fast trainable regressors designed to approximate exact sparse codes, we propose a way of constructing feed-forward neural networks capable of approximating the learned models at a fraction of the computational cost of exact solvers. In the shift-invariant case, this leads to a principled way of constructing task-specific convolutional networks. We illustrate the proposed models on several experiments in music analysis and image processing applications.", "citation": "Citations (17)", "departments": ["Duke University", "Tel Aviv University", "Tel Aviv University", "Tel Aviv University", "Duke University"], "authors": ["Pablo Sprechmann.....http://dblp.org/pers/hd/s/Sprechmann:Pablo", "Roee Litman.....http://dblp.org/pers/hd/l/Litman:Roee", "Tal Ben Yakar.....http://dblp.org/pers/hd/y/Yakar:Tal_Ben", "Alexander M. Bronstein.....http://dblp.org/pers/hd/b/Bronstein:Alexander_M=", "Guillermo Sapiro.....http://dblp.org/pers/hd/s/Sapiro:Guillermo"], "conf": "nips", "year": "2013", "pages": 9}