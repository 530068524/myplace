{"title": "Stein Variational Gradient Descent: A General Purpose Bayesian Inference Algorithm.", "fields": ["kullback leibler divergence", "gradient descent", "stochastic gradient descent", "inference", "bayesian inference"], "abstract": "We propose a general purpose variational inference algorithm that forms a natural counterpart of gradient descent for optimization. Our method iteratively transports a set of particles to match the target distribution, by applying a form of functional gradient descent that minimizes the KL divergence. Empirical studies are performed on various real world models and datasets, on which our method is competitive with existing state-of-the-art methods. The derivation of our method is based on a new theoretical result that connects the derivative of KL divergence under smooth transforms with Stein\u2019s identity and a recently proposed kernelized Stein discrepancy, which is of independent interest.", "citation": "Citations (101)", "departments": ["Dartmouth College", "Dartmouth College"], "authors": ["Qiang Liu.....http://dblp.org/pers/hd/l/Liu_0001:Qiang", "Dilin Wang.....http://dblp.org/pers/hd/w/Wang:Dilin"], "conf": "nips", "year": "2016", "pages": 9}