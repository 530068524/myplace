{"title": "Model-based Reinforcement Learning and the Eluder Dimension.", "fields": ["curse of dimensionality", "cardinality", "parameterized complexity", "markov decision process", "regret"], "abstract": "We consider the problem of learning to optimize an unknown Markov decision process (MDP). We show that, if the MDP can be parameterized within some known function class, we can obtain regret bounds that scale with the dimensionality, rather than cardinality, of the system. We characterize this dependence explicitly as O(\u221adKdET) where T is time elapsed, dK is the Kolmogorov dimension and dE is the eluder dimension. These represent the first unified regret bounds for model-based reinforcement learning and provide state of the art guarantees in several important settings. Moreover, we present a simple and computationally efficient algorithm posterior sampling for reinforcement learning (PSRL) that satisfies these bounds.", "citation": "Citations (16)", "year": "2014", "departments": ["Stanford University", "Stanford University"], "conf": "nips", "authors": ["Ian Osband.....http://dblp.org/pers/hd/o/Osband:Ian", "Benjamin Van Roy.....http://dblp.org/pers/hd/r/Roy:Benjamin_Van"], "pages": 9}