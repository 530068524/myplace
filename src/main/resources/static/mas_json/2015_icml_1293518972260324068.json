{"title": "Adding vs. Averaging in Distributed Primal-Dual Optimization.", "fields": ["scaling", "rate of convergence", "machine learning", "bottleneck", "convergence"], "abstract": "Distributed optimization methods for large-scale machine learning suffer from a communication bottleneck. It is difficult to reduce this bottleneck while still efficiently and accurately aggregating partial work from different machines. In this paper, we present a novel generalization of the recent communication-efficient primal-dual framework (COCOA) for distributed optimization. Our framework, COCOA+, allows for additive combination of local updates to the global parameters at each iteration, whereas previous schemes with convergence guarantees only allow conservative averaging. We give stronger (primal-dual) convergence rate guarantees for both COCOA as well as our new variants, and generalize the theory for both methods to cover non-smooth convex loss functions. We provide an extensive experimental comparison that shows the markedly improved performance of COCOA+ on several real-world distributed datasets, especially when scaling up the number of machines.", "citation": "Citations (88)", "year": "2015", "departments": ["Lehigh University", "University of California, Berkeley", "ETH Zurich", "University of California, Berkeley", "University of Edinburgh"], "conf": "icml", "authors": ["Chenxin Ma.....http://dblp.org/pers/hd/m/Ma:Chenxin", "Virginia Smith.....http://dblp.org/pers/hd/s/Smith:Virginia", "Martin Jaggi.....http://dblp.org/pers/hd/j/Jaggi:Martin", "Michael I. Jordan.....http://dblp.org/pers/hd/j/Jordan:Michael_I=", "Peter Richt\u00e1rik.....http://dblp.org/pers/hd/r/Richt=aacute=rik:Peter", "Martin Tak\u00e1c.....http://dblp.org/pers/hd/t/Tak=aacute=c:Martin"], "pages": 10}