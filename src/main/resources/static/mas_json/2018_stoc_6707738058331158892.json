{"title": "Learning geometric concepts with nasty noise.", "fields": ["concept class", "subspace topology", "adversary", "correctness", "bounded function"], "abstract": "We study the efficient learnability of geometric concept classes \u2014 specifically, low-degree polynomial threshold functions (PTFs) and intersections of halfspaces \u2014 when a fraction of the training data is adversarially corrupted. We give the first polynomial-time PAC learning algorithms for these concept classes with  dimension-independent  error guarantees in the presence of  nasty noise  under the Gaussian distribution. In the nasty noise model, an omniscient adversary can arbitrarily corrupt a small fraction of both the unlabeled data points and their labels. This model generalizes well-studied noise models, including the malicious noise model and the agnostic (adversarial label noise) model. Prior to our work, the only concept class for which efficient malicious learning algorithms were known was the class of  origin-centered  halfspaces.    At the core of our results is an efficient algorithm to approximate the  low-degree Chow-parameters  of any bounded function in the presence of nasty noise. Our robust approximation algorithm for the Chow parameters provides near-optimal error guarantees for a range of distribution families satisfying mild concentration bounds and moment conditions. At the technical level, this algorithm employs an iterative \u201cspectral\u201d technique for outlier detection and removal inspired by recent work in robust unsupervised learning, which makes essential use of low-degree multivariate polynomials.    Our robust learning algorithm for low-degree PTFs provides dimension-independent error guarantees for a class of tame distributions, including Gaussians and, more generally, any logconcave distribution with (approximately) known low-degree moments. For LTFs under the Gaussian distribution, using a refinement of the localization technique, we give a polynomial-time algorithm that achieves a near-optimal error of  O (\u0454), where \u0454 is the noise rate. Our robust learning algorithm for intersections of halfspaces proceeds by projecting down to an appropriate low-dimensional subspace. Its correctness makes essential use of a novel robust inverse independence lemma that is of independent interest.", "citation": "Citations (5)", "year": "2018", "departments": ["University of Southern California", "University of California, San Diego", "University of Southern California"], "conf": "stoc", "authors": ["Ilias Diakonikolas.....http://dblp.org/pers/hd/d/Diakonikolas:Ilias", "Daniel M. Kane.....http://dblp.org/pers/hd/k/Kane:Daniel_M=", "Alistair Stewart.....http://dblp.org/pers/hd/s/Stewart:Alistair"], "pages": 13}