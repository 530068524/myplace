{"title": "Incorporating Prototype Theory in Convolutional Neural Networks.", "fields": ["prototype theory", "computational model", "convolutional neural network", "artificial neural network", "deep learning"], "abstract": "Deep artificial neural networks have made remarkable progress in different tasks in the field of computer vision. However, the empirical analysis of these models and investigation of their failure cases has received attention recently. In this work, we show that deep learning models cannot generalize to atypical images that are substantially different from training images. This is in contrast to the superior generalization ability of the visual system in the human brain. We focus on Convolutional Neural Networks (CNN) as the state-of-the-art models in object recognition and classification; investigate this problem in more detail, and hypothesize that training CNN models suffer from unstructured loss minimization. We propose computational models to improve the generalization capacity of CNNs by considering how typical a training image looks like. By conducting an extensive set of experiments we show that involving a typicality measure can improve the classification results on a new set of images by a large margin. More importantly, this significant improvement is achieved without fine-tuning the CNN model on the target image set.", "citation": "Citations (1)", "year": "2016", "departments": ["Rutgers University", "Rutgers University", "Rutgers University"], "conf": "ijcai", "authors": ["Babak Saleh.....http://dblp.org/pers/hd/s/Saleh:Babak", "Ahmed M. Elgammal.....http://dblp.org/pers/hd/e/Elgammal:Ahmed_M=", "Jacob Feldman.....http://dblp.org/pers/hd/f/Feldman:Jacob"], "pages": 8}