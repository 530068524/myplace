{"title": "On Graduated Optimization for Stochastic Non-Convex Problems.", "fields": ["global optimum", "continuation", "heuristic", "graduated optimization", "regular polygon"], "abstract": "The graduated optimization approach, also known as the continuation method, is a popular heuristic to solving non-convex problems that has received renewed interest over the last decade. Despite being popular, very little is known in terms of its theoretical convergence analysis.\n\nIn this paper we describe a new first-order algorithm based on graduated optimization and analyze its performance. We characterize a family of non-convex functions for which this algorithm provably converges to a global optimum. In particular, we prove that the algorithm converges to an e-approximate solution within O(1/e2) gradient-based steps. We extend our algorithm and analysis to the setting of stochastic non-convex optimization with noisy gradient feedback, attaining the same convergence rate. Additionally, we discuss the setting of \"zero-order optimization\", and devise a variant of our algorithm which converges at rate of O(d2/e4).", "citation": "Citations (17)", "year": "2016", "departments": ["Princeton University", "Technion \u2013 Israel Institute of Technology", "Hebrew University of Jerusalem"], "conf": "icml", "authors": ["Elad Hazan.....http://dblp.org/pers/hd/h/Hazan:Elad", "Kfir Yehuda Levy.....http://dblp.org/pers/hd/l/Levy:Kfir_Yehuda", "Shai Shalev-Shwartz.....http://dblp.org/pers/hd/s/Shalev=Shwartz:Shai"], "pages": 9}