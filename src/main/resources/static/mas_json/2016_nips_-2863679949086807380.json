{"title": "Fairness in Learning: Classic and Contextual Bandits.", "fields": ["polynomial", "confidence interval", "regret", "versa", "exponential function"], "abstract": "We introduce the study of fairness in multi-armed bandit problems. Our fairness definition demands that, given a pool of applicants, a worse applicant is never favored over a better one, despite a learning algorithm\u2019s uncertainty over the true payoffs. In the classic stochastic bandits problem we provide a provably fair algorithm based on \u201cchained\u201d confidence intervals, and prove a cumulative regret bound with a cubic dependence on the number of arms. We further show that any fair algorithm must have such a dependence, providing a strong separation between fair and unfair learning that extends to the general contextual case. In the general contextual case, we prove a tight connection between fairness and the KWIK (Knows What It Knows) learning model: a KWIK algorithm for a class of functions can be transformed into a provably fair contextual bandit algorithm and vice versa. This tight connection allows us to provide a provably fair algorithm for the linear contextual bandit problem with a polynomial dependence on the dimension, and to show (for a different class of functions) a worst-case exponential gap in regret between fair and non-fair learning algorithms.", "citation": "Citations (66)", "departments": ["University of Pennsylvania", "University of Pennsylvania", "University of Pennsylvania", "University of Pennsylvania"], "authors": ["Matthew Joseph.....http://dblp.org/pers/hd/j/Joseph:Matthew", "Michael J. Kearns.....http://dblp.org/pers/hd/k/Kearns:Michael_J=", "Jamie H. Morgenstern.....http://dblp.org/pers/hd/m/Morgenstern:Jamie_H=", "Aaron Roth.....http://dblp.org/pers/hd/r/Roth:Aaron"], "conf": "nips", "year": "2016", "pages": 9}