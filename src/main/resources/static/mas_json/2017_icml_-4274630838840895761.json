{"title": "Modular Multitask Reinforcement Learning with Policy Sketches.", "fields": ["modular design", "abstraction", "transfer of learning", "baseline", "tying"], "abstract": "We describe a framework for multitask deep reinforcement learning guided by\npolicy sketches. Sketches annotate each task with a sequence of named subtasks,\nproviding high-level structural relationships among tasks, but not providing the\ndetailed guidance required by previous work on learning policy abstractions for\nRL (e.g. intermediate rewards, subtask completion signals, or intrinsic motivations).\nOur approach associates every subtask with its own modular subpolicy,\nand jointly optimizes over full task-specific policies by tying parameters across\nshared subpolicies. This optimization is accomplished via a simple decoupled\nactor\u2013critic training objective that facilitates learning common behaviors from\ndissimilar reward functions. We evaluate the effectiveness of our approach on a\nmaze navigation game and a 2-D Minecraft-inspired crafting game. Both games\nfeature extremely sparse rewards that can be obtained only after completing a\nnumber of high-level subgoals (e.g. escaping from a sequence of locked rooms or\ncollecting and combining various ingredients in the proper order). Experiments\nillustrate two main advantages of our approach. First, we outperform standard\nbaselines that learn task-specific or shared monolithic policies. Second, our\nmethod naturally induces a library of primitive behaviors that can be recombined\nto rapidly acquire policies for new tasks.", "citation": "Citations (26)", "departments": ["University of California, Berkeley", "University of California, Berkeley", "University of California, Berkeley"], "authors": ["Jacob Andreas.....http://dblp.org/pers/hd/a/Andreas:Jacob", "Dan Klein.....http://dblp.org/pers/hd/k/Klein:Dan", "Sergey Levine.....http://dblp.org/pers/hd/l/Levine:Sergey"], "conf": "icml", "year": "2017", "pages": 10}