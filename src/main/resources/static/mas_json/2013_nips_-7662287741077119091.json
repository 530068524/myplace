{"title": "Reconciling \"priors\" & \"priors\" without prejudice?", "fields": ["prior probability", "estimator", "penalty method", "bayes estimator", "posterior probability"], "abstract": "There are two major routes to address linear inverse problems. Whereas regularization-based approaches build estimators as solutions of penalized regression optimization problems, Bayesian estimators rely on the posterior distribution of the unknown, given some assumed family of priors. While these may seem radically different approaches, recent results have shown that, in the context of additive white Gaussian denoising, the Bayesian conditional mean estimator is always the solution of a penalized regression problem. The contribution of this paper is twofold. First, we extend the additive white Gaussian denoising results to general linear inverse problems with colored Gaussian noise. Second, we characterize conditions under which the penalty function associated to the conditional mean estimator can satisfy certain popular properties such as convexity, separability, and smoothness. This sheds light on some tradeoff between computational efficiency and estimation accuracy in sparse regularization, and draws some connections between Bayesian estimation and proximal optimization.", "citation": "Citations (3)", "departments": ["French Institute for Research in Computer Science and Automation", "French Institute for Research in Computer Science and Automation"], "authors": ["R\u00e9mi Gribonval.....http://dblp.org/pers/hd/g/Gribonval:R=eacute=mi", "Pierre Machart.....http://dblp.org/pers/hd/m/Machart:Pierre"], "conf": "nips", "year": "2013", "pages": 9}