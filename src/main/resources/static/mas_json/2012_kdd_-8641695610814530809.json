{"title": "NASA: achieving lower regrets and faster rates via adaptive stepsizes.", "fields": ["stochastic optimization", "convex optimization", "stochastic gradient descent", "adaptive learning", "stochastic approximation"], "abstract": "The classic Stochastic Approximation (SA) method achieves optimal rates under the black-box model. This optimality does not rule out better algorithms when more information about functions and data is available.   We present a family of  Noise Adaptive Stochastic Approximation (NASA)  algorithms for online convex optimization and stochastic convex optimization. NASA is an adaptive variant of Mirror Descent Stochastic Approximation. It is novel in its practical variation-dependent stepsizes and better theoretical guarantees. We show that comparing with state-of-the-art adaptive and non-adaptive SA methods, lower regrets and faster rates can be achieved under low-variation assumptions.", "citation": "Not cited", "departments": ["Georgia Institute of Technology", "Georgia Institute of Technology"], "authors": ["Hua Ouyang.....http://dblp.org/pers/hd/o/Ouyang:Hua", "Alexander G. Gray.....http://dblp.org/pers/hd/g/Gray:Alexander_G="], "conf": "kdd", "year": "2012", "pages": 9}