{"title": "HONOR: Hybrid Optimization for NOn-convex Regularized problems.", "fields": ["regular polygon", "gradient descent", "honor", "hessian matrix", "speedup"], "abstract": "Recent years have witnessed the superiority of non-convex sparse learning formulations over their convex counterparts in both theory and practice. However, due to the non-convexity and non-smoothness of the regularizer, how to efficiently solve the non-convex optimization problem for large-scale data is still quite challenging. In this paper, we propose an efficient Hybrid Optimization algorithm for NOn-convex Regularized problems (HONOR). Specifically, we develop a hybrid scheme which effectively integrates a Quasi-Newton (QN) step and a Gradient Descent (GD) step. Our contributions are as follows: (1) HONOR incorporates the second-order information to greatly speed up the convergence, while it avoids solving a regularized quadratic programming and only involves matrix-vector multiplications without explicitly forming the inverse Hessian matrix. (2) We establish a rigorous convergence analysis for HONOR, which shows that convergence is guaranteed even for non-convex problems, while it is typically challenging to analyze the convergence for non-convex problems. (3) We conduct empirical studies on large-scale data sets and results demonstrate that HONOR converges significantly faster than state-of-the-art algorithms.", "citation": "Not cited", "year": "2015", "departments": ["University of Michigan", "University of Michigan"], "conf": "nips", "authors": ["Pinghua Gong.....http://dblp.org/pers/hd/g/Gong:Pinghua", "Jieping Ye.....http://dblp.org/pers/hd/y/Ye:Jieping"], "pages": 9}