{"title": "Automatic Generation of Grounded Visual Questions.", "fields": ["machine learning", "question answering", "computer science", "artificial intelligence", "natural language processing"], "abstract": "In this paper, we propose a new task and solution for vision and language: generation of grounded visual questions. Visual question answering (VQA) is an emerging topic which links textual questions with visual input. To the best of our knowledge, it lacks automatic method to generate reasonable and versatile questions. So far, almost all the textual questions are generated manually, as well as the corresponding answers. To this end, we propose a system that automatically generates visually grounded questions . First, visual input is analyzed with deep caption model. Second, the captions along with VGG-16 features are used as input for our proposed question generator to generate visually grounded questions. Finally, to enable generating of versatile questions, a question type selection module is provided which selects reasonable question types and provide them as parameters for question generation. This is done using a hybrid LSTM with both visual and answer input. Our system is trained using VQA and Visual7W dataset and shows reasonable results on automatically generating of new visual questions. We also propose a quantitative metric for automatic evaluation of the question quality.", "citation": "Citations (4)", "year": "2017", "departments": ["Tianjin University", "Max Planck Society", "Commonwealth Scientific and Industrial Research Organisation", "Nankai University", "Tianjin University"], "conf": "ijcai", "authors": ["Shijie Zhang.....http://dblp.org/pers/hd/z/Zhang:Shijie", "Lizhen Qu.....http://dblp.org/pers/hd/q/Qu:Lizhen", "Shaodi You.....http://dblp.org/pers/hd/y/You:Shaodi", "Zhenglu Yang.....http://dblp.org/pers/hd/y/Yang:Zhenglu", "Jiawan Zhang.....http://dblp.org/pers/hd/z/Zhang:Jiawan"], "pages": 9}