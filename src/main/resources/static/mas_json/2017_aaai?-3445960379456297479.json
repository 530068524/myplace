{"title": "Where to Add Actions in Human-in-the-Loop Reinforcement Learning.", "fields": ["human in the loop", "intuition", "imperfect", "reinforcement learning", "creativity"], "abstract": "In order for reinforcement learning systems to learn quickly in vast action spaces such as the space of all possible pieces of text or the space of all images, leveraging human intuition and creativity is key. However, a human-designed action space is likely to be initially imperfect and limited; furthermore, humans may improve at creating useful actions with practice or new information.  Therefore, we propose a framework in which a human adds actions to a reinforcement learning system over time to boost performance.  In this setting, however, it is key that we use human effort as efficiently as possible, and one significant danger is that humans waste effort adding actions at places (states) that aren't very important.  Therefore, we propose Expected Local Improvement (ELI), an automated method which selects states at which to query humans for a new action.  We evaluate ELI on a variety of simulated domains adapted from the literature, including domains with over a million actions and domains where the simulated experts change over time.  We find ELI demonstrates excellent empirical performance, even in settings where the synthetic \"experts\" are quite poor.", "citation": "Citations (1)", "departments": ["University of Washington", "Carnegie Mellon University", "University of Washington", "Enlearn"], "authors": ["Travis Mandel.....http://dblp.org/pers/hd/m/Mandel:Travis", "Yun-En Liu.....http://dblp.org/pers/hd/l/Liu:Yun=En", "Emma Brunskill.....http://dblp.org/pers/hd/b/Brunskill:Emma", "Zoran Popovic.....http://dblp.org/pers/hd/p/Popovic:Zoran"], "conf": "aaai", "year": "2017", "pages": 7}