{"title": "A Communication-Optimal Framework for Contracting Distributed Tensors.", "fields": ["operator", "matrix multiplication", "supercomputer", "tensor contraction", "distributed memory"], "abstract": "Tensor contractions are extremely compute intensive generalized matrix multiplication operations encountered in many computational science fields, such as quantum chemistry and nuclear physics. Unlike distributed matrix multiplication, which has been extensively studied, limited work has been done in understanding distributed tensor contractions. In this paper, we characterize distributed tensor contraction algorithms on torus networks. We develop a framework with three fundamental communication operators to generate communication-efficient contraction algorithms for arbitrary tensor contractions. We show that for a given amount of memory per processor, the framework is communication optimal for all tensor contractions. We demonstrate performance and scalability of the framework on up to 262,144 cores on a Blue Gene/Q supercomputer.", "citation": "Citations (10)", "year": "2014", "departments": ["Ohio State University", "Ohio State University", "Ohio State University", "Ohio State University", "Pacific Northwest National Laboratory"], "conf": "sc", "authors": ["Samyam Rajbhandari.....http://dblp.org/pers/hd/r/Rajbhandari:Samyam", "Akshay Nikam.....http://dblp.org/pers/hd/n/Nikam:Akshay", "Pai-Wei Lai.....http://dblp.org/pers/hd/l/Lai:Pai=Wei", "Kevin Stock.....http://dblp.org/pers/hd/s/Stock:Kevin", "Sriram Krishnamoorthy.....http://dblp.org/pers/hd/k/Krishnamoorthy:Sriram", "P. Sadayappan.....http://dblp.org/pers/hd/s/Sadayappan:P="], "pages": 12}