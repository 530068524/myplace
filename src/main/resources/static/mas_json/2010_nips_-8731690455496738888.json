{"title": "Direct Loss Minimization for Structured Prediction.", "fields": ["bleu", "binary classification", "hinge loss", "learning rule", "structured prediction"], "abstract": "In discriminative machine learning one is interested in training a system to optimize a certain desired measure of performance, or loss. In binary classification one typically tries to minimizes the error rate. But in structured prediction each task often has its own measure of performance such as the BLEU score in machine translation or the intersection-over-union score in PASCAL segmentation. The most common approaches to structured prediction, structural SVMs and CRFs, do not minimize the task loss: the former minimizes a surrogate loss with no guarantees for task loss and the latter minimizes log loss independent of task loss. The main contribution of this paper is a theorem stating that a certain perceptron-like learning rule, involving features vectors derived from loss-adjusted inference, directly corresponds to the gradient of task loss. We give empirical results on phonetic alignment of a standard test set from the TIMIT corpus, which surpasses all previously reported results on this problem.", "citation": "Citations (90)", "departments": ["Toyota Technological Institute at Chicago", "Toyota Technological Institute at Chicago", "Toyota Technological Institute at Chicago"], "authors": ["David A. McAllester.....http://dblp.org/pers/hd/m/McAllester:David_A=", "Tamir Hazan.....http://dblp.org/pers/hd/h/Hazan:Tamir", "Joseph Keshet.....http://dblp.org/pers/hd/k/Keshet:Joseph"], "conf": "nips", "year": "2010", "pages": 9}