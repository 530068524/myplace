{"title": "Probabilistic Domain Modelling With Contextualized Distributional Semantic Vectors.", "fields": ["distributional semantics", "generative grammar", "probabilistic logic", "generative model", "automatic summarization"], "abstract": "Generative probabilistic models have been used for content modelling and template induction, and are typically trained on small corpora in the target domain. In contrast, vector space models of distributional semantics are trained on large corpora, but are typically applied to domaingeneral lexical disambiguation tasks. We introduce Distributional Semantic Hidden Markov Models, a novel variant of a hidden Markov model that integrates these two approaches by incorporating contextualized distributional semantic vectors into a generative model as observed emissions. Experiments in slot induction show that our approach yields improvements in learning coherent entity clusters in a domain. In a subsequent extrinsic evaluation, we show that these improvements are also reflected in multi-document summarization.", "citation": "Citations (3)", "year": "2013", "departments": ["University of Toronto", "University of Toronto"], "conf": "acl", "authors": ["Jackie Chi Kit Cheung.....http://dblp.org/pers/hd/c/Cheung:Jackie_Chi_Kit", "Gerald Penn.....http://dblp.org/pers/hd/p/Penn:Gerald"], "pages": 10}