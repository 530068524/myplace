{"title": "To Understand Deep Learning We Need to Understand Kernel Learning.", "fields": ["overfitting", "deep learning", "kernel method", "phenomenon", "kernel"], "abstract": "Generalization performance of classifiers in deep learning has recently become a subject of intense study. Heavily over-parametrized deep models tend to fit training data exactly. Despite overfitting, they perform well on test data, a phenomenon not yet fully understood. \nThe first point of our paper is that strong performance of overfitted classifiers is not a unique feature of deep learning. Using real-world and synthetic datasets, we establish that kernel classifiers trained to have zero classification error (overfitting) or even zero regression error (interpolation) perform very well on test data. \nWe proceed to prove lower bounds on the norm of overfitted solutions for smooth kernels, showing that they increase nearly exponentially with the data size. Since most generalization bounds depend polynomially on the norm of the solution, this result implies that they diverge as data increases. Furthermore, the existing bounds do not apply to interpolated classifiers. \nWe also show experimentally that (non-smooth) Laplacian kernels easily fit random labels using a version of SGD, a finding that parallels results reported for ReLU neural networks. In contrast, fitting noisy data requires many more epochs for smooth Gaussian kernels. The observation that the performance of overfitted Laplacian and Gaussian classifiers on the test is quite similar, suggests that generalization is tied to the properties of the kernel function rather than the optimization process. \nWe see that some key phenomena of deep learning are manifested similarly in kernel methods in the overfitted regime. We argue that progress on understanding deep learning will be difficult, until more analytically tractable \"shallow\" kernel methods are better understood. The experimental and theoretical results presented in this paper indicate a need for new theoretical ideas for understanding classical kernel methods.", "citation": "Citations (1)", "departments": ["Ohio State University", "Ohio State University"], "authors": ["Mikhail Belkin.....http://dblp.org/pers/hd/b/Belkin:Mikhail", "Siyuan Ma.....http://dblp.org/pers/hd/m/Ma:Siyuan", "Soumik Mandal.....http://dblp.org/pers/hd/m/Mandal:Soumik"], "conf": "icml", "year": "2018", "pages": 9}