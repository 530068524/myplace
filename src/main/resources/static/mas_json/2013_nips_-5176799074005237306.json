{"title": "Optimistic policy iteration and natural actor-critic: A unifying view and a non-optimality result.", "fields": ["markov property", "special case", "reinforcement learning", "pathological", "bellman equation"], "abstract": "Approximate dynamic programming approaches to the reinforcement learning problem are often categorized into greedy value function methods and value-based policy gradient methods. As our first main result, we show that an important subset of the latter methodology is, in fact, a limiting special case of a general formulation of the former methodology; optimistic policy iteration encompasses not only most of the greedy value function methods but also natural actor-critic methods, and permits one to directly interpolate between them. The resulting continuum adjusts the strength of the Markov assumption in policy improvement and, as such, can be seen as dual in spirit to the continuum in TD(\u03bb)-style algorithms in policy evaluation. As our second main result, we show for a substantial subset of soft-greedy value function approaches that, while having the potential to avoid policy oscillation and policy chattering, this subset can never converge toward an optimal policy, except in a certain pathological case. Consequently, in the context of approximations (either in state estimation or in value function representation), the majority of greedy value function methods seem to be deemed to suffer either from the risk of oscillation/chattering or from the presence of systematic sub-optimality.", "citation": "Citations (1)", "departments": ["Aalto University"], "authors": ["Paul Wagner.....http://dblp.org/pers/hd/w/Wagner:Paul"], "conf": "nips", "year": "2013", "pages": 9}