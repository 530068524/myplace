{"title": "Inexact Proximal Gradient Methods for Non-Convex and Non-Smooth Optimization.", "fields": ["operator", "proximal gradient methods", "gradient method", "regular polygon", "proximal gradient methods for learning"], "abstract": "Non-convex and non-smooth optimization plays an important role in machine learning. Proximal gradient method is one of the most important methods for solving the non-convex and non-smooth problems, where a proximal operator need to be solved exactly for each step. However, in a lot of problems the proximal operator does not have an analytic solution, or is expensive to obtain an exact solution. In this paper, we propose inexact proximal gradient methods (not only a basic inexact proximal gradient method (IPG), but also a Nesterov's accelerated inexact proximal gradient method (AIPG)) for non-convex and non-smooth optimization, which tolerate an error in the calculation of the proximal operator. Theoretical analysis shows that IPG and AIPG have the same convergence rates as in the error-free case, provided that the errors decrease at appropriate rates.", "citation": "Not cited", "departments": ["University of Pittsburgh", "University of Pittsburgh", "University of Texas at Arlington"], "authors": ["Bin Gu.....http://dblp.org/pers/hd/g/Gu:Bin", "De Wang.....http://dblp.org/pers/hd/w/Wang:De", "Zhouyuan Huo.....http://dblp.org/pers/hd/h/Huo:Zhouyuan", "Heng Huang.....http://dblp.org/pers/hd/h/Huang:Heng"], "conf": "aaai", "year": "2018", "pages": 8}