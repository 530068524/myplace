{"title": "Generalization and Equilibrium in Generative Adversarial Nets (GANs).", "fields": ["open problem", "machine learning", "generative grammar", "artificial neural network", "adversarial system"], "abstract": "This paper makes progress on several open theoretical issues related to Generative Adversarial Networks. A definition is provided for what it means for the training to generalize, and it is shown that generalization is not guaranteed for the popular distances between distributions such as Jensen-Shannon or Wasserstein. We introduce a new metric called neural net distance for which generalization does occur. We also show that an approximate pure equilibrium in the 2-player game exists for a natural training objective (Wasserstein). Showing such a result has been an open problem (for any training objective). \nFinally, the above theoretical ideas lead us to propose a new training protocol, MIX+GAN, which can be combined with any existing method. We present experiments showing that it stabilizes and improves some existing methods.", "citation": "Citations (110)", "departments": ["Princeton University", "Duke University", "Princeton University", "Princeton University", "Princeton University"], "authors": ["Sanjeev Arora.....http://dblp.org/pers/hd/a/Arora:Sanjeev", "Rong Ge.....http://dblp.org/pers/hd/g/Ge_0001:Rong", "Yingyu Liang.....http://dblp.org/pers/hd/l/Liang:Yingyu", "Tengyu Ma.....http://dblp.org/pers/hd/m/Ma:Tengyu", "Yi Zhang.....http://dblp.org/pers/hd/z/Zhang:Yi"], "conf": "icml", "year": "2017", "pages": 9}