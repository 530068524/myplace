{"title": "Regularized Policy Iteration.", "fields": ["function approximation", "regularization", "reproducing kernel hilbert space", "residual", "temporal difference learning"], "abstract": "In this paper we consider approximate policy-iteration-based reinforcement learning algorithms. In order to implement a flexible function approximation scheme we propose the use of non-parametric methods with regularization, providing a convenient way to control the complexity of the function approximator. We propose two novel regularized policy iteration algorithms by adding L2-regularization to two widely-used policy evaluation methods: Bellman residual minimization (BRM) and least-squares temporal difference learning (LSTD). We derive efficient implementation for our algorithms when the approximate value-functions belong to a reproducing kernel Hilbert space. We also provide finite-sample performance bounds for our algorithms and show that they are able to achieve optimal rates of convergence under the studied conditions.", "citation": "Citations (128)", "year": "2008", "departments": ["University of Alberta", "French Institute for Research in Computer Science and Automation", "McGill University", "University of Alberta"], "conf": "nips", "authors": ["Amir Massoud Farahmand.....http://dblp.org/pers/hd/f/Farahmand:Amir_Massoud", "Mohammad Ghavamzadeh.....http://dblp.org/pers/hd/g/Ghavamzadeh:Mohammad", "Csaba Szepesv\u00e1ri.....http://dblp.org/pers/hd/s/Szepesv=aacute=ri:Csaba", "Shie Mannor.....http://dblp.org/pers/hd/m/Mannor:Shie"], "pages": 8}