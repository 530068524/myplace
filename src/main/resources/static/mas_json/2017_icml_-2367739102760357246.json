{"title": "SARAH: A Novel Method for Machine Learning Problems Using Stochastic Recursive Gradient.", "fields": ["inner loop", "recursion", "convex function", "convexity", "rate of convergence"], "abstract": "In this paper, we propose a StochAstic Recursive grAdient algoritHm (SARAH), as well as its practical variant SARAH+, as a novel approach to the finite-sum minimization problems. Different from the vanilla SGD and other modern stochastic methods such as SVRG, S2GD, SAG and SAGA, SARAH admits a simple recursive framework for updating stochastic gradient estimates; when comparing to SAG/SAGA, SARAH does not require a storage of past gradients. The linear convergence rate of SARAH is proven under strong convexity assumption. We also prove a linear convergence rate (in the strongly convex case) for an inner loop of SARAH, the property that SVRG does not possess. Numerical experiments demonstrate the efficiency of our algorithm.", "citation": "Citations (11)", "year": "2017", "departments": ["Lehigh University", "Lehigh University", "Lehigh University", "Lehigh University"], "conf": "icml", "authors": ["Lam M. Nguyen.....http://dblp.org/pers/hd/n/Nguyen:Lam_M=", "Jie Liu.....http://dblp.org/pers/hd/l/Liu:Jie", "Katya Scheinberg.....http://dblp.org/pers/hd/s/Scheinberg:Katya", "Martin Tak\u00e1c.....http://dblp.org/pers/hd/t/Tak=aacute=c:Martin"], "pages": 9}