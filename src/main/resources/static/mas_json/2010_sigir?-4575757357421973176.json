{"title": "The effect of assessor error on IR system evaluation.", "fields": ["crowdsourcing", "scaling", "robustness", "systematic error", "information retrieval"], "abstract": "Recent efforts in test collection building have focused on scaling back the number of necessary relevance judgments and then scaling up the number of search topics. Since the largest source of variation in a Cranfield-style experiment comes from the topics, this is a reasonable approach. However, as topic set sizes grow, and researchers look to crowdsourcing and Amazon's Mechanical Turk to collect relevance judgments, we are faced with issues of quality control. This paper examines the robustness of the TREC Million Query track methods when some assessors make significant and systematic errors. We find that while averages are robust, assessor errors can have a large effect on system rankings.", "citation": "Citations (95)", "departments": ["University of Delaware", "National Institute of Standards and Technology"], "authors": ["Ben Carterette.....http://dblp.org/pers/hd/c/Carterette:Ben", "Ian Soboroff.....http://dblp.org/pers/hd/s/Soboroff:Ian"], "conf": "sigir", "year": "2010", "pages": 8}