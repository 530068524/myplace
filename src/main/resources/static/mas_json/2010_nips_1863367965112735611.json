{"title": "Parallelized Stochastic Gradient Descent.", "fields": ["multi core processor", "backpropagation", "stochastic gradient descent", "gradient method", "latency"], "abstract": "With the increase in available data parallel machine learning has become an increasingly pressing problem. In this paper we present the first parallel stochastic gradient descent algorithm including a detailed analysis and experimental evidence. Unlike prior work on parallel optimization algorithms [5, 7] our variant comes with parallel acceleration guarantees and it poses no overly tight latency constraints, which might only be available in the multicore setting. Our analysis introduces a novel proof technique \u2014 contractive mappings to quantify the speed of convergence of parameter distributions to their asymptotic limits. As a side effect this answers the question of how quickly stochastic gradient descent algorithms reach the asymptotically normal regime [1, 8].", "citation": "Citations (665)", "departments": ["Yahoo!", "Yahoo!", "Yahoo!", "Yahoo!"], "authors": ["Martin Zinkevich.....http://dblp.org/pers/hd/z/Zinkevich:Martin", "Markus Weimer.....http://dblp.org/pers/hd/w/Weimer:Markus", "Alexander J. Smola.....http://dblp.org/pers/hd/s/Smola:Alexander_J=", "Lihong Li.....http://dblp.org/pers/hd/l/Li_0001:Lihong"], "conf": "nips", "year": "2010", "pages": 9}