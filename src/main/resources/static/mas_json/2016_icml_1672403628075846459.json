{"title": "Recurrent Orthogonal Networks and Long-Memory Tasks.", "fields": ["computer science", "matrix", "artificial intelligence", "machine learning"], "abstract": "Although RNNs have been shown to be powerful tools for processing sequential data, finding architectures or optimization strategies that allow them to model very long term dependencies is still an active area of research. In this work, we carefully analyze two synthetic datasets originally outlined in (Hochreiter & Schmidhuber, 1997) which are used to evaluate the ability of RNNs to store information over many time steps. We explicitly construct RNN solutions to these problems, and using these constructions, illuminate both the problems themselves and the way in which RNNs store different types of information in their hidden states. These constructions furthermore explain the success of recent methods that specify unitary initializations or constraints on the transition matrices.", "citation": "Citations (21)", "year": "2016", "departments": ["New York University", "Facebook", "New York University"], "conf": "icml", "authors": ["Mikael Henaff.....http://dblp.org/pers/hd/h/Henaff:Mikael", "Arthur Szlam.....http://dblp.org/pers/hd/s/Szlam:Arthur", "Yann LeCun.....http://dblp.org/pers/hd/l/LeCun:Yann"], "pages": 9}