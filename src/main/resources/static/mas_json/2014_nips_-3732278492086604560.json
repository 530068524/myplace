{"title": "RAAM: The Benefits of Robustness in Approximating Aggregated MDPs in Reinforcement Learning.", "fields": ["approximation error", "robustness", "bellman equation", "markov decision process", "reinforcement learning"], "abstract": "We describe how to use robust Markov decision processes for value function approximation with state aggregation. The robustness serves to reduce the sensitivity to the approximation error of sub-optimal policies in comparison to classical methods such as fitted value iteration. This results in reducing the bounds on the \u03b3-discounted infinite horizon performance loss by a factor of 1/(1-\u03b3) while preserving polynomial-time computational complexity. Our experimental results show that using the robust representation can significantly improve the solution quality with minimal additional computational cost.", "citation": "Citations (2)", "year": "2014", "departments": ["IBM", "IBM"], "conf": "nips", "authors": ["Marek Petrik.....http://dblp.org/pers/hd/p/Petrik:Marek", "Dharmashankar Subramanian.....http://dblp.org/pers/hd/s/Subramanian:Dharmashankar"], "pages": 9}