{"title": "Variational Inference for Policy Search in changing situations.", "fields": ["machine learning", "robotics", "inference", "robot", "search algorithm"], "abstract": "Many policy search algorithms minimize the Kullback-Leibler (KL) divergence to a certain target distribution in order to fit their policy. The commonly used KL-divergence forces the resulting policy to be 'reward-attracted'. The policy tries to reproduce all positively rewarded experience while negative experience is neglected. However, the KL-divergence is not symmetric and we can also minimize the the reversed KL-divergence, which is typically used in variational inference. The policy now becomes 'cost-averse'. It tries to avoid reproducing any negatively-rewarded experience while maximizing exploration.\n\nDue to this 'cost-averseness' of the policy, Variational Inference for Policy Search (VIP) has several interesting properties. It requires no kernel-band with nor exploration rate, such settings are determined automatically by the inference. The algorithm meets the performance of state-of-the-art methods while being applicable to simultaneously learning in multiple situations.\n\nWe concentrate on using VIP for policy search in robotics. We apply our algorithm to learn dynamic counterbalancing of different kinds of pushes with human-like 2-link and 4-link robots.", "citation": "Citations (28)", "year": "2011", "departments": ["Graz University of Technology"], "conf": "icml", "authors": ["Gerhard Neumann.....http://dblp.org/pers/hd/n/Neumann:Gerhard"], "pages": 8}