{"title": "The Multilinear Structure of ReLU Networks.", "fields": ["bilinear interpolation", "multilinear form", "linear separability", "rectifier", "multilinear map"], "abstract": "We study the loss surface of neural networks that involve only rectified linear unit (ReLU) nonlinearities from a theoretical point-of-view. Any such network defines a piecewise multilinear form in parameter space. As a consequence, optima of such networks generically occur in non-differentiable regions of parameter space and so any understanding of such networks must carefully take into account their non-smooth nature. We then proceed to leverage this multilinear structure in an analysis of a neural network with one hidden-layer. Under the assumption of linearly separable data, the piecewise bilinear structure of the loss allows us to provide an explicit description of all critical points.", "citation": "Citations (1)", "departments": ["Loyola Marymount University", "California State University, Long Beach"], "authors": ["Thomas Laurent.....http://dblp.org/pers/hd/l/Laurent:Thomas", "James von Brecht.....http://dblp.org/pers/hd/b/Brecht:James_von"], "conf": "icml", "year": "2018", "pages": 9}