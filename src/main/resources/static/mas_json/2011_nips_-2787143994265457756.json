{"title": "On Tracking The Partition Function.", "fields": ["boltzmann machine", "importance sampling", "estimator", "gradient descent", "random field"], "abstract": "Markov Random Fields (MRFs) have proven very powerful both as density estimators and feature extractors for classification. However, their use is often limited by an inability to estimate the partition function Z. In this paper, we exploit the gradient descent training procedure of restricted Boltzmann machines (a type of MRF) to track the log partition function during learning. Our method relies on two distinct sources of information: (1) estimating the change AZ incurred by each gradient update, (2) estimating the difference in Z over a small set of tempered distributions using bridge sampling. The two sources of information are then combined using an inference procedure similar to Kalman filtering. Learning MRFs through Tempered Stochastic Maximum Likelihood, we can estimate Z using no more temperatures than are required for learning. Comparing to both exact values and estimates using annealed importance sampling (AIS), we show on several datasets that our method is able to accurately track the log partition function. In contrast to AIS, our method provides this estimate at each time-step, at a computational cost similar to that required for training alone.", "citation": "Citations (11)", "departments": ["Universit\u00e9 de Montr\u00e9al", "Universit\u00e9 de Montr\u00e9al", "Universit\u00e9 de Montr\u00e9al"], "authors": ["Guillaume Desjardins.....http://dblp.org/pers/hd/d/Desjardins:Guillaume", "Aaron C. Courville.....http://dblp.org/pers/hd/c/Courville:Aaron_C=", "Yoshua Bengio.....http://dblp.org/pers/hd/b/Bengio:Yoshua"], "conf": "nips", "year": "2011", "pages": 9}