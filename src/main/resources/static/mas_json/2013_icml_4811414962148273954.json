{"title": "Learning with Marginalized Corrupted Features.", "fields": ["stability", "data set", "training set", "test data", "exponential family"], "abstract": "The goal of machine learning is to develop predictors that generalize well to test data. Ideally, this is achieved by training on very large (infinite) training data sets that capture all variations in the data distribution. In the case of finite training data, an effective solution is to extend the training set with artificially created examples--which, however, is also computationally costly. We propose to corrupt training examples with noise from known distributions within the exponential family and present a novel learning algorithm, called marginalized corrupted features (MCF), that trains robust predictors by minimizing the expected value of the loss function under the corrupting distribution-- essentially learning with infinitely many (corrupted) training examples. We show empirically on a variety of data sets that MCF classifiers can be trained efficiently, may generalize substantially better to test data, and are more robust to feature deletion at test time.", "citation": "Citations (114)", "departments": ["Delft University of Technology", "Washington University in St. Louis", "Washington University in St. Louis", "Washington University in St. Louis"], "authors": ["Laurens van der Maaten.....http://dblp.org/pers/hd/m/Maaten:Laurens_van_der", "Minmin Chen.....http://dblp.org/pers/hd/c/Chen:Minmin", "Stephen Tyree.....http://dblp.org/pers/hd/t/Tyree:Stephen", "Kilian Q. Weinberger.....http://dblp.org/pers/hd/w/Weinberger:Kilian_Q="], "conf": "icml", "year": "2013", "pages": 9}