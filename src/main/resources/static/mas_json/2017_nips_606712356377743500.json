{"title": "Discriminative State Space Models.", "fields": ["parameter space", "vector autoregression", "state space", "pose", "cusum", "convex optimization", "gaussian", "state space representation", "graphical model", "discriminative model", "linear dynamical system", "motion estimation", "speech corpus", "structural risk minimization", "autoregressive model", "mathematical optimization", "machine learning", "time domain", "word error rate"], "abstract": "In this paper we compare three frameworks for discriminative training of continuous-density hidden Markov models (CD-HMMs). Specifically, we compare two popular frameworks, based on conditional maximum likelihood (CML) and minimum classification error (MCE), to a new framework based on margin maximization. Unlike CML and MCE, our formulation of large margin training explicitly penalizes incorrect decodings by an amount proportional to the number of mislabeled hidden states. It also leads to a convex optimization over the parameter space of CD-HMMs, thus avoiding the problem of spurious local minima. We used discriminatively trained CD-HMMs from all three frameworks to build phonetic recognizers on the TIMIT speech corpus. The different recognizers employed exactly the same acoustic front end and hidden state space, thus enabling us to isolate the effect of different cost functions, parameterizations, and numerical optimizations. Experimentally, we find that our framework for large margin training yields significantly lower error rates than both CML and MCE training.", "citation": "Not cited", "year": "2017", "departments": ["University of California, Berkeley", "University of California, San Diego", "Johns Hopkins University", "Johns Hopkins University", "Rutgers University", "Rutgers University", "Google", "Courant Institute of Mathematical Sciences"], "conf": "nips", "authors": ["Vitaly Kuznetsov.....http://dblp.org/pers/hd/k/Kuznetsov:Vitaly", "Mehryar Mohri.....http://dblp.org/pers/hd/m/Mohri:Mehryar"], "pages": 9}