{"title": "Transformation-Grounded Image Generation Network for Novel 3D View Synthesis.", "fields": ["machine learning", "view synthesis", "solid modeling", "pixel", "distortion"], "abstract": "We present a transformation-grounded image generation network for novel 3D view synthesis from a single image. Our approach first explicitly infers the parts of the geometry visible both in the input and novel views and then casts the remaining synthesis problem as image completion. Specifically, we both predict a flow to move the pixels from the input to the novel view along with a novel visibility map that helps deal with occulsion/disocculsion. Next, conditioned on those intermediate results, we hallucinate (infer) parts of the object invisible in the input image. In addition to the new network structure, training with a combination of adversarial and perceptual loss results in a reduction in common artifacts of novel view synthesis such as distortions and holes, while successfully generating high frequency details and preserving visual aspects of the input image. We evaluate our approach on a wide range of synthetic and real examples. Both qualitative and quantitative results show our method achieves significantly better results compared to existing methods.", "citation": "Citations (26)", "year": "2017", "departments": ["University of North Carolina at Chapel Hill", "Adobe Systems", "Adobe Systems", "Adobe Systems", "University of North Carolina at Chapel Hill"], "conf": "cvpr", "authors": ["Eunbyung Park.....http://dblp.org/pers/hd/p/Park:Eunbyung", "Jimei Yang.....http://dblp.org/pers/hd/y/Yang:Jimei", "Ersin Yumer.....http://dblp.org/pers/hd/y/Yumer:Ersin", "Duygu Ceylan.....http://dblp.org/pers/hd/c/Ceylan:Duygu", "Alexander C. Berg.....http://dblp.org/pers/hd/b/Berg:Alexander_C="], "pages": 10}