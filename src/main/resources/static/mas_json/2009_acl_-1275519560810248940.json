{"title": "Robust Machine Translation Evaluation with Entailment Features.", "fields": ["example based machine translation", "bleu", "textual entailment", "evaluation of machine translation", "logical consequence"], "abstract": "Existing evaluation metrics for machine translation lack crucial robustness: their correlations with human quality judgments vary considerably across languages and genres. We believe that the main reason is their inability to properly capture meaning: A good translation candidate means the same thing as the reference translation, regardless of formulation. We propose a metric that evaluates MT output based on a rich set of features motivated by textual entailment, such as lexical-semantic (in-)compatibility and argument structure overlap. We compare this metric against a combination metric of four state-of-the-art scores (BLEU, NIST, TER, and METEOR) in two different settings. The combination metric out-performs the individual scores, but is bested by the entailment-based metric. Combining the entailment and traditional features yields further improvements.", "citation": "Citations (78)", "year": "2009", "departments": ["University of Stuttgart", "Stanford University", "Stanford University", "Stanford University"], "conf": "acl", "authors": ["Sebastian Pad\u00f3.....http://dblp.org/pers/hd/p/Pad=oacute=:Sebastian", "Michel Galley.....http://dblp.org/pers/hd/g/Galley:Michel", "Daniel Jurafsky.....http://dblp.org/pers/hd/j/Jurafsky:Daniel", "Christopher D. Manning.....http://dblp.org/pers/hd/m/Manning:Christopher_D="], "pages": 9}