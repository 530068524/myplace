{"title": "Reducing the sampling complexity of topic models.", "fields": ["alias", "amortized analysis", "topic model", "generative model", "speedup"], "abstract": "Inference in topic models typically involves a sampling step to associate latent variables with observations. Unfortunately the generative model loses sparsity as the amount of data increases, requiring O(k) operations per word for k topics. In this paper we propose an algorithm which scales linearly with the number of actually instantiated topics k d  in the document. For large document collections and in structured hierarchical models k d  ll k. This yields an order of magnitude speedup. Our method applies to a wide variety of statistical models such as PDP [16,4] and HDP [19].   At its core is the idea that dense, slowly changing distributions can be approximated efficiently by the combination of a Metropolis-Hastings step, use of sparsity, and amortized constant time sampling via Walker's alias method.", "citation": "Citations (124)", "departments": ["Carnegie Mellon University", "Google", "Google", "Carnegie Mellon University"], "authors": ["Aaron Q. Li.....http://dblp.org/pers/hd/l/Li:Aaron_Q=", "Amr Ahmed.....http://dblp.org/pers/hd/a/Ahmed:Amr", "Sujith Ravi.....http://dblp.org/pers/hd/r/Ravi:Sujith", "Alexander J. Smola.....http://dblp.org/pers/hd/s/Smola:Alexander_J="], "conf": "kdd", "year": "2014", "pages": 10}