{"title": "Single or Multiple? Combining Word Representations Independently Learned from Text and WordNet.", "fields": ["distributional semantics", "random walk", "pattern recognition", "wordnet", "machine learning"], "abstract": "Text and Knowledge Bases are complementary sources of information. Given the success of distributed word representations learned from text, several techniques to infuse additional information from sources like WordNet into word representations have been proposed. In this paper, we follow an alternative route. We learn word representations from text and WordNet independently, and then explore simple and sophisticated methods to combine them. The combined representations are applied to an extensive set of datasets on word similarity and relatedness. Simple combination methods happen to perform better that more complex methods like CCA or retrofitting, showing that, in the case of WordNet, learning word representations separately is preferable to learning one single representation space or adding WordNet information directly. A key factor, which we illustrate with examples, is that the WordNet-based representations captures similarity relations encoded in WordNet better than retrofitting.\n\nIn addition, we show that the average of the similarities from six word representations yields results beyond the state-of-the-art in several datasets, reinforcing the opportunities to explore further combination techniques.", "citation": "Citations (11)", "departments": ["University of the Basque Country", "University of the Basque Country", "University of the Basque Country"], "authors": ["Josu Goikoetxea.....http://dblp.org/pers/hd/g/Goikoetxea:Josu", "Eneko Agirre.....http://dblp.org/pers/hd/a/Agirre:Eneko", "Aitor Soroa.....http://dblp.org/pers/hd/s/Soroa:Aitor"], "conf": "aaai", "year": "2016", "pages": 7}