{"title": "Hierarchical Implicit Models and Likelihood-Free Variational Inference.", "fields": ["data set", "inference", "generative grammar", "probabilistic logic", "scalability"], "abstract": "Implicit probabilistic models are a flexible class of models defined by a simulation process for data. They form the basis for models which encompass our understanding of the physical word. Despite this fundamental nature, the use of implicit models remains limited due to challenge in positing complex latent structure in them, and the ability to inference in such models with large data sets. In this paper, we first introduce the hierarchical implicit models (HIMs). HIMs combine the idea of implicit densities with hierarchical Bayesian modeling thereby defining models via simulators of data with rich hidden structure. Next, we develop likelihood-free variational inference (LFVI), a scalable variational inference algorithm for HIMs. Key to LFVI is specifying a variational family that is also implicit. This matches the model's flexibility and allows for accurate approximation of the posterior. We demonstrate diverse applications: a large-scale physical simulator for predator-prey populations in ecology; a Bayesian generative adversarial network for discrete data; and a deep implicit model for symbol generation.", "citation": "Citations (23)", "year": "2017", "departments": ["Columbia University", "OpenAI", "Princeton University", "Columbia University"], "conf": "nips", "authors": ["Dustin Tran.....http://dblp.org/pers/hd/t/Tran:Dustin", "Rajesh Ranganath.....http://dblp.org/pers/hd/r/Ranganath:Rajesh", "David M. Blei.....http://dblp.org/pers/hd/b/Blei:David_M="], "pages": 11}