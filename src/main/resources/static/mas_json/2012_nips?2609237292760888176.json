{"title": "Tight Bounds on Profile Redundancy and Distinguishability.", "fields": ["multiset", "minimax", "data element", "ranging", "regret"], "abstract": "The minimax KL-divergence of any distribution from all distributions in a collection P has several practical implications. In compression, it is called redundancy and represents the least additional number of bits over the entropy needed to encode the output of any distribution in \u01a4. In online estimation and learning, it is the lowest expected log-loss regret when guessing a sequence of random values generated by a distribution in \u01a4. In hypothesis testing, it upper bounds the largest number of distinguishable distributions in \u01a4. Motivated by problems ranging from population estimation to text classification and speech recognition, several machine-learning and information-theory researchers have recently considered label-invariant observations and properties induced by i.i.d. distributions. A sufficient statistic for all these properties is the data's profile, the multiset of the number of times each data element appears. Improving on a sequence of previous works, we show that the redundancy of the collection of distributions induced over profiles by length-n i.i.d. sequences is between 0.3 \u00b7 n1/3 and n1/3 log2 n, in particular, establishing its exact growth power.", "citation": "Citations (8)", "departments": ["University of California, San Diego", "Yahoo!", "University of California, San Diego"], "authors": ["Jayadev Acharya.....http://dblp.org/pers/hd/a/Acharya:Jayadev", "Hirakendu Das.....http://dblp.org/pers/hd/d/Das:Hirakendu", "Alon Orlitsky.....http://dblp.org/pers/hd/o/Orlitsky:Alon"], "conf": "nips", "year": "2012", "pages": 9}