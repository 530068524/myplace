{"title": "Dual Averaging Method for Regularized Stochastic Learning and Online Optimization.", "fields": ["online algorithm", "online machine learning", "stochastic gradient descent", "subgradient method", "proximal gradient methods for learning"], "abstract": "We consider regularized stochastic learning and online optimization problems, where the objective function is the sum of two convex terms: one is the loss function of the learning task, and the other is a simple regularization term such as l1-norm for promoting sparsity. We develop a new online algorithm, the regularized dual averaging (RDA) method, that can explicitly exploit the regularization structure in an online setting. In particular, at each iteration, the learning variables are adjusted by solving a simple optimization problem that involves the running average of all past subgradients of the loss functions and the whole regularization term, not just its subgradient. Computational experiments show that the RDA method can be very effective for sparse online learning with l1-regularization.", "citation": "Citations (111)", "year": "2009", "departments": ["Microsoft"], "conf": "nips", "authors": ["Lin Xiao.....http://dblp.org/pers/hd/x/Xiao:Lin"], "pages": 9}