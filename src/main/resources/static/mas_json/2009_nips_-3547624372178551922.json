{"title": "Periodic Step Size Adaptation for Single Pass On-line Learning.", "fields": ["quasi newton method", "stochastic gradient descent", "periodic graph", "hessian matrix", "jacobian matrix and determinant"], "abstract": "It has been established that the second-order stochastic gradient descent (2SGD) method can potentially achieve generalization performance as well as empirical optimum in a single pass (i.e., epoch) through the training examples. However, 2SGD requires computing the inverse of the Hessian matrix of the loss function, which is prohibitively expensive. This paper presents Periodic Step-size Adaptation (PSA), which approximates the Jacobian matrix of the mapping function and explores a linear relation between the Jacobian and Hessian to approximate the Hessian periodically and achieve near-optimal results in experiments on a wide variety of models and tasks.", "citation": "Citations (1)", "year": "2009", "departments": ["Information Sciences Institute", "Academia Sinica", "Academia Sinica", "National Taiwan University of Science and Technology"], "conf": "nips", "authors": ["Chun-Nan Hsu.....http://dblp.org/pers/hd/h/Hsu:Chun=Nan", "Yu-Ming Chang.....http://dblp.org/pers/hd/c/Chang:Yu=Ming", "Han-Shen Huang.....http://dblp.org/pers/hd/h/Huang:Han=Shen", "Yuh-Jye Lee.....http://dblp.org/pers/hd/l/Lee:Yuh=Jye"], "pages": 9}