{"title": "Learning word embeddings efficiently with noise-contrastive estimation.", "fields": ["syntax", "machine learning", "language model", "scalability", "order of magnitude"], "abstract": "Continuous-valued word embeddings learned by neural language models have recently been shown to capture semantic and syntactic information about words very well, setting performance records on several word similarity tasks. The best results are obtained by learning high-dimensional embeddings from very large quantities of data, which makes scalability of the training method a critical factor.\n\nWe propose a simple and scalable new approach to learning word embeddings based on training log-bilinear models with noise-contrastive estimation. Our approach is simpler, faster, and produces better results than the current state-of-the-art method. We achieve results comparable to the best ones reported, which were obtained on a cluster, using four times less data and more than an order of magnitude less computing time. We also investigate several model types and find that the embeddings learned by the simpler models perform at least as well as those learned by the more complex ones.", "citation": "Citations (279)", "departments": ["DeepMind Technologies", "DeepMind Technologies"], "authors": ["Andriy Mnih.....http://dblp.org/pers/hd/m/Mnih:Andriy", "Koray Kavukcuoglu.....http://dblp.org/pers/hd/k/Kavukcuoglu:Koray"], "conf": "nips", "year": "2013", "pages": 9}