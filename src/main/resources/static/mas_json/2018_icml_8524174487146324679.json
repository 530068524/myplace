{"title": "An Inference-Based Policy Gradient Method for Learning Options.", "fields": ["debugging", "formalism", "gradient method", "reinforcement learning", "abstraction"], "abstract": "In the pursuit of increasingly intelligent learning systems, abstraction plays a vital role in enabling sophisticated decisions to be made in complex environments. The options framework provides formalism for such abstraction over sequences of decisions. However most models require that options be given a priori, presumably specified by hand, which is neither efficient, nor scalable. Indeed, it is preferable to learn options directly from interaction with the environment. Despite several efforts, this remains a difficult problem: many approaches require access to a model of the environmental dynamics, and inferred options are often not interpretable, which limits our ability to explain the system behavior for verification or debugging purposes. In this work we develop a novel policy gradient method for the automatic learning of policies with options. This algorithm uses inference methods to simultaneously improve all of the options available to an agent, and thus can be employed in an off-policy manner, without observing option labels. Experimental results show that the options learned can be interpreted. Further, we find that the method presented here is more sample efficient than existing methods, leading to faster and more stable learning of policies with options.", "citation": "Not cited", "departments": ["McGill University", "McGill University", "McGill University"], "authors": ["Matthew Smith.....http://dblp.org/pers/hd/s/Smith:Matthew", "Herke Hoof.....http://dblp.org/pers/hd/h/Hoof:Herke", "Joelle Pineau.....http://dblp.org/pers/hd/p/Pineau:Joelle"], "conf": "icml", "year": "2018", "pages": 10}