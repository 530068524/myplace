{"title": "Chinese NER Using Lattice LSTM.", "fields": ["lattice", "machine learning", "lexicon", "artificial intelligence", "sentence"], "abstract": "We investigate a lattice-structured LSTM model for Chinese NER, which encodes a sequence of input characters as well as all potential words that match a lexicon. Compared with character-based methods, our model explicitly leverages word and word sequence information. Compared with word-based methods, lattice LSTM does not suffer from segmentation errors. Gated recurrent cells allow our model to choose the most relevant characters and words from a sentence for better NER results. Experiments on various datasets show that lattice LSTM outperforms both word-based and character-based LSTM baselines, achieving the best results.", "citation": "Not cited", "departments": [], "authors": ["Yue Zhang.....http://dblp.org/pers/hd/z/Zhang:Yue", "Jie Yang.....http://dblp.org/pers/hd/y/Yang:Jie"], "conf": "acl", "year": "2018", "pages": 11}