{"title": "Weakly-Supervised Grammar-Informed Bayesian CCG Parser Learning.", "fields": ["prior probability", "combinatory categorial grammar", "formalism", "universal set", "parsing"], "abstract": "Combinatory Categorial Grammar (CCG) is a lexicalized grammar formalism in which words are associated with categories that, in combination with a small universal set of rules, specify the syntactic configurations in which they may occur. Previous work has shown that learning sequence models for CCG tagging can be improved by using priors that are sensitive to the formal properties of CCG as well as cross-linguistic universals. We extend this approach to the task of learning a full CCG parser from weak supervision. We present a Bayesian formulation for CCG parser induction that assumes only supervision in the form of an incomplete tag dictionary mapping some word types to sets of potential categories. Our approach outperforms a baseline model trained with uniform priors by exploiting universal, intrinsic properties of the CCG formalism to bias the model toward simpler, more cross-linguistically common categories.", "citation": "Citations (6)", "departments": ["University of Texas at Austin", "Carnegie Mellon University", "University of Texas at Austin", "Carnegie Mellon University"], "authors": ["Dan Garrette.....http://dblp.org/pers/hd/g/Garrette:Dan", "Chris Dyer.....http://dblp.org/pers/hd/d/Dyer:Chris", "Jason Baldridge.....http://dblp.org/pers/hd/b/Baldridge:Jason", "Noah A. Smith.....http://dblp.org/pers/hd/s/Smith:Noah_A="], "conf": "aaai", "year": "2015", "pages": 7}