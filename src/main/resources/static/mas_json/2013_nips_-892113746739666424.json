{"title": "(Nearly) Optimal Algorithms for Private Online Learning in Full-information and Bandit Settings.", "fields": ["logarithm", "mathematical optimization", "regret", "artificial intelligence", "machine learning"], "abstract": "We give differentially private algorithms for a large class of online learning algorithms, in both the full information and bandit settings. Our algorithms aim to minimize a convex loss function which is a sum of smaller convex loss terms, one for each data point. To design our algorithms, we modify the popular mirror descent approach, or rather a variant called follow the approximate leader.\n\nThe technique leads to the first nonprivate algorithms for private online learning in the bandit setting. In the full information setting, our algorithms improve over the regret bounds of previous work (due to Dwork, Naor, Pitassi and Rothblum (2010) and Jain, Kothari and Thakurta (2012)). In many cases, our algorithms (in both settings) match the dependence on the input length, T, of the optimal nonprivate regret bounds up to logarithmic factors in T. Our algorithms require logarithmic space and update time.", "citation": "Citations (21)", "departments": ["Microsoft", "Pennsylvania State University"], "authors": ["Abhradeep Guha Thakurta.....http://dblp.org/pers/hd/t/Thakurta:Abhradeep_Guha", "Adam D. Smith.....http://dblp.org/pers/hd/s/Smith:Adam_D="], "conf": "nips", "year": "2013", "pages": 9}