{"title": "Statistical analysis of stochastic gradient methods for generalized linear models.", "fields": ["robustness", "delta method", "mathematical optimization", "stochastic gradient descent", "generalized linear model"], "abstract": "We study the statistical properties of stochastic gradient descent (SGD) using explicit and implicit updates for fitting generalized linear models (GLMs). Initially, we develop a computationally efficient algorithm to implement implicit SGD learning of GLMs. Next, we obtain exact formulas for the bias and variance of both updates which leads to two important observations on their comparative statistical properties. First, in small samples, the estimates from the implicit procedure are more biased than the estimates from the explicit one, but their empirical variance is smaller and they are more robust to learning rate misspecification. Second, the two procedures are statistically identical in the limit: they are both unbiased, converge at the same rate and have the same asymptotic variance. Our set of experiments confirm our theory and more broadly suggest that the implicit procedure can be a competitive choice for fitting large-scale models, especially when robustness is a concern.", "citation": "Citations (19)", "year": "2014", "departments": ["Harvard University", "Google", "Harvard University"], "conf": "icml", "authors": ["Panagiotis Toulis.....http://dblp.org/pers/hd/t/Toulis:Panagiotis", "Edoardo M. Airoldi.....http://dblp.org/pers/hd/a/Airoldi:Edoardo_M=", "Jason Rennie.....http://dblp.org/pers/hd/r/Rennie:Jason"], "pages": 9}