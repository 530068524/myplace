{"title": "Achieving Privacy in the Adversarial Multi-Armed Bandit.", "fields": ["mathematical proof", "adversarial system", "information sensitivity", "multi armed bandit", "differential privacy"], "abstract": "In this paper, we improve the previously best known regret\nChristos Dimitrakakis\nUniversity of Lille, France Chalmers University of Technology, Sweden Harvard University, USA christos.dimitrakakis@gmail.com\nmovie recommendations can be formalized similarly (Pandey and Olston 2006).\nPrivacy can be a serious issue in the bandit setting (c.f. (Jain, Kothari, and Thakurta 2012; Thakurta and Smith 2013; Mishra and Thakurta 2015; Zhao et al. 2014)). For example, in clinical trials, we may want to detect and publish results about the best drug without leaking sensitive information, such as the patient\u2019s health condition and genome. Differen- tial privacy (Dwork 2006) formally bounds the amount of information that a third party can learn no matter their power or side information.\nDifferential privacy has been used before in the stochastic setting (Tossou and Dimitrakakis 2016; Mishra and Thakurta 2015; Jain, Kothari, and Thakurta 2012) where the authors obtain optimal algorithms up to logarithmic factors. In the adversarial setting, (Thakurta and Smith 2013) adapts an algorithm called Follow The Approximate Leader to make it private and obtain a regret bound of O(T2/3). In this work, we show that a number of simple algorithms can satisfy privacy guarantees, while achieving nearly optimal regret (up to logarithmic factors) that scales naturally with the level of privacy desired.\nOur work is also of independent interest for non-private multi-armed bandit algorithms, as there are competitive with the current state of the art against switching-cost adversaries (where we recover the optimal bound). Finally, we provide rigorous empirical results against a variety of adversaries.\nThe following section gives the main background and nota- tions. Section 3.1 describes meta-algorithms that perturb the gain sequence to achieve privacy, while Section 3.2 explains how to leverage the privacy inherent in the EXP3 algorithm by modifying the way gains are used. Section 4 compares our algorithms with EXP3 in a variety of settings. The full proofs of all our main results are in the full version.\n2 Preliminaries\n2.1 The Multi-Armed Bandit problem\nFormally, a bandit game is defined between an adversary and an agent as follows: there is a set of K arms A, and at each round t, the agent plays an arm It \u2208 A. Given the choice It, the adversary grants the agent a gain gIt,t \u2208 [0,1]. The agent only observes the gain of arm It, and not that of any other arms. The goal of this agent is to maximize its total gain\nbound to achieve e-differential privacy in oblivious adversarial\nbandits from O(T2/3/e) to O(\u221aT lnT/e). This is achieved\n by combining a Laplace Mechanism with EXP3. We show that\nthough EXP3 is already differentially private, it leaks a linear\namount of information in T . However, we can improve this\nprivacy by relying on its intrinsic exponential mechanism for\n\u221a\n selecting actions. This allows us to reach O(\na regret of O(T2/3) that holds against an adaptive adversary, an improvement from the best known of O(T3/4). This is done by using an algorithm that run EXP3 in a mini-batch loop. Finally, we run experiments that clearly demonstrate the validity of our theoretical analysis.", "citation": "Citations (2)", "departments": ["Chalmers University of Technology", "Chalmers University of Technology"], "authors": ["Aristide Charles Yedia Tossou.....http://dblp.org/pers/hd/t/Tossou:Aristide_Charles_Yedia", "Christos Dimitrakakis.....http://dblp.org/pers/hd/d/Dimitrakakis:Christos"], "conf": "aaai", "year": "2017", "pages": 7}