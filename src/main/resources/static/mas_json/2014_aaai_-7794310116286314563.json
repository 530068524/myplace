{"title": "Reconsidering Mutual Information Based Feature Selection: A Statistical Significance View.", "fields": ["overfitting", "almost surely", "mutual information", "phenomenon", "class variable"], "abstract": "Mutual information (MI) based approaches are a popular feature selection paradigm. Although the stated goal of MI-based feature selection is to identify a subset of features that share the highest mutual information with the class variable, most current MI-based techniques are greedy methods that make use of low dimensional MI quantities. The reason for using low dimensional approximation has been mostly attributed to the difficulty associated with estimating the high dimensional MI from limited samples. In this paper, we argue a different viewpoint that, given a very large amount of data, the high dimensional MI objective is still problematic to be employed as a meaningful optimization criterion, due to its overfitting nature: the MI almost always increases as more features are added, thus leading to a trivial solution which includes all features. We propose a novel approach to the MI-based feature selection problem, in which the overfitting phenomenon is controlled rigourously by means of a statistical test. We develop local and global optimization algorithms for this new feature selection model, and demonstrate its effectiveness in the applications of explaining variables and objects.", "citation": "Citations (5)", "departments": ["University of Melbourne", "University of Melbourne", "University of Melbourne"], "authors": ["Nguyen Xuan Vinh.....http://dblp.org/pers/hd/v/Vinh:Nguyen_Xuan", "Jeffrey Chan.....http://dblp.org/pers/hd/c/Chan:Jeffrey", "James Bailey.....http://dblp.org/pers/hd/b/Bailey:James"], "conf": "aaai", "year": "2014", "pages": 7}