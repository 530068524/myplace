{"title": "Bayesian Uncertainty Estimation for Batch Normalized Deep Networks.", "fields": ["deep learning", "approximate inference", "inference", "normalization", "formal methods"], "abstract": "Deep neural networks have led to a series of breakthroughs, dramatically improving the state-of-the-art in many domains. The techniques driving these advances, however, lack a formal method to account for model uncertainty. While the Bayesian approach to learning provides a solid theoretical framework to handle uncertainty, inference in Bayesian-inspired deep neural networks is difficult. In this paper, we provide a practical approach to Bayesian learning that relies on a regularization technique found in nearly every modern network, batch normalization. We show that training a deep network using batch normalization is equivalent to approximate inference in Bayesian models, and we demonstrate how this finding allows us to make useful estimates of the model uncertainty. Using our approach, it is possible to make meaningful uncertainty estimates using conventional architectures without modifying the network or the training procedure. Our approach is thoroughly validated in a series of empirical experiments on different tasks and using various measures, showing it to outperform baselines on a majority of datasets with strong statistical significance.", "citation": "Citations (2)", "departments": ["Royal Institute of Technology", "Royal Institute of Technology", "Royal Institute of Technology"], "authors": ["Mattias Teye.....http://dblp.org/pers/hd/t/Teye:Mattias", "Hossein Azizpour.....http://dblp.org/pers/hd/a/Azizpour:Hossein", "Kevin Smith.....http://dblp.org/pers/hd/s/Smith:Kevin"], "conf": "icml", "year": "2018", "pages": 10}