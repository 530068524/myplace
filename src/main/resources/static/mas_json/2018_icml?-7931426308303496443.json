{"title": "Soft Actor-Critic: Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor.", "fields": ["convergence", "hyperparameter", "principle of maximum entropy", "sample complexity", "reinforcement learning"], "abstract": "Model-free deep reinforcement learning (RL) algorithms have been demonstrated on a range of challenging decision making and control tasks. However, these methods typically suffer from two major challenges: very high sample complexity and brittle convergence properties, which necessitate meticulous hyperparameter tuning. Both of these challenges severely limit the applicability of such methods to complex, real-world domains. In this paper, we propose soft actor-critic, an off-policy actor-critic deep RL algorithm based on the maximum entropy reinforcement learning framework. In this framework, the actor aims to maximize expected reward while also maximizing entropy - that is, succeed at the task while acting as randomly as possible. Prior deep RL methods based on this framework have been formulated as either off-policy Q-learning, or on-policy policy gradient methods. By combining off-policy updates with a stable stochastic actor-critic formulation, our method achieves state-of-the-art performance on a range of continuous control benchmark tasks, outperforming prior on-policy and off-policy methods. Furthermore, we demonstrate that, in contrast to other off-policy algorithms, our approach is very stable, achieving very similar performance across different random seeds.", "citation": "Citations (12)", "departments": ["University of California, Berkeley", "University of California, Berkeley", "University of California, Berkeley", "University of California, Berkeley"], "authors": ["Tuomas Haarnoja.....http://dblp.org/pers/hd/h/Haarnoja:Tuomas", "Aurick Zhou.....http://dblp.org/pers/hd/z/Zhou:Aurick", "Pieter Abbeel.....http://dblp.org/pers/hd/a/Abbeel:Pieter", "Sergey Levine.....http://dblp.org/pers/hd/l/Levine:Sergey"], "conf": "icml", "year": "2018", "pages": 10}