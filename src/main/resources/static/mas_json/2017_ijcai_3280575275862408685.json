{"title": "Efficient Private ERM for Smooth Objectives.", "fields": ["stationary point", "convex function", "empirical risk minimization", "stochastic gradient descent", "expected utility hypothesis"], "abstract": "In this paper, we consider efficient differentially private empirical risk minimization from the viewpoint of optimization algorithms. For strongly convex and smooth objectives, we prove that gradient descent with output perturbation not only achieves nearly optimal utility, but also significantly improves the running time of previous state-of-the-art private optimization algorithms, for both $\\epsilon$-DP and $(\\epsilon, \\delta)$-DP. For non-convex but smooth objectives, we propose an RRPSGD (Random Round Private Stochastic Gradient Descent) algorithm, which provably converges to a stationary point with privacy guarantee. Besides the expected utility bounds, we also provide guarantees in high probability form. Experiments demonstrate that our algorithm consistently outperforms existing method in both utility and running time.", "citation": "Citations (2)", "year": "2017", "departments": ["Peking University", "University of California, Berkeley", "Peking University"], "conf": "ijcai", "authors": ["Jiaqi Zhang.....http://dblp.org/pers/hd/z/Zhang:Jiaqi", "Kai Zheng.....http://dblp.org/pers/hd/z/Zheng_0007:Kai", "Wenlong Mou.....http://dblp.org/pers/hd/m/Mou:Wenlong", "Liwei Wang.....http://dblp.org/pers/hd/w/Wang_0001:Liwei"], "pages": 7}