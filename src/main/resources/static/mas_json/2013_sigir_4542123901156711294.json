{"title": "Sequential testing in classifier evaluation yields biased estimates of effectiveness.", "fields": ["data mining", "supervised learning", "classifier", "sequential analysis", "machine learning"], "abstract": "It is common to develop and validate classifiers through a process of repeated testing, with nested training and/or test sets of increasing size. We demonstrate in this paper that such repeated testing leads to biased estimates of classifier effectiveness. Experiments on a range of text classification tasks under three sequential testing frameworks show all three lead to optimistic estimates of effectiveness. We calculate empirical adjustments to unbias estimates on our data set, and identify directions for research that could lead to general techniques for avoiding bias while reducing labeling costs.", "citation": "Citations (3)", "departments": ["University of Maryland, College Park", "University of Maryland, College Park", "University of Maryland, College Park", "David D. Lewis  ... hicago, IL, USA"], "authors": ["William Webber.....http://dblp.org/pers/hd/w/Webber:William", "Mossaab Bagdouri.....http://dblp.org/pers/hd/b/Bagdouri:Mossaab", "David D. Lewis.....http://dblp.org/pers/hd/l/Lewis:David_D=", "Douglas W. Oard.....http://dblp.org/pers/hd/o/Oard:Douglas_W="], "conf": "sigir", "year": "2013", "pages": 4}