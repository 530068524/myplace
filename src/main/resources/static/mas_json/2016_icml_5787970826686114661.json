{"title": "Learning and Inference via Maximum Inner Product Search.", "fields": ["probabilistic logic", "bottleneck", "normalizing constant", "language model", "nearest neighbor search"], "abstract": "A large class of commonly used probabilistic models known as log-linear models are defined up to a normalization constant. Typical learning algorithms for such models require solving a sequence of probabilistic inference queries. These inferences are typically intractable, and are a major bottleneck for learning models with large output spaces. In this paper, we provide a new approach for amortizing the cost of a sequence of related inference queries, such as the ones arising during learning. Our technique relies on a surprising connection with algorithms developed in the past two decades for similarity search in large data bases. Our approach achieves improved running times with provable approximation guarantees. We show that it performs well both on synthetic data and neural language models with large output spaces.", "citation": "Citations (5)", "year": "2016", "departments": ["Stanford University", "Stanford University"], "conf": "icml", "authors": ["Stephen Mussmann.....http://dblp.org/pers/hd/m/Mussmann:Stephen", "Stefano Ermon.....http://dblp.org/pers/hd/e/Ermon:Stefano"], "pages": 10}