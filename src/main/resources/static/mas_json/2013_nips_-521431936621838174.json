{"title": "Improved and Generalized Upper Bounds on the Complexity of Policy Iteration.", "fields": ["state space", "inverse", "m 2", "simplex", "discrete mathematics", "binary logarithm", "pi", "markov decision process"], "abstract": "Given a Markov decision process (MDP) with n states and a total number m of actions, we study the number of iterations needed by policy iteration (PI) algorithms to converge to the optimal \u03b3 -discounted policy. We consider two variations of PI: Howard\u2019s PI that changes the actions in all states with a positive advantage, and Simplex-PI that only changes the action in the state with maximal advantage. We show that Howard\u2019s PI terminates after at most O (( m /(1 \u2212 \u03b3 ))log(1/(1 \u2212 \u03b3 ))) iterations, improving by a factor O (log n ) a result by Hansen et al. [Hansen TD, Miltersen PB, Zwick U (2013) Strategy iteration is strongly polynomial for two-player turn-based stochastic games with a constant discount factor. J. ACM 60(1):1:1\u20131:16.], whereas Simplex-PI terminates after at most O (( n m /(1 \u2212 \u03b3 ))log(1/(1 \u2212 \u03b3 ))) iterations, improving by a factor O (log n ) a result by Ye [Ye Y (2011) The simplex and policy-iteration methods are strongly polynomial for the Markov decision problem with a fixed discount rate. Math. Oper. Res. 36(4):593\u2013603.]. Under some structural properties of the MDP, we then consider bounds that are independent of the discount factor \u03b3 : quantities of interest are bounds \u03c4 t and \u03c4 r \u2014uniform on all states and policies\u2014respectively, on the expected time spent in transient states and the inverse of the frequency of visits in recurrent states given that the process starts from the uniform distribution. Indeed, we show that Simplex-PI terminates after at most O ( n 3 m 2 \u03c4 t \u03c4 r ) iterations. This extends a recent result for deterministic MDPs by Post and Ye [Post I, Ye Y (2013) The simplex method is strongly polynomial for deterministic Markov decision processes. Khanna S, ed. Proc. 24th ACM-SIAM Sympos. Discrete Algorithms, SODA '13 (SIAM, Philadelphia), 1465\u20131473.] in which \u03c4 t \u2264 1 and \u03c4 r \u2264 n ; in particular it shows that Simplex-PI is strongly polynomial for a much larger class of MDPs. We explain why similar results seem hard to derive for Howard\u2019s PI. Finally, under the additional (restrictive) assumption that the state space is partitioned in two sets, respectively, states that are transient and recurrent for all policies, we show that both Howard\u2019s PI and Simplex-PI terminate after at most O ( m ( n 2 \u03c4 t + n \u03c4 r )) iterations.", "citation": "Citations (11)", "departments": ["University of Lorraine"], "authors": ["Bruno Scherrer.....http://dblp.org/pers/hd/s/Scherrer:Bruno"], "conf": "nips", "year": "2013", "pages": 9}