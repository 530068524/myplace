{"title": "Hypothesis Transfer Learning via Transformation Functions.", "fields": ["risk analysis", "kernel", "generalization error", "kernel smoother", "transfer of learning"], "abstract": "We consider the Hypothesis Transfer Learning (HTL) problem where one incorporates a hypothesis trained on the source domain into the learning procedure of the target domain. Existing theoretical analysis either only studies specific algorithms or only presents upper bounds on the generalization error but not on the excess risk. In this paper, we propose a unified algorithm-dependent framework for HTL through a novel notion of transformation functions, which characterizes the relation between the source and the target domains. We conduct a general risk analysis of this framework and in particular, we show for the first time, if two domains are related, HTL enjoys faster convergence rates of excess risks for Kernel Smoothing and Kernel Ridge Regression than those of the classical non-transfer learning settings. We accompany this framework with an analysis of cross-validation for HTL to search for the best transfer technique and gracefully reduce to non-transfer learning when HTL is not helpful. Experiments on robotics and neural imaging data demonstrate the effectiveness of our framework.", "citation": "Citations (1)", "year": "2017", "departments": ["Carnegie Mellon University", "Carnegie Mellon University", "Carnegie Mellon University", "Carnegie Mellon University"], "conf": "nips", "authors": ["Simon S. Du.....http://dblp.org/pers/hd/d/Du:Simon_S=", "Jayanth Koushik.....http://dblp.org/pers/hd/k/Koushik:Jayanth", "Aarti Singh.....http://dblp.org/pers/hd/s/Singh:Aarti", "Barnab\u00e1s P\u00f3czos.....http://dblp.org/pers/hd/p/P=oacute=czos:Barnab=aacute=s"], "pages": 11}