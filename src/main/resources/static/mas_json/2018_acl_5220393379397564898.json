{"title": "A Unified Model for Extractive and Abstractive Summarization using Inconsistency Loss.", "fields": ["unified model", "machine learning", "paragraph", "automatic summarization", "rouge"], "abstract": "We propose a unified model combining the strength of extractive and abstractive summarization. On the one hand, a simple extractive model can obtain sentence-level attention with high ROUGE scores but less readable. On the other hand, a more complicated abstractive model can obtain word-level dynamic attention to generate a more readable paragraph. In our model, sentence-level attention is used to modulate the word-level attention such that words in less attended sentences are less likely to be generated. Moreover, a novel inconsistency loss function is introduced to penalize the inconsistency between two levels of attentions. By end-to-end training our model with the inconsistency loss and original losses of extractive and abstractive models, we achieve state-of-the-art ROUGE scores while being the most informative and readable summarization on the CNN/Daily Mail dataset in a solid human evaluation.", "citation": "Not cited", "departments": [], "authors": ["Min Sun.....http://dblp.org/pers/hd/s/Sun:Min", "Wan Ting Hsu.....http://dblp.org/pers/hd/h/Hsu:Wan_Ting", "Chieh-Kai Lin.....http://dblp.org/pers/hd/l/Lin:Chieh=Kai", "Ming-Ying Lee.....http://dblp.org/pers/hd/l/Lee:Ming=Ying", "Kerui Min.....http://dblp.org/pers/hd/m/Min:Kerui", "Jing Tang.....http://dblp.org/pers/hd/t/Tang:Jing"], "conf": "acl", "year": "2018", "pages": 10}