{"title": "Towards Robust Neural Machine Translation.", "fields": ["machine translation", "machine learning", "natural language processing", "robustness", "encoder"], "abstract": "Small perturbations in the input can severely distort intermediate representations and thus impact translation quality of neural machine translation (NMT) models. In this paper, we propose to improve the robustness of NMT models with adversarial stability training. The basic idea is to make both the encoder and decoder in NMT models robust against input perturbations by enabling them to behave similarly for the original input and its perturbed counterpart. Experimental results on Chinese-English, English-German and English-French translation tasks show that our approaches can not only achieve significant improvements over strong NMT systems but also improve the robustness of NMT models.", "citation": "Citations (20)", "year": "2018", "departments": ["Tsinghua University", "Tencent", "Tencent", "Tencent", "Tsinghua University"], "conf": "acl", "authors": ["Yang Liu.....http://dblp.org/pers/hd/l/Liu_0005:Yang", "Zhaopeng Tu.....http://dblp.org/pers/hd/t/Tu:Zhaopeng", "Fandong Meng.....http://dblp.org/pers/hd/m/Meng:Fandong", "Yong Cheng.....http://dblp.org/pers/hd/c/Cheng:Yong", "Junjie Zhai.....http://dblp.org/pers/hd/z/Zhai:Junjie"], "pages": 11}