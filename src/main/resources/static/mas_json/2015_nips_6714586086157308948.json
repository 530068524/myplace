{"title": "Fast Convergence of Regularized Learning in Games.", "fields": ["convergence", "mathematical economics", "adversary", "regret", "machine learning"], "abstract": "We show that natural classes of regularized learning algorithms with a form of recency bias achieve faster convergence rates to approximate efficiency and to coarse correlated equilibria in multiplayer normal form games. When each player in a game uses an algorithm from our class, their individual regret decays at O(T-3/4), while the sum of utilities converges to an approximate optimum at O(T-1)-an improvement upon the worst case O(T-1/2) rates. We show a black-box reduction for any algorithm in the class to achieve O(T-1/2) rates against an adversary, while maintaining the faster rates against algorithms in the class. Our results extend those of Rakhlin and Shridharan [17] and Daskalakis et al. [4], who only analyzed two-player zero-sum games for specific algorithms.", "citation": "Citations (20)", "year": "2015", "departments": ["Microsoft", "Microsoft", "Princeton University", "Microsoft"], "conf": "nips", "authors": ["Vasilis Syrgkanis.....http://dblp.org/pers/hd/s/Syrgkanis:Vasilis", "Alekh Agarwal.....http://dblp.org/pers/hd/a/Agarwal:Alekh", "Haipeng Luo.....http://dblp.org/pers/hd/l/Luo:Haipeng", "Robert E. Schapire.....http://dblp.org/pers/hd/s/Schapire:Robert_E="], "pages": 9}