{"title": "Regularized Off-Policy TD-Learning.", "fields": ["convex optimization", "regularization", "saddle", "feature selection", "temporal difference learning"], "abstract": "We present a novel l1 regularized off-policy convergent TD-learning method (termed RO-TD), which is able to learn sparse representations of value functions with low computational complexity. The algorithmic framework underlying RO-TD integrates two key ideas: off-policy convergent gradient TD methods, such as TDC, and a convex-concave saddle-point formulation of non-smooth convex optimization, which enables first-order solvers and feature selection using online convex regularization. A detailed theoretical and experimental analysis of RO-TD is presented. A variety of experiments are presented to illustrate the off-policy convergence, sparse feature selection capability and low computational cost of the RO-TD algorithm.", "citation": "Citations (28)", "departments": ["University of Massachusetts Amherst", "University of Massachusetts Amherst", "University of Wisconsin-Madison"], "authors": ["Bo Liu.....http://dblp.org/pers/hd/l/Liu_0006:Bo", "Sridhar Mahadevan.....http://dblp.org/pers/hd/m/Mahadevan:Sridhar", "Ji Liu.....http://dblp.org/pers/hd/l/Liu_0002:Ji"], "conf": "nips", "year": "2012", "pages": 9}