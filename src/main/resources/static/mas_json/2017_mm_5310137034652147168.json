{"title": "Learning to Recognise Unseen Classes by A Few Similes.", "fields": ["contextual image classification", "machine learning", "inference", "annotation", "caltech 101"], "abstract": "Existing image classification systems often suffer from re-training models for novel unseen classes. Zero-shot learning (ZSL) aims to recognise these unseen classes directly using trained models with a further inference procedure. However, existing approaches highly rely on human-defined class-attribute associations to achieve the inference, which significantly increases the annotation cost. This paper aims to address ZSL on non-attribute tasks, i.e. only training images with labels are used as most of the supervised settings. Our main contributions are: 1) to circumvent expensive attributes, we propose to use semantic similes that directly indicate the unseen-to-seen associations; 2) a novel similarity-based representation is proposed to represent both visual images and semantic similes in a unified embedding space; 3) in order to reduce the annotation cost, we use only a few similes to infer a class-level prototype for each unseen class. On two popular benchmarks, AwA and aPY, extensive experiments manifest that our method can significantly improve the state-of-the-art results using only two similes for each unseen class. Furthermore, we revisit the Caltech 101 dataset without attributes. Our ZSL results can exceed that of previous supervised methods.", "citation": "Citations (2)", "departments": ["University of Sheffield", "University of East Anglia"], "authors": ["Yang Long.....http://dblp.org/pers/hd/l/Long:Yang", "Ling Shao.....http://dblp.org/pers/hd/s/Shao_0001:Ling"], "conf": "mm", "year": "2017", "pages": 9}