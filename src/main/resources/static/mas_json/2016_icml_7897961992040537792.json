{"title": "Stochastic Variance Reduction for Nonconvex Optimization.", "fields": ["variance reduction", "stochastic gradient descent", "speedup", "subclass", "convexity"], "abstract": "We study nonconvex finite-sum problems and analyze stochastic variance reduced gradient (SVRG) methods for them. SVRG and related methods have recently surged into prominence for convex optimization given their edge over stochastic gradient descent (SGD); but their theoretical analysis almost exclusively assumes convexity. In contrast, we obtain nonasymptotic rates of convergence of SVRG for nonconvex optimization, showing that it is provably faster than SGD and gradient descent. We also analyze a subclass of nonconvex problems on which SVRG attains linear convergence to the global optimum. We extend our analysis to mini-batch variants, showing (theoretical) linear speedup due to minibatching in parallel settings.", "citation": "Citations (140)", "year": "2016", "departments": ["Carnegie Mellon University", "Carnegie Mellon University", "Massachusetts Institute of Technology", "Carnegie Mellon University", "Carnegie Mellon University"], "conf": "icml", "authors": ["Sashank J. Reddi.....http://dblp.org/pers/hd/r/Reddi:Sashank_J=", "Ahmed Hefny.....http://dblp.org/pers/hd/h/Hefny:Ahmed", "Suvrit Sra.....http://dblp.org/pers/hd/s/Sra:Suvrit", "Barnab\u00e1s P\u00f3czos.....http://dblp.org/pers/hd/p/P=oacute=czos:Barnab=aacute=s", "Alexander J. Smola.....http://dblp.org/pers/hd/s/Smola:Alexander_J="], "pages": 10}