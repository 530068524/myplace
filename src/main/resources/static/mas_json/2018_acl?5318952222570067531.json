{"title": "Backpropagating through Structured Argmax using a SPIGOT.", "fields": ["backpropagation", "estimator", "inference", "dependency grammar", "parsing"], "abstract": "We introduce the structured projection of intermediate gradients optimization technique (SPIGOT), a new method for backpropagating through neural networks that include hard-decision structured predictions (e.g., parsing) in intermediate layers. SPIGOT requires no marginal inference, unlike structured attention networks (Kim et al., 2017) and some reinforcement learning-inspired solutions (Yogatama et al., 2017). Like so-called straight-through estimators (Hinton, 2012), SPIGOT defines gradient-like quantities associated with intermediate nondifferentiable operations, allowing backpropagation before and after them; SPIGOT's proxy aims to ensure that, after a parameter update, the intermediate structure will remain well-formed. \nWe experiment on two structured NLP pipelines: syntactic-then-semantic dependency parsing, and semantic parsing followed by sentiment classification. We show that training with SPIGOT leads to a larger improvement on the downstream task than a modularly-trained pipeline, the straight-through estimator, and structured attention, reaching a new state of the art on semantic dependency parsing.", "citation": "Not cited", "year": "2018", "departments": ["Peking University", "Carnegie Mellon University", "University of Washington"], "conf": "acl", "authors": ["Noah A. Smith.....http://dblp.org/pers/hd/s/Smith:Noah_A=", "Hao Peng.....http://dblp.org/pers/hd/p/Peng:Hao", "Sam Thomson.....http://dblp.org/pers/hd/t/Thomson:Sam"], "pages": 11}