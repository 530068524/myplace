{"title": "Prior-Free Exploration Bonus for and beyond Near Bayes-Optimal Behavior.", "fields": ["prior probability", "dilemma", "reinforcement learning", "dirichlet distribution", "bayes theorem"], "abstract": "We study Bayesian reinforcement learning (RL) as a solution of the exploration-exploitation dilemma. As full Bayesian planning is intractable except for special cases, previous work has proposed several approximation methods. However, these were often computationally expensive or limited to Dirichlet priors. In this paper, we propose a new algorithm that is fast and of polynomial time for near Bayesian optimal policy with any prior distributions that are not greatly misspecified. Perhaps even more interestingly, the proposed algorithm can naturally avoid being misled by incorrect beliefs, while effectively utilizing useful parts of prior information. It can work well even when an utterly misspecified prior is assigned. In that case, the algorithm will follow PAC-MDP behavior instead, if an existing PACMDP algorithm does so. The proposed algorithm naturally outperformed other algorithms compared with it on a standard benchmark problem.", "citation": "Citations (1)", "year": "2013", "departments": ["National Defense Academy of Japan", "BWBP Artificial ... oratory, Japan"], "conf": "ijcai", "authors": ["Kenji Kawaguchi.....http://dblp.org/pers/hd/k/Kawaguchi:Kenji", "Hiroshi Sato.....http://dblp.org/pers/hd/s/Sato:Hiroshi"], "pages": 7}