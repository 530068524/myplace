{"title": "Domain Generalization via Invariant Feature Representation.", "fields": ["invariant", "machine learning", "component analysis", "input output", "kernel"], "abstract": "This paper investigates domain generalization: How to take knowledge acquired from an arbitrary number of related domains and apply it to previously unseen domains? We propose Domain-Invariant Component Analysis (DICA), a kernel-based optimization algorithm that learns an invariant transformation by minimizing the dissimilarity across domains, whilst preserving the functional relationship between input and output variables. A learning-theoretic analysis shows that reducing dissimilarity improves the expected generalization ability of classifiers on new domains, motivating the proposed algorithm. Experimental results on synthetic and real-world datasets demonstrate that DICA successfully learns invariant features and improves classifier performance in practice.", "citation": "Citations (95)", "departments": ["Max Planck Society", "ETH Zurich", "Max Planck Society"], "authors": ["Krikamol Muandet.....http://dblp.org/pers/hd/m/Muandet:Krikamol", "David Balduzzi.....http://dblp.org/pers/hd/b/Balduzzi:David", "Bernhard Sch\u00f6lkopf.....http://dblp.org/pers/hd/s/Sch=ouml=lkopf:Bernhard"], "conf": "icml", "year": "2013", "pages": 9}