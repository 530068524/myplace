{"title": "Set Cover at Web Scale.", "fields": ["hyperlink", "cardinality", "web mining", "web crawler", "orders of magnitude"], "abstract": "The classic Set Cover problem requires selecting a minimum size subset  A  \u2286  F  from a family of finite subsets  F  Of  U  such that the elements covered by  A  are the ones covered by  F . It naturally occurs in many settings in web search, web mining and web advertising. The greedy algorithm that iteratively selects a set in  F  that covers the most uncovered elements, yields an optimum (1+ln | U |)-approximation but is inherently sequential. In this work we give the first MapReduce Set Cover algorithm that scales to problem sizes of \u223c 1 trillion elements and runs in log  p   \u0394 iterations for a nearly optimum approximation ratio of  p  ln \u0394, where \u0394 is the cardinality of the largest set in  F     A web crawler is a system for bulk downloading of web pages. Given a set of seed URLs, the crawler downloads and extracts the hyperlinks embedded in them and schedules the crawling of the pages addressed by those hyperlinks for a subsequent iteration. While the average page out-degree is \u223c 50, the crawled corpus grows at a much smaller rate, implying a significant outlink overlap. Using our MapReduce  Set Cover  heuristic as a building block, we present the first large-scale seed generation algorithm that scales to \u223c 20 billion nodes and discovers new pages at a rate \u223c 4x faster than that obtained by prior art heuristics.", "citation": "Citations (6)", "departments": ["Yahoo!", "Yahoo!"], "authors": ["Stergios Stergiou.....http://dblp.org/pers/hd/s/Stergiou:Stergios", "Kostas Tsioutsiouliklis.....http://dblp.org/pers/hd/t/Tsioutsiouliklis:Kostas"], "conf": "kdd", "year": "2015", "pages": 9}