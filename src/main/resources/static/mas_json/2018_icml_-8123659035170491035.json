{"title": "Clustering Semi-Random Mixtures of Gaussians.", "fields": ["cluster analysis", "time complexity", "adversary", "mixture model", "monotone polygon"], "abstract": "Gaussian mixture models (GMM) are the most widely used statistical model for the $k$-means clustering problem and form a popular framework for clustering in machine learning and data analysis. In this paper, we propose a natural semi-random model for $k$-means clustering that generalizes the Gaussian mixture model, and that we believe will be useful in identifying robust algorithms. In our model, a semi-random adversary is allowed to make arbitrary \"monotone\" or helpful changes to the data generated from the Gaussian mixture model. \nOur first contribution is a polynomial time algorithm that provably recovers the ground-truth up to small classification error w.h.p., assuming certain separation between the components. Perhaps surprisingly, the algorithm we analyze is the popular Lloyd's algorithm for $k$-means clustering that is the method-of-choice in practice. Our second result complements the upper bound by giving a nearly matching information-theoretic lower bound on the number of misclassified points incurred by any $k$-means clustering algorithm on the semi-random model.", "citation": "Citations (1)", "departments": ["Northwestern University", "Rutgers University"], "authors": ["Pranjal Awasthi.....http://dblp.org/pers/hd/a/Awasthi:Pranjal", "Aravindan Vijayaraghavan.....http://dblp.org/pers/hd/v/Vijayaraghavan:Aravindan"], "conf": "icml", "year": "2018", "pages": 10}