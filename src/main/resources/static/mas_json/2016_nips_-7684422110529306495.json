{"title": "Unifying Count-Based Exploration and Intrinsic Motivation.", "fields": ["intrinsic value", "artificial intelligence", "reinforcement learning", "generalization", "machine learning"], "abstract": "We consider an agent's uncertainty about its environment and the problem of generalizing this uncertainty across states. Specifically, we focus on the problem of exploration in non-tabular reinforcement learning. Drawing inspiration from the intrinsic motivation literature, we use density models to measure uncertainty, and propose a novel algorithm for deriving a pseudo-count from an arbitrary density model. This technique enables us to generalize count-based exploration algorithms to the non-tabular case. We apply our ideas to Atari 2600 games, providing sensible pseudo-counts from raw pixels. We transform these pseudo-counts into exploration bonuses and obtain significantly improved exploration in a number of hard games, including the infamously difficult Montezuma's Revenge.", "citation": "Citations (186)", "departments": ["Google", "Google", "Google", "New York University"], "authors": ["Marc G. Bellemare.....http://dblp.org/pers/hd/b/Bellemare:Marc_G=", "Sriram Srinivasan.....http://dblp.org/pers/hd/s/Srinivasan:Sriram", "Georg Ostrovski.....http://dblp.org/pers/hd/o/Ostrovski:Georg", "Tom Schaul.....http://dblp.org/pers/hd/s/Schaul:Tom", "David Saxton.....http://dblp.org/pers/hd/s/Saxton:David", "R\u00e9mi Munos.....http://dblp.org/pers/hd/m/Munos:R=eacute=mi"], "conf": "nips", "year": "2016", "pages": 9}