{"title": "Scalable sparse tensor decompositions in distributed memory systems.", "fields": ["approximation algorithm", "distributed memory", "tensor", "sparse matrix", "multilinear subspace learning"], "abstract": "We investigate an efficient parallelization of the most common iterative sparse tensor decomposition algorithms on distributed memory systems. A key operation in each iteration of these algorithms is the matricized tensor times Khatri-Rao product (MTTKRP). This operation amounts to element-wise vector multiplication and reduction depending on the sparsity of the tensor. We investigate a fine and a coarse-grain task definition for this operation, and propose hypergraph partitioning-based methods for these task definitions to achieve the load balance as well as reduce the communication requirements. We also design a distributed memory sparse tensor library, HyperTensor, which implements a well-known algorithm for the CANDECOMP-/PARAFAC (CP) tensor decomposition using the task definitions and the associated partitioning methods. We use this library to test the proposed implementation of MTTKRP in CP decomposition context, and report scalability results up to 1024 MPI ranks. We observed up to 194 fold speedups using 512 MPI processes on a well-known real world data, and significantly better performance results with respect to a state of the art implementation.", "citation": "Citations (27)", "year": "2015", "departments": ["\u00c9cole normale sup\u00e9rieure de Lyon", "\u00c9cole normale sup\u00e9rieure de Lyon"], "conf": "sc", "authors": ["Oguz Kaya.....http://dblp.org/pers/hd/k/Kaya:Oguz", "Bora U\u00e7ar.....http://dblp.org/pers/hd/u/U=ccedil=ar:Bora"], "pages": 11}