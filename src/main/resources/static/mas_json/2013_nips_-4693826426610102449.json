{"title": "Reinforcement Learning in Robust Markov Decision Processes.", "fields": ["state space", "regret", "minimax", "reinforcement learning", "adversarial system", "markov decision process"], "abstract": "An important challenge in Markov decision processes (MDP) is to ensure robustness with respect to unexpected or adversarial system behavior. A standard paradigm to tackle this challenge is the robust MDP framework that models the parameters as arbitrary elements of pre-defined \u201cuncertainty sets,\u201d and seeks the minimax policy\u2014the policy that performs the best under the worst realization of the parameters in the uncertainty set. A crucial issue of the robust MDP framework, largely unaddressed in literature, is how to find appropriate description of the uncertainty in a principled data-driven way. In this paper we address this problem using an online learning approach: we devise an algorithm that, without knowing the true uncertainty model, is able to adapt its level of protection to uncertainty, and in the long run performs as well as the minimax policy as if the true uncertainty model is known. Indeed, the algorithm achieves similar regret bounds as standard MDP where no parameter is adversarial, which shows that with virtually no extra cost we can adapt robust learning to handle uncertainty in MDPs. To the best of our knowledge, this is the first attempt to learn uncertainty in robust MDPs.", "citation": "Not cited", "departments": ["National University of Singapore", "National University of Singapore", "Technion \u2013 Israel Institute of Technology"], "authors": ["Shiau Hong Lim.....http://dblp.org/pers/hd/l/Lim:Shiau_Hong", "Huan Xu.....http://dblp.org/pers/hd/x/Xu:Huan", "Shie Mannor.....http://dblp.org/pers/hd/m/Mannor:Shie"], "conf": "nips", "year": "2013", "pages": 9}