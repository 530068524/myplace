{"title": "Convergence of Least Squares Temporal Difference Methods Under General Conditions.", "fields": ["almost surely", "markov model", "markov decision process", "markov kernel", "temporal difference learning"], "abstract": "We consider approximate policy evaluation for finite state and action Markov decision processes (MDP) in the off-policy learning context and with the simulation-based least squares temporal difference algorithm, LSTD(\u03bb). We establish for the discounted cost criterion that the off-policy LSTD(\u03bb) converges almost surely under mild, minimal conditions. We also analyze other convergence and boundedness properties of the iterates involved in the algorithm, and based on them, we suggest a modification in its practical implementation. Our analysis uses theories of both finite space Markov chains and Markov chains on topological spaces.", "citation": "Citations (22)", "departments": ["University of Helsinki"], "authors": ["Huizhen Yu.....http://dblp.org/pers/hd/y/Yu:Huizhen"], "conf": "icml", "year": "2010", "pages": 8}