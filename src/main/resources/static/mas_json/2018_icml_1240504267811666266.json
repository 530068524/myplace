{"title": "StrassenNets: Deep Learning with a Multiplication Budget.", "fields": ["strassen algorithm", "deep learning", "matrix multiplication", "multiplication", "resnet"], "abstract": "A large fraction of the arithmetic operations required to evaluate deep neural networks (DNNs) are due to matrix multiplications, both in convolutional and fully connected layers. Matrix multiplications can be cast as $2$-layer sum-product networks (SPNs) (arithmetic circuits), disentangling multiplications and additions. We leverage this observation for end-to-end learning of low-cost (in terms of multiplications) approximations of linear operations in DNN layers. Specifically, we propose to replace matrix multiplication operations by SPNs, with widths corresponding to the budget of multiplications we want to allocate to each layer, and learning the edges of the SPNs from data. Experiments on CIFAR-10 and ImageNet show that this method applied to ResNet yields significantly higher accuracy than existing methods for a given multiplication budget, or leads to the same or higher accuracy compared to existing methods while using significantly fewer multiplications. Furthermore, our approach allows fine-grained control of the tradeoff between arithmetic complexity and accuracy of DNN models. Finally, we demonstrate that the proposed framework is able to rediscover Strassen's matrix multiplication algorithm, i.e., it can learn to multiply $2 \\times 2$ matrices using only $7$ multiplications instead of $8$.", "citation": "Not cited", "departments": ["ETH Zurich", "California Institute of Technology", "Dolores Technologies"], "authors": ["Michael Tschannen.....http://dblp.org/pers/hd/t/Tschannen:Michael", "Aran Khanna.....http://dblp.org/pers/hd/k/Khanna:Aran", "Animashree Anandkumar.....http://dblp.org/pers/hd/a/Anandkumar:Animashree"], "conf": "icml", "year": "2018", "pages": 10}