{"title": "TAP: A TLP-aware cache management policy for a CPU-GPU heterogeneous architecture.", "fields": ["smart cache", "adaptive replacement cache", "snoopy cache", "mesif protocol", "cache pollution"], "abstract": "Combining CPUs and GPUs on the same chip has become a popular architectural trend. However, these heterogeneous architectures put more pressure on shared resource management. In particular, managing the last-level cache (LLC) is very critical to performance. Lately, many researchers have proposed several shared cache management mechanisms, including dynamic cache partitioning and promotion-based cache management, but no cache management work has been done on CPU-GPU heterogeneous architectures. Sharing the LLC between CPUs and GPUs brings new challenges due to the different characteristics of CPU and GPGPU applications. Unlike most memory-intensive CPU benchmarks that hide memory latency with caching, many GPGPU applications hide memory latency by combining thread-level parallelism (TLP) and caching. In this paper, we propose a TLP-aware cache management policy for CPU-GPU heterogeneous architectures. We introduce a core-sampling mechanism to detect how caching affects the performance of a GPGPU application. Inspired by previous cache management schemes, Utility-based Cache Partitioning (UCP) and Re-Reference Interval Prediction (RRIP), we propose two new mechanisms: TAP-UCP and TAP-RRIP. TAP-UCP improves performance by 5% over UCP and 11% over LRU on 152 heterogeneous workloads, and TAP-RRIP improves performance by 9% over RRIP and 12% over LRU.", "citation": "Citations (114)", "departments": ["Georgia Institute of Technology", "Georgia Institute of Technology"], "authors": ["Jaekyu Lee.....http://dblp.org/pers/hd/l/Lee:Jaekyu", "Hyesoon Kim.....http://dblp.org/pers/hd/k/Kim:Hyesoon"], "conf": "hpca", "year": "2012", "pages": 12}