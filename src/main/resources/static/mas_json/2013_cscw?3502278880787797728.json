{"title": "Quality control mechanisms for crowdsourcing: peer review, arbitration, & expertise at familysearch indexing.", "fields": ["crowdsourcing", "transcription", "field experiment", "search engine indexing", "arbitration"], "abstract": "The FamilySearch Indexing project has enabled hundreds of thousands of volunteers to transcribe billions of records, making it one of the largest crowdsourcing initiatives in the world. Assuring high quality transcriptions (i.e., indexes) with a reasonable amount of volunteer effort is essential to keep pace with the mounds of newly digitized documents. Using historical data, we show the relationship between prior experience and native language on transcriber agreement. We then present a field experiment comparing the effectiveness (accuracy) and efficiency (time) of two quality control mechanisms: (1) Arbitration -- the existing mechanism wherein two volunteers independently transcribe records and disagreements go to an arbitrator, and (2) Peer Review -- a mechanism wherein one volunteer's work is reviewed by another volunteer. Peer Review is significantly more efficient, though not as effective for certain fields as Arbitration. Design suggestions for FamilySearch Indexing and related crowdsourcing initiatives are provided.", "citation": "Citations (23)", "departments": ["Brigham Young University", "Brigham Young University", "Brigham Young University", "FamilySearch, S ... City, Utah, USA", "FamilySearch, S ... City, Utah, USA"], "authors": ["Derek L. Hansen.....http://dblp.org/pers/hd/h/Hansen:Derek_L=", "Patrick John Schone.....http://dblp.org/pers/hd/s/Schone:Patrick_John", "Douglas Corey.....http://dblp.org/pers/hd/c/Corey:Douglas", "Matthew Reid.....http://dblp.org/pers/hd/r/Reid:Matthew", "Jake Gehring.....http://dblp.org/pers/hd/g/Gehring:Jake"], "conf": "cscw", "year": "2013", "pages": 12}