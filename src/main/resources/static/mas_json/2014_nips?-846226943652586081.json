{"title": "Cone-Constrained Principal Component Analysis.", "fields": ["convex cone", "minimax", "magnitude", "optimal estimation", "dimensionality reduction"], "abstract": "Estimating a vector from noisy quadratic observations is a task that arises naturally in many contexts, from dimensionality reduction, to synchronization and phase retrieval problems. It is often the case that additional information is available about the unknown vector (for instance, sparsity, sign or magnitude of its entries). Many authors propose non-convex quadratic optimization problems that aim at exploiting optimally this information. However, solving these problems is typically NP-hard.\n\nWe consider a simple model for noisy quadratic observation of an unknown vector v0. The unknown vector is constrained to belong to a cone C \u220b v0. While optimal estimation appears to be intractable for the general problems in this class, we provide evidence that it is tractable when C is a convex cone with an efficient projection. This is surprising, since the corresponding optimization problem is non-convex and -from a worst case perspective- often NP hard. We characterize the resulting minimax risk in terms of the statistical dimension of the cone \u03b4(C). This quantity is already known to control the risk of estimation from gaussian observations and random linear measurements. It is rather surprising that the same quantity plays a role in the estimation risk from quadratic measurements.", "citation": "Citations (12)", "year": "2014", "departments": ["Stanford University", "Stanford University", "Stanford University"], "conf": "nips", "authors": ["Yash Deshpande.....http://dblp.org/pers/hd/d/Deshpande:Yash", "Andrea Montanari.....http://dblp.org/pers/hd/m/Montanari:Andrea", "Emile Richard.....http://dblp.org/pers/hd/r/Richard:Emile"], "pages": 9}