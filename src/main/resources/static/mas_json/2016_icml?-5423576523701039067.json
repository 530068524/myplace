{"title": "A Superlinearly-Convergent Proximal Newton-type Method for the Optimization of Finite Sums.", "fields": ["convex function", "local convergence", "small number", "finite set", "machine learning", "mathematics", "proximal gradient methods", "mathematical optimization", "standard algorithms", "artificial intelligence"], "abstract": "We consider the problem of optimizing the strongly convex sum of a finite number of convex functions. Standard algorithms for solving this problem in the class of incremental/stochastic methods have at most a linear convergence rate. We propose a new incremental method whose convergence rate is superlinear - the Newton-type incremental method (NIM). The idea of the method is to introduce a model of the objective with the same sum-of-functions structure and further update a single component of the model per iteration. We prove that NIM has a superlinear local convergence rate and linear global convergence rate. Experiments show that the method is very effective for problems with a large number of functions and a small number of variables.", "citation": "Not cited", "year": "2016", "departments": ["National Research University \u2013 Higher School of Economics", "Moscow State University"], "conf": "icml", "authors": ["Anton Rodomanov.....http://dblp.org/pers/hd/r/Rodomanov:Anton", "Dmitry Kropotov.....http://dblp.org/pers/hd/k/Kropotov:Dmitry"], "pages": 9}