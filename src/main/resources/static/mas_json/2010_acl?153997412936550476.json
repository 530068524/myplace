{"title": "Viterbi Training for PCFGs: Hardness Results and Competitiveness of Uniform Initialization.", "fields": ["viterbi algorithm", "soft output viterbi algorithm", "probabilistic logic", "initialization", "iterative viterbi decoding"], "abstract": "We consider the search for a maximum likelihood assignment of hidden derivations and grammar weights for a probabilistic context-free grammar, the problem approximately solved by \"Viterbi training.\" We show that solving and even approximating Viterbi training for PCFGs is NP-hard. We motivate the use of uniformat-random initialization for Viterbi EM as an optimal initializer in absence of further information about the correct model parameters, providing an approximate bound on the log-likelihood.", "citation": "Citations (14)", "year": "2010", "departments": ["Carnegie Mellon University", "Carnegie Mellon University"], "conf": "acl", "authors": ["Shay B. Cohen.....http://dblp.org/pers/hd/c/Cohen:Shay_B=", "Noah A. Smith.....http://dblp.org/pers/hd/s/Smith:Noah_A="], "pages": 10}