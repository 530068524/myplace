{"title": "Accelerated Variance Reduced Stochastic ADMM.", "fields": ["convex function", "regular polygon", "acceleration", "momentum", "rate of convergence"], "abstract": "Recently, many variance reduced stochastic alternating direction method of multipliers (ADMM) methods (e.g. SAG-ADMM, SDCA-ADMM and SVRG-ADMM) have made exciting progress such as linear convergence rates for strongly convex problems. However, the best known convergence rate for general convex problems is O(1/ T ) as opposed to O(1/ T  2 ) of accelerated batch algorithms, where T is the number of iterations. Thus, there still remains a gap in convergence rates between existing stochastic ADMM and batch algorithms. To bridge this gap, we introduce the momentum acceleration trick for batch optimization into the stochastic variance reduced gradient based ADMM (SVRG-ADMM), which leads to an accelerated (ASVRG-ADMM) method. Then we design two different momentum term update rules for strongly convex and general convex cases. We prove that ASVRG-ADMM converges linearly for strongly convex problems. Besides having a low-iteration complexity as existing stochastic ADMM methods, ASVRG-ADMM improves the convergence rate on general convex problems from O(1/ T ) to O(1/T 2 ). Our experimental results show the effectiveness of ASVRG-ADMM.", "citation": "Citations (6)", "departments": ["The Chinese University of Hong Kong", "The Chinese University of Hong Kong", "The Chinese University of Hong Kong"], "authors": ["Yuanyuan Liu.....http://dblp.org/pers/hd/l/Liu:Yuanyuan", "Fanhua Shang.....http://dblp.org/pers/hd/s/Shang:Fanhua", "James Cheng.....http://dblp.org/pers/hd/c/Cheng:James"], "conf": "aaai", "year": "2017", "pages": 7}