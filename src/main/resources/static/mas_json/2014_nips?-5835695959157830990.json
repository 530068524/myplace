{"title": "Distributed Variational Inference in Sparse Gaussian Process Regression and Latent Variable Models.", "fields": ["inference", "mnist database", "dimensionality reduction", "kriging", "latent variable"], "abstract": "Gaussian processes (GPs) are a powerful tool for probabilistic inference over functions. They have been applied to both regression and non-linear dimensionality reduction, and offer desirable properties such as uncertainty estimates, robustness to over-fitting, and principled ways for tuning hyper-parameters. However the scalability of these models to big datasets remains an active topic of research.\n\nWe introduce a novel re-parametrisation of variational inference for sparse GP regression and latent variable models that allows for an efficient distributed algorithm. This is done by exploiting the decoupling of the data given the inducing points to re-formulate the evidence lower bound in a Map-Reduce setting.\n\nWe show that the inference scales well with data and computational resources, while preserving a balanced distribution of the load among the nodes. We further demonstrate the utility in scaling Gaussian processes to big data. We show that GP performance improves with increasing amounts of data in regression (on flight data with 2 million records) and latent variable modelling (on MNIST). The results show that GPs perform better than many common models often used for big data.", "citation": "Citations (97)", "year": "2014", "departments": ["University of Cambridge", "University of Cambridge", "University of Cambridge"], "conf": "nips", "authors": ["Yarin Gal.....http://dblp.org/pers/hd/g/Gal:Yarin", "Mark van der Wilk.....http://dblp.org/pers/hd/w/Wilk:Mark_van_der", "Carl E. Rasmussen.....http://dblp.org/pers/hd/r/Rasmussen:Carl_E="], "pages": 9}