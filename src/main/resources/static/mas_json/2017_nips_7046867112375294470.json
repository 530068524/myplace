{"title": "End-to-end Differentiable Proving.", "fields": ["radial basis function kernel", "backward chaining", "rule of inference", "unification", "differentiable function"], "abstract": "We introduce deep neural networks for end-to-end differentiable theorem proving that operate on dense vector representations of symbols. These neural networks are recursively constructed by following the backward chaining algorithm as used in Prolog. Specifically, we replace symbolic unification with a differentiable computation on vector representations of symbols using a radial basis function kernel, thereby combining symbolic reasoning with learning subsymbolic vector representations. The resulting neural network can be trained to infer facts from a given incomplete knowledge base using gradient descent. By doing so, it learns to (i) place representations of similar symbols in close proximity in a vector space, (ii) make use of such similarities to prove facts, (iii) induce logical rules, and (iv) it can use provided and induced logical rules for complex multi-hop reasoning. On four benchmark knowledge bases we demonstrate that this architecture outperforms ComplEx, a state-of-the-art neural link prediction model, while at the same time inducing interpretable function-free first-order logic rules.", "citation": "Citations (9)", "year": "2017", "departments": ["University of Oxford", "University College London"], "conf": "nips", "authors": ["Tim Rockt\u00e4schel.....http://dblp.org/pers/hd/r/Rockt=auml=schel:Tim", "Sebastian Riedel.....http://dblp.org/pers/hd/r/Riedel_0001:Sebastian"], "pages": 13}