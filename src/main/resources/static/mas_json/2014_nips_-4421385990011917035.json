{"title": "Analysis of Learning from Positive and Unlabeled Data.", "fields": ["semi supervised learning", "hinge loss", "unsupervised learning", "generalization error", "anomaly detection"], "abstract": "Learning a classifier from positive and unlabeled data is an important class of classification problems that are conceivable in many practical applications. In this paper, we first show that this problem can be solved by cost-sensitive learning between positive and unlabeled data. We then show that convex surrogate loss functions such as the hinge loss may lead to a wrong classification boundary due to an intrinsic bias, but the problem can be avoided by using non-convex loss functions such as the ramp loss. We next analyze the excess risk when the class prior is estimated from data, and show that the classification accuracy is not sensitive to class prior estimation if the unlabeled data is dominated by the positive data (this is naturally satisfied in inlier-based outlier detection because inliers are dominant in the unlabeled dataset). Finally, we provide generalization error bounds and show that, for an equal number of labeled and unlabeled samples, the generalization error of learning only from positive and unlabeled samples is no worse than 2\u221a2 times the fully supervised case. These theoretical findings are also validated through experiments.", "citation": "Citations (28)", "year": "2014", "departments": ["University of Tokyo", "Baidu", "University of Tokyo"], "conf": "nips", "authors": ["Marthinus Christoffel du Plessis.....http://dblp.org/pers/hd/p/Plessis:Marthinus_Christoffel_du", "Gang Niu.....http://dblp.org/pers/hd/n/Niu:Gang", "Masashi Sugiyama.....http://dblp.org/pers/hd/s/Sugiyama:Masashi"], "pages": 9}