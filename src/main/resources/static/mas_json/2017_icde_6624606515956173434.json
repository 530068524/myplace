{"title": "Modeling Scalability of Distributed Machine Learning.", "fields": ["deep learning", "belief propagation", "active learning", "computational learning theory", "online machine learning"], "abstract": "Abstract-Present day machine learning is computationallyintensive and processes large amounts of data. It is implementedin a distributed fashion in order to address these scalabilityissues. The work is parallelized across a number of computingnodes. It is usually hard to estimate in advance how manynodes to use for a particular workload. We propose a simpleframework for estimating the scalability of distributed machinelearning algorithms. We measure the scalability by means of thespeedup an algorithm achieves with more nodes. We proposetime complexity models for gradient descent and graphicalmodel inference. We validate the gradient descent model withexperiments on deep learning training and graphical inferenceswith experiments on loopy belief propagation. The proposedframework was used to study the scalability of machine learningalgorithms in Apache Spark", "citation": "Not cited", "departments": ["Hewlett-Packard", "Hewlett-Packard", "Hewlett-Packard"], "authors": ["Alexander Ulanov.....http://dblp.org/pers/hd/u/Ulanov:Alexander", "Andrey Simanovsky.....http://dblp.org/pers/hd/s/Simanovsky:Andrey", "Manish Marwah.....http://dblp.org/pers/hd/m/Marwah:Manish"], "conf": "icde", "year": "2017", "pages": 6}