{"title": "Policy Optimization with Second-Order Advantage Information.", "fields": ["subspace topology", "estimator", "quadratic equation", "control variates", "factorization"], "abstract": "Policy optimization on high-dimensional continuous control tasks exhibits its difficulty caused by the large variance of the policy gradient estimators. We present the action subspace dependent gradient (ASDG) estimator which incorporates the Rao-Blackwell theorem (RB) and Control Variates (CV) into a unified framework to reduce the variance. To invoke RB, our proposed algorithm (POSA) learns the underlying factorization structure among the action space based on the second-order advantage information. POSA captures the quadratic information explicitly and efficiently by utilizing the wide & deep architecture. Empirical studies show that our proposed approach demonstrates the performance improvements on high-dimensional synthetic settings and OpenAI Gym's MuJoCo continuous control tasks.", "citation": "Not cited", "departments": ["The Chinese University of Hong Kong", "The Chinese University of Hong Kong", "The Chinese University of Hong Kong", "Tencent"], "authors": ["Jiajin Li.....http://dblp.org/pers/hd/l/Li:Jiajin", "Baoxiang Wang.....http://dblp.org/pers/hd/w/Wang:Baoxiang", "Shengyu Zhang.....http://dblp.org/pers/hd/z/Zhang:Shengyu"], "conf": "ijcai", "year": "2018", "pages": 7}