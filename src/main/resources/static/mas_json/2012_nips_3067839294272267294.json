{"title": "A Stochastic Gradient Method with an Exponential Convergence Rate for Finite Training Sets.", "fields": ["convex function", "nonlinear conjugate gradient method", "stochastic gradient descent", "stochastic optimization", "proximal gradient methods", "standard algorithms"], "abstract": "We propose a new stochastic gradient method for optimizing the sum of a finite set of smooth functions, where the sum is strongly convex. While standard stochastic gradient methods converge at sublinear rates for this problem, the proposed method incorporates a memory of previous gradient values in order to achieve a linear convergence rate. In a machine learning context, numerical experiments indicate that the new algorithm can dramatically outperform standard algorithms, both in terms of optimizing the training error and reducing the test error quickly.", "citation": "Citations (428)", "departments": ["\u00c9cole Normale Sup\u00e9rieure", "\u00c9cole Normale Sup\u00e9rieure", "\u00c9cole Normale Sup\u00e9rieure"], "authors": ["Nicolas Le Roux.....http://dblp.org/pers/hd/r/Roux:Nicolas_Le", "Mark W. Schmidt.....http://dblp.org/pers/hd/s/Schmidt:Mark_W=", "Francis R. Bach.....http://dblp.org/pers/hd/b/Bach:Francis_R="], "conf": "nips", "year": "2012", "pages": 9}