{"title": "Ultra-Fast Optimization Algorithm for Sparse Multi Kernel Learning.", "fields": ["compromise", "regularization", "stochastic gradient descent", "rate of convergence", "kernel"], "abstract": "Many state-of-the-art approaches for Multi Kernel Learning (MKL) struggle at finding a compromise between performance, sparsity of the solution and speed of the optimization process. In this paper we look at the MKL problem at the same time from a learning and optimization point of view. So, instead of designing a regularizer and then struggling to find an efficient method to minimize it, we design the regularizer while keeping the optimization algorithm in mind. Hence, we introduce a novel MKL formulation, which mixes elements of p-norm and elastic-net kind of regularization. We also propose a fast stochastic gradient descent method that solves the novel MKL formulation. We show theoretically and empirically that our method has 1) state-of-the-art performance on many classification tasks; 2) exact sparse solutions with a tunable level of sparsity; 3) a convergence rate bound that depends only logarithmically on the number of kernels used, and is independent of the sparsity required; 4) independence on the particular convex loss function used.", "citation": "Citations (64)", "year": "2011", "departments": ["University of Milan", "\u00c9cole Polytechnique F\u00e9d\u00e9rale de Lausanne", "Idiap Research Institute"], "conf": "icml", "authors": ["Francesco Orabona.....http://dblp.org/pers/hd/o/Orabona:Francesco", "Jie Luo.....http://dblp.org/pers/hd/l/Luo:Jie"], "pages": 8}