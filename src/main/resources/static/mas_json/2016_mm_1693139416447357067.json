{"title": "Event Localization in Music Auto-tagging.", "fields": ["clips", "labeled data", "convolutional neural network", "scarcity", "visual objects"], "abstract": "In music auto-tagging, people develop models to automatically label a music clip with attributes such as instruments, styles or acoustic properties. Many of these tags are actually descriptors of local events in a music clip, rather than a holistic description of the whole clip. Localizing such tags in time can potentially innovate the way people retrieve and interact with music, but little work has been done to date due to the scarcity of labeled data with granularity specific enough to the frame level. Most labeled data for training a learning-based model for music auto-tagging are in the clip level, providing no cues when and how long these attributes appear in a music clip. To bridge this gap, we propose in this paper a convolutional neural network (CNN) architecture that is able to make accurate frame-level predictions of tags in unseen music clips by using only clip-level annotations in the training phase. Our approach is motivated by recent advances in computer vision for localizing visual objects, but we propose new designs of the CNN architecture to account for the temporal information of music and the variable duration of such local tags in time. We report extensive experiments to gain insights into the problem of event localization in music, and validate through experiments the effectiveness of the proposed approach. In addition to quantitative evaluations, we also present qualitative analyses showing the model can indeed learn certain characteristics of music tags.", "citation": "Citations (5)", "departments": ["National Taiwan University", "Academia Sinica"], "authors": ["Jen-Yu Liu.....http://dblp.org/pers/hd/l/Liu:Jen=Yu", "Yi-Hsuan Yang.....http://dblp.org/pers/hd/y/Yang:Yi=Hsuan"], "conf": "mm", "year": "2016", "pages": 10}