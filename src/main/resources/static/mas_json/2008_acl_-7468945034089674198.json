{"title": "Learning Bigrams from Unigrams.", "fields": ["perplexity", "test set", "bigram", "marginal likelihood", "bag of words model"], "abstract": "Traditional wisdom holds that once documents are turned into bag-of-words (unigram count) vectors, word orders are completely lost. We introduce an approach that, perhaps surprisingly, is able to learn a bigram language model from a set of bag-of-words documents. At its heart, our approach is an EM algorithm that seeks a model which maximizes the regularized marginal likelihood of the bagof-words documents. In experiments on seven corpora, we observed that our learned bigram language models: i) achieve better test set perplexity than unigram models trained on the same bag-of-words documents, and are not far behind \u201coracle bigram models\u201d trained on the corresponding ordered documents; ii) assign higher probabilities to sensible bigram word pairs; iii) improve the accuracy of ordereddocument recovery from a bag-of-words. Our approach opens the door to novel phenomena, for example, privacy leakage from index files.", "citation": "Citations (6)", "year": "2008", "departments": ["University of Wisconsin-Madison", "University of Wisconsin-Madison", "McGill University", "University of Wisconsin-Madison"], "conf": "acl", "authors": ["Xiaojin Zhu.....http://dblp.org/pers/hd/z/Zhu_0001:Xiaojin", "Andrew B. Goldberg.....http://dblp.org/pers/hd/g/Goldberg:Andrew_B=", "Michael Rabbat.....http://dblp.org/pers/hd/r/Rabbat:Michael", "Robert D. Nowak.....http://dblp.org/pers/hd/n/Nowak:Robert_D="], "pages": 9}