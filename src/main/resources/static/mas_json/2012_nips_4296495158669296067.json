{"title": "Mixability in Statistical Learning.", "fields": ["minimum description length", "inference engine", "special case", "rotation formalisms in three dimensions", "regret"], "abstract": "Statistical learning and sequential prediction are two different but related formalisms to study the quality of predictions. Mapping out their relations and transferring ideas is an active area of investigation. We provide another piece of the puzzle by showing that an important concept in sequential prediction, the mixability of a loss, has a natural counterpart in the statistical setting, which we call stochastic mixability. Just as ordinary mixability characterizes fast rates for the worst-case regret in sequential prediction, stochastic mixability characterizes fast rates in statistical learning. We show that, in the special case of log-loss, stochastic mixability reduces to a well-known (but usually unnamed) martingale condition, which is used in existing convergence theorems for minimum description length and Bayesian inference. In the case of 0/1-loss, it reduces to the margin condition of Mammen and Tsybakov, and in the case that the model under consideration contains all possible predictors, it is equivalent to ordinary mixability.", "citation": "Citations (9)", "departments": ["University of Paris-Sud", "Leiden University", "NICTA", "NICTA"], "authors": ["Tim van Erven.....http://dblp.org/pers/hd/e/Erven:Tim_van", "Peter D. Gr\u00fcnwald.....http://dblp.org/pers/hd/g/Gr=uuml=nwald:Peter_D=", "Mark D. Reid.....http://dblp.org/pers/hd/r/Reid:Mark_D=", "Robert C. Williamson.....http://dblp.org/pers/hd/w/Williamson:Robert_C="], "conf": "nips", "year": "2012", "pages": 9}