{"title": "Indexed block coordinate descent for large-scale linear classification with limited memory.", "fields": ["search engine indexing", "regular polygon", "coordinate descent", "linear classifier", "sublinear function"], "abstract": "Linear Classification has achieved complexity linear to the data size. However, in many applications, data contain large amount of samples that does not help improve the quality of model, but still cost much I/O and memory to process. In this paper, we show how a Block Coordinate Descent method based on Nearest-Neighbor Index can significantly reduce such cost when learning a dual-sparse model. In particular, we employ truncated loss function to induce a series of convex programs with superior dual sparsity, and solve each dual using Indexed Block Coordinate Descent, which makes use of Approximate Nearest Neighbor (ANN) search to select active dual variables without I/O cost on irrelevant samples. We prove that, despite the bias and weak guarantee from ANN query, the proposed algorithm has global convergence to the solution defined on entire dataset, with sublinear complexity each iteration. Experiments in both sufficient and limited memory conditions show that the proposed approach learns many times faster than other state-of-the-art solvers without sacrificing accuracy.", "citation": "Citations (3)", "departments": ["National Taiwan University", "National Taiwan University", "National Taiwan University", "National Taiwan University", "National Taiwan University"], "authors": ["Ian En-Hsu Yen.....http://dblp.org/pers/hd/y/Yen:Ian_En=Hsu", "Chun-Fu Chang.....http://dblp.org/pers/hd/c/Chang:Chun=Fu", "Ting-Wei Lin.....http://dblp.org/pers/hd/l/Lin:Ting=Wei", "Shan-Wei Lin.....http://dblp.org/pers/hd/l/Lin:Shan=Wei", "Shou-De Lin.....http://dblp.org/pers/hd/l/Lin:Shou=De"], "conf": "kdd", "year": "2013", "pages": 9}