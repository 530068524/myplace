{"title": "Dropout Training as Adaptive Regularization.", "fields": ["overfitting", "training set", "fisher information", "generalized linear model", "document classification"], "abstract": "Dropout and other feature noising schemes control overfitting by artificially corrupting the training data. For generalized linear models, dropout performs a form of adaptive regularization. Using this viewpoint, we show that the dropout regularizer is first-order equivalent to an L2 regularizer applied after scaling the features by an estimate of the inverse diagonal Fisher information matrix. We also establish a connection to AdaGrad, an online learning algorithm, and find that a close relative of AdaGrad operates by repeatedly solving linear dropout-regularized problems. By casting dropout as regularization, we develop a natural semi-supervised algorithm that uses unlabeled data to create a better adaptive regularizer. We apply this idea to document classification tasks, and show that it consistently boosts the performance of dropout training, improving on state-of-the-art results on the IMDB reviews dataset.", "citation": "Citations (244)", "departments": ["Stanford University", "Stanford University", "Stanford University"], "authors": ["Stefan Wager.....http://dblp.org/pers/hd/w/Wager:Stefan", "Sida I. Wang.....http://dblp.org/pers/hd/w/Wang:Sida_I=", "Percy Liang.....http://dblp.org/pers/hd/l/Liang:Percy"], "conf": "nips", "year": "2013", "pages": 9}