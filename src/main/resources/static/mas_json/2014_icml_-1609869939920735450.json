{"title": "An Information Geometry of Statistical Manifold Learning.", "fields": ["information geometry", "manifold alignment", "nonlinear dimensionality reduction", "local tangent space alignment", "statistical manifold"], "abstract": "Manifold learning seeks low-dimensional representations of high-dimensional data. The main tactics have been exploring the geometry in an input data space and an output embedding space. We develop a manifold learning theory in a hypothesis space consisting of models. A model means a specific instance of a collection of points, e.g., the input data collectively or the output embedding collectively. The semi-Riemannian metric of this hypothesis space is uniquely derived in closed form based on the information geometry of probability distributions. There, manifold learning is interpreted as a trajectory of intermediate models. The volume of a continuous region reveals an amount of information. It can be measured to define model complexity and embedding quality. This provides deep unified perspectives of manifold learning theory.", "citation": "Citations (9)", "year": "2014", "departments": ["University of Geneva", "University of Geneva"], "conf": "icml", "authors": ["Ke Sun.....http://dblp.org/pers/hd/s/Sun_0001:Ke", "St\u00e9phane Marchand-Maillet.....http://dblp.org/pers/hd/m/Marchand=Maillet:St=eacute=phane"], "pages": 9}