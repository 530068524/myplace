{"title": "Lock-Free Optimization for Non-Convex Problems.", "fields": ["convergence", "regular polygon", "mathematical optimization", "machine learning", "non blocking algorithm"], "abstract": "Stochastic gradient descent~(SGD) and its variants have attracted much attention in machine learning due to their efficiency and effectiveness for optimization. To handle large-scale problems, researchers have recently proposed several lock-free strategy based parallel SGD~(LF-PSGD) methods for multi-core systems. However, existing works have only proved the convergence of these LF-PSGD methods for convex problems. To the best of our knowledge, no work has proved the convergence of the LF-PSGD methods for non-convex problems. In this paper, we provide the theoretical proof about the convergence of two representative LF-PSGD methods, Hogwild! and AsySVRG, for non-convex problems. Empirical results also show that both Hogwild! and AsySVRG are convergent on non-convex problems, which successfully verifies our theoretical results.", "citation": "Not cited", "departments": ["Nanjing University", "Nanjing University", "Nanjing University"], "authors": ["Shen-Yi Zhao.....http://dblp.org/pers/hd/z/Zhao:Shen=Yi", "Gong-Duo Zhang.....http://dblp.org/pers/hd/z/Zhang:Gong=Duo", "Wu-Jun Li.....http://dblp.org/pers/hd/l/Li:Wu=Jun"], "conf": "aaai", "year": "2017", "pages": 7}